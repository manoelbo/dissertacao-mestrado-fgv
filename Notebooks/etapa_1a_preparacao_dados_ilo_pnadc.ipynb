{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6deeebd6",
   "metadata": {},
   "source": [
    "# ETAPA 1 - Análise Descritiva da Exposição de IA Generativa com ILO Index e PNADc\n",
    "## PREPARAÇÃO DOS DADOS \n",
    "\n",
    "\n",
    "**Dissertação:** Inteligência Artificial Generativa e o Mercado de Trabalho Brasileiro: Uma Análise de Exposição Ocupacional e seus Efeitos Distributivos.\n",
    "\n",
    "**Aluno:** Manoel Brasil Orlandi\n",
    "\n",
    "### Contextualização\n",
    "\n",
    "A rápida difusão de modelos de IA generativa (LLMs, geradores de imagem/código) levanta questões centrais sobre seus impactos no mercado de trabalho. Para mensurar esse potencial de impacto, a  Organização Internacional do Trabalho (OIT) criou índice de exposição ocupacional à IA generativa, publicado como *Working Paper* 140 (WP140). O índice atribui scores de exposição a cada ocupação da classificação ISCO-08, com base na avaliação de suas tarefas constituintes por modelos de linguagem e validação humana.\n",
    "\n",
    "Este notebook prepara uma base de dados que junta os microdados da **PNAD Contínua** (Pesquisa Nacional por Amostra de Domicílios Contínua, IBGE, 3º trimestre de 2025) ao **índice de exposição à IA generativa da OIT**, para depois ser aplicada para caracterizar a exposição do mercado de trabalho brasileiro a essa tecnologia.\n",
    "\n",
    "### Objetivo\n",
    "\n",
    "Construir a base analítica que une PNAD Contínua e o índice de exposição à IA (ILO), com ocupações em COD e ISCO-08.\n",
    "\n",
    "**Entradas:** Microdados PNAD (BigQuery), planilha ILO (Gmyrek et al., 2025), estrutura COD.  \n",
    "**Saída principal:** `data/output/pnad_ilo_merged.csv`\n",
    "\n",
    "### Referências principais\n",
    "\n",
    "- Gmyrek, P., Berg, J. & Cappelli, D. (2025). *Generative AI and Jobs: An updated global assessment of potential effects on job quantity and quality*. ILO Working Paper 140.\n",
    "- IBGE. *Pesquisa Nacional por Amostra de Domicílios Contínua* (PNADc), 3º trimestre de 2025."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bab46382",
   "metadata": {},
   "source": [
    "### 1. Configuração do ambiente\n",
    "Definir caminhos, importar bibliotecas e configurar logs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "8bseat3wbbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m26.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3.10 -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Instalar dependências no kernel atual (executar apenas uma vez)\n",
    "%pip install pandas numpy pyarrow openpyxl basedosdados --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "80796a93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuração carregada com sucesso.\n",
      "  PNAD: 2025 Q3\n",
      "  Projeto GCP: mestrado-pnad-2026\n",
      "  Salário mínimo: R$ 1518\n",
      "  ILO file: data/input/Final_Scores_ISCO08_Gmyrek_et_al_2025.xlsx (existe: True)\n",
      "  Setores CNAE mapeados: 19 categorias\n",
      "  Setores críticos IA: ['Informação e Comunicação', 'Finanças e Seguros', 'Serviços Profissionais']\n"
     ]
    }
   ],
   "source": [
    "# Etapa 1.1 - Preparação de Dados - Configuração do ambiente\n",
    "\n",
    "import warnings\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import re\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# Caminhos (relativos ao diretório do notebook)\n",
    "# ---------------------------------------------------------------------------\n",
    "DATA_INPUT     = Path(\"data/input\")\n",
    "DATA_RAW       = Path(\"data/raw\")\n",
    "DATA_PROCESSED = Path(\"data/processed\")\n",
    "DATA_OUTPUT    = Path(\"data/output\")\n",
    "\n",
    "for d in [DATA_RAW, DATA_PROCESSED, DATA_OUTPUT]:\n",
    "    d.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# Parâmetros PNAD / GCP\n",
    "# ---------------------------------------------------------------------------\n",
    "GCP_PROJECT_ID  = \"mestrado-pnad-2026\"\n",
    "PNAD_ANO        = 2025\n",
    "PNAD_TRIMESTRE  = 3  \n",
    "SALARIO_MINIMO  = 1518  # Valor vigente em Q3/2025 (R$)\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# Arquivo ILO (já copiado para data/input)\n",
    "# ---------------------------------------------------------------------------\n",
    "ILO_FILE = DATA_INPUT / \"Final_Scores_ISCO08_Gmyrek_et_al_2025.xlsx\"\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# Mapeamentos\n",
    "# ---------------------------------------------------------------------------\n",
    "REGIAO_MAP = {\n",
    "    'RO': 'Norte', 'AC': 'Norte', 'AM': 'Norte', 'RR': 'Norte',\n",
    "    'PA': 'Norte', 'AP': 'Norte', 'TO': 'Norte',\n",
    "    'MA': 'Nordeste', 'PI': 'Nordeste', 'CE': 'Nordeste', 'RN': 'Nordeste',\n",
    "    'PB': 'Nordeste', 'PE': 'Nordeste', 'AL': 'Nordeste', 'SE': 'Nordeste', 'BA': 'Nordeste',\n",
    "    'MG': 'Sudeste', 'ES': 'Sudeste', 'RJ': 'Sudeste', 'SP': 'Sudeste',\n",
    "    'PR': 'Sul', 'SC': 'Sul', 'RS': 'Sul',\n",
    "    'MS': 'Centro-Oeste', 'MT': 'Centro-Oeste', 'GO': 'Centro-Oeste', 'DF': 'Centro-Oeste',\n",
    "}\n",
    "\n",
    "GRANDES_GRUPOS = {\n",
    "    '1': 'Dirigentes e gerentes',\n",
    "    '2': 'Profissionais das ciências',\n",
    "    '3': 'Técnicos nível médio',\n",
    "    '4': 'Apoio administrativo',\n",
    "    '5': 'Serviços e vendedores',\n",
    "    '6': 'Agropecuária qualificada',\n",
    "    '7': 'Indústria qualificada',\n",
    "    '8': 'Operadores de máquinas',\n",
    "    '9': 'Ocupações elementares',\n",
    "}\n",
    "\n",
    "RACA_AGREGADA_MAP = {\n",
    "    '1': 'Branca',\n",
    "    '2': 'Negra',   # Preta\n",
    "    '4': 'Negra',   # Parda\n",
    "    '3': 'Outras',  # Amarela\n",
    "    '5': 'Outras',  # Indígena\n",
    "    '9': 'Outras',  # Sem declaração\n",
    "}\n",
    "\n",
    "POSICAO_FORMAL = ['1', '3', '5']  # Empregado c/ carteira, Militar, Empregador\n",
    "\n",
    "IDADE_BINS   = [0, 25, 35, 45, 55, 100]\n",
    "IDADE_LABELS = ['18-24', '25-34', '35-44', '45-54', '55+']\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# Mapeamento CNAE Domiciliar 2.0 → Setor agregado\n",
    "# Seções A-T conforme classificação oficial IBGE.\n",
    "# Fonte: IBGE, Classificação Nacional de Atividades Econômicas (CNAE 2.0)\n",
    "# ---------------------------------------------------------------------------\n",
    "CNAE_SETOR_MAP = {\n",
    "    # A - Agropecuária\n",
    "    '01': 'Agropecuária', '02': 'Agropecuária', '03': 'Agropecuária',\n",
    "    # B - Indústria Extrativa\n",
    "    '05': 'Ind. Extrativa', '06': 'Ind. Extrativa', '07': 'Ind. Extrativa',\n",
    "    '08': 'Ind. Extrativa', '09': 'Ind. Extrativa',\n",
    "    # C - Indústria de Transformação\n",
    "    '10': 'Ind. Transformação', '11': 'Ind. Transformação', '12': 'Ind. Transformação',\n",
    "    '13': 'Ind. Transformação', '14': 'Ind. Transformação', '15': 'Ind. Transformação',\n",
    "    '16': 'Ind. Transformação', '17': 'Ind. Transformação', '18': 'Ind. Transformação',\n",
    "    '19': 'Ind. Transformação', '20': 'Ind. Transformação', '21': 'Ind. Transformação',\n",
    "    '22': 'Ind. Transformação', '23': 'Ind. Transformação', '24': 'Ind. Transformação',\n",
    "    '25': 'Ind. Transformação', '26': 'Ind. Transformação', '27': 'Ind. Transformação',\n",
    "    '28': 'Ind. Transformação', '29': 'Ind. Transformação', '30': 'Ind. Transformação',\n",
    "    '31': 'Ind. Transformação', '32': 'Ind. Transformação', '33': 'Ind. Transformação',\n",
    "    # D+E - Utilidades (Eletricidade, Gás, Água, Esgoto, Resíduos)\n",
    "    '35': 'Utilidades', '36': 'Utilidades', '37': 'Utilidades',\n",
    "    '38': 'Utilidades', '39': 'Utilidades',\n",
    "    # F - Construção\n",
    "    '41': 'Construção', '42': 'Construção', '43': 'Construção',\n",
    "    # G - Comércio\n",
    "    '45': 'Comércio', '46': 'Comércio', '47': 'Comércio',\n",
    "    # H - Transporte, Armazenagem e Correio\n",
    "    '49': 'Transporte', '50': 'Transporte', '51': 'Transporte',\n",
    "    '52': 'Transporte', '53': 'Transporte',\n",
    "    # I - Alojamento e Alimentação\n",
    "    '55': 'Alojamento e Alimentação', '56': 'Alojamento e Alimentação',\n",
    "    # J - Informação e Comunicação\n",
    "    '58': 'Informação e Comunicação', '59': 'Informação e Comunicação',\n",
    "    '60': 'Informação e Comunicação', '61': 'Informação e Comunicação',\n",
    "    '62': 'Informação e Comunicação', '63': 'Informação e Comunicação',\n",
    "    # K - Atividades Financeiras\n",
    "    '64': 'Finanças e Seguros', '65': 'Finanças e Seguros', '66': 'Finanças e Seguros',\n",
    "    # L - Atividades Imobiliárias\n",
    "    '68': 'Atividades Imobiliárias',\n",
    "    # M - Atividades Profissionais, Científicas e Técnicas\n",
    "    '69': 'Serviços Profissionais', '70': 'Serviços Profissionais',\n",
    "    '71': 'Serviços Profissionais', '72': 'Serviços Profissionais',\n",
    "    '73': 'Serviços Profissionais', '74': 'Serviços Profissionais',\n",
    "    '75': 'Serviços Profissionais',\n",
    "    # N - Atividades Administrativas e Serviços Complementares\n",
    "    '77': 'Serviços Administrativos', '78': 'Serviços Administrativos',\n",
    "    '79': 'Serviços Administrativos', '80': 'Serviços Administrativos',\n",
    "    '81': 'Serviços Administrativos', '82': 'Serviços Administrativos',\n",
    "    # O - Administração Pública\n",
    "    '84': 'Administração Pública',\n",
    "    # P - Educação\n",
    "    '85': 'Educação',\n",
    "    # Q - Saúde Humana e Serviços Sociais\n",
    "    '86': 'Saúde', '87': 'Saúde', '88': 'Saúde',\n",
    "    # R - Artes, Cultura, Esporte e Recreação\n",
    "    '90': 'Artes e Cultura', '91': 'Artes e Cultura',\n",
    "    '92': 'Artes e Cultura', '93': 'Artes e Cultura',\n",
    "    # S - Outras Atividades de Serviços\n",
    "    '94': 'Outros Serviços', '95': 'Outros Serviços', '96': 'Outros Serviços',\n",
    "    # T - Serviços Domésticos\n",
    "    '97': 'Serviços Domésticos',\n",
    "}\n",
    "\n",
    "# Setores com maior proporção de tarefas expostas à IA generativa\n",
    "# Ref: Gmyrek et al. (2024); Eloundou et al. (2023)\n",
    "SETORES_CRITICOS_IA = [\n",
    "    'Informação e Comunicação',\n",
    "    'Finanças e Seguros',\n",
    "    'Serviços Profissionais',\n",
    "]\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# Funções utilitárias – estatísticas ponderadas\n",
    "# ---------------------------------------------------------------------------\n",
    "def weighted_mean(values, weights):\n",
    "    \"\"\"Média ponderada (ignora NaN).\"\"\"\n",
    "    mask = ~(pd.isna(values) | pd.isna(weights))\n",
    "    if mask.sum() == 0:\n",
    "        return np.nan\n",
    "    return np.average(values[mask], weights=weights[mask])\n",
    "\n",
    "def weighted_std(values, weights):\n",
    "    \"\"\"Desvio-padrão ponderado (ignora NaN).\"\"\"\n",
    "    mask = ~(pd.isna(values) | pd.isna(weights))\n",
    "    if mask.sum() == 0:\n",
    "        return np.nan\n",
    "    avg = np.average(values[mask], weights=weights[mask])\n",
    "    variance = np.average((values[mask] - avg) ** 2, weights=weights[mask])\n",
    "    return np.sqrt(variance)\n",
    "\n",
    "def weighted_quantile(values, weights, quantile):\n",
    "    \"\"\"Quantil ponderado por pesos amostrais (ignora NaN).\n",
    "    Fonte: adaptado de etapa1_ia_generativa/src/utils/weighted_stats.py\n",
    "    \"\"\"\n",
    "    mask = ~(pd.isna(values) | pd.isna(weights))\n",
    "    if mask.sum() == 0:\n",
    "        return np.nan\n",
    "    sorted_idx = np.argsort(values[mask])\n",
    "    sorted_values = values[mask].iloc[sorted_idx]\n",
    "    sorted_weights = weights[mask].iloc[sorted_idx]\n",
    "    cumsum = np.cumsum(sorted_weights)\n",
    "    cutoff = quantile * cumsum.iloc[-1]\n",
    "    return sorted_values.iloc[np.searchsorted(cumsum, cutoff)]\n",
    "\n",
    "def weighted_qcut(values, weights, q, labels=None):\n",
    "    \"\"\"Classificação em quantis ponderados por peso amostral.\n",
    "\n",
    "    Diferente de pd.qcut (que divide por contagem de linhas), esta função\n",
    "    calcula os breakpoints de modo que cada faixa represente ~1/q da\n",
    "    POPULAÇÃO (soma dos pesos), não da amostra.\n",
    "\n",
    "    Parâmetros:\n",
    "        values  : pd.Series com os valores a classificar\n",
    "        weights : pd.Series com os pesos amostrais\n",
    "        q       : int, número de quantis (5 = quintis, 10 = decis)\n",
    "        labels  : lista de labels (len == q), ou None para retornar inteiros 1..q\n",
    "\n",
    "    Retorna:\n",
    "        pd.Series (Categorical) com os labels atribuídos\n",
    "    \"\"\"\n",
    "    mask = values.notna() & weights.notna()\n",
    "    breakpoints = [values[mask].min() - 1e-10]  # incluir mínimo\n",
    "    for i in range(1, q):\n",
    "        bp = weighted_quantile(values[mask], weights[mask], i / q)\n",
    "        breakpoints.append(bp)\n",
    "    breakpoints.append(values[mask].max() + 1e-10)  # incluir máximo\n",
    "\n",
    "    # Remover duplicatas mantendo ordem (pode acontecer com valores concentrados)\n",
    "    breakpoints = sorted(set(breakpoints))\n",
    "\n",
    "    if labels is not None and len(labels) != len(breakpoints) - 1:\n",
    "        labels = None  # fallback se breakpoints colapsaram\n",
    "\n",
    "    result = pd.cut(values, bins=breakpoints, labels=labels, include_lowest=True)\n",
    "    return result\n",
    "\n",
    "print(\"Configuração carregada com sucesso.\")\n",
    "print(f\"  PNAD: {PNAD_ANO} Q{PNAD_TRIMESTRE}\")\n",
    "print(f\"  Projeto GCP: {GCP_PROJECT_ID}\")\n",
    "print(f\"  Salário mínimo: R$ {SALARIO_MINIMO}\")\n",
    "print(f\"  ILO file: {ILO_FILE} (existe: {ILO_FILE.exists()})\")\n",
    "print(f\"  Setores CNAE mapeados: {len(set(CNAE_SETOR_MAP.values()))} categorias\")\n",
    "print(f\"  Setores críticos IA: {SETORES_CRITICOS_IA}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd41833d",
   "metadata": {},
   "source": [
    "### 2a. Download dos microdados PNAD\n",
    "Extrair da PNAD Contínua (BigQuery) as variáveis necessárias para o trimestre/ano definido.\n",
    "**Saída:** `data/raw/pnad_*.parquet`\n",
    "\n",
    "#### Ficha técnica dos dados\n",
    "\n",
    "| Item | Descrição |\n",
    "|------|-----------|\n",
    "| **Fonte** | PNAD Contínua (PNADc), IBGE |\n",
    "| **Período** | 3º trimestre de 2025 |\n",
    "| **Acesso** | Base dos Dados (BigQuery) |\n",
    "| **Peso amostral** | V1028 (projeção de população para dados trimestrais) |\n",
    "| **Universo** | População ocupada com código de ocupação válido |\n",
    "\n",
    "#### Variáveis selecionadas\n",
    "\n",
    "| Variável IBGE | Nome no dataset | Descrição |\n",
    "|---------------|-----------------|-----------|\n",
    "| V2007 | `sexo` | Sexo biológico |\n",
    "| V2009 | `idade` | Idade em anos |\n",
    "| V2010 | `raca_cor` | Cor ou raça (autoclassificação) |\n",
    "| VD3004 | `nivel_instrucao` | Nível de instrução mais elevado alcançado |\n",
    "| V4010 | `cod_ocupacao` | Código de ocupação (COD, 4 dígitos) |\n",
    "| V4013 | `grupamento_atividade` | Grupamento de atividade (CNAE Domiciliar 2.0) |\n",
    "| VD4009 | `posicao_ocupacao` | Posição na ocupação |\n",
    "| VD4016 | `rendimento_habitual` | Rendimento mensal habitual do trabalho principal |\n",
    "| VD4020 | `rendimento_efetivo` | Rendimento mensal efetivo do trabalho principal |\n",
    "| VD4031 | `horas_habituais` | Horas habitualmente trabalhadas (todos os trabalhos) |\n",
    "| VD4035 | `horas_efetivas` | Horas efetivamente trabalhadas na semana de referência |\n",
    "| V1028 | `peso` | Peso amostral (projeção de população) |\n",
    "\n",
    "> **Nota metodológica — Variáveis de renda:** O rendimento mensal habitual (`VD4016`) é a \n",
    "> variável primária para análises estruturais de exposição ocupacional, por ser menos volátil \n",
    "> que o rendimento efetivo (`VD4020`), que captura flutuações mensais por horas extras, bônus, \n",
    "> etc. (Cf. IBGE, Notas Metodológicas PNAD Contínua, 2023). Ambas são mantidas na base.\n",
    "\n",
    "> **Nota metodológica — Horas trabalhadas:** Utiliza-se `VD4031` (horas habitualmente \n",
    "> trabalhadas em todos os trabalhos, variável derivada IBGE) como variável principal de jornada, \n",
    "> por ter cobertura superior à variável bruta `V4019` (~30% de preenchimento na versão anterior). \n",
    "> `VD4035` (horas efetivamente trabalhadas na semana de referência) é incluída para análises \n",
    "> de sazonalidade e produtividade.\n",
    "\n",
    "> **Nota metodológica — Inclusão de todos os ocupados:** A query inclui **todos os ocupados \n",
    "> com código de ocupação válido**, independentemente de terem renda declarada. O filtro de renda \n",
    "> é aplicado via flag `tem_renda` na etapa de limpeza (4a).\n",
    "\n",
    "> **Nota metodológica — Variáveis indisponíveis:** As variáveis `V4040` (tempo no emprego atual) \n",
    "> e `V4018` (porte da empresa) não estão populadas na fonte utilizada (Base dos Dados/BigQuery) \n",
    "> para o período analisado (Q3/2025), sendo portanto excluídas desta análise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "1dae0a2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arquivo PNAD encontrado localmente: pnad_2025q3.parquet\n",
      "Carregado: 220,091 observações\n",
      "  OK: Arquivo corresponde à configuração (2025 Q3)\n",
      "\n",
      "df_pnad_raw: 220,091 linhas x 15 colunas\n",
      "Período: 2025 Q3\n"
     ]
    }
   ],
   "source": [
    "# Etapa 1.2a - Preparação de Dados - Download dos microdados PNAD\n",
    "# Lógica: se o parquet já existe em data/raw/, carrega direto; senão, baixa do BigQuery.\n",
    "\n",
    "pnad_files = sorted(DATA_RAW.glob(\"pnad_*.parquet\"))\n",
    "\n",
    "if pnad_files:\n",
    "    # --- Caminho rápido: arquivo local já disponível ---\n",
    "    pnad_path = pnad_files[-1]  # mais recente\n",
    "    print(f\"Arquivo PNAD encontrado localmente: {pnad_path.name}\")\n",
    "    df_pnad_raw = pd.read_parquet(pnad_path)\n",
    "    print(f\"Carregado: {len(df_pnad_raw):,} observações\")\n",
    "\n",
    "    # Validar que o arquivo corresponde à configuração\n",
    "    match = re.search(r\"pnad_(\\d{4})q(\\d)\", pnad_path.name)\n",
    "    if match:\n",
    "        ano_arquivo, trim_arquivo = int(match.group(1)), int(match.group(2))\n",
    "        if ano_arquivo != PNAD_ANO or trim_arquivo != PNAD_TRIMESTRE:\n",
    "            print(f\"  WARNING: Arquivo é {ano_arquivo} Q{trim_arquivo}, \"\n",
    "                  f\"mas config diz {PNAD_ANO} Q{PNAD_TRIMESTRE}!\")\n",
    "            print(f\"  Atualizando variáveis de config para corresponder aos dados.\")\n",
    "            PNAD_ANO = ano_arquivo\n",
    "            PNAD_TRIMESTRE = trim_arquivo\n",
    "        else:\n",
    "            print(f\"  OK: Arquivo corresponde à configuração ({PNAD_ANO} Q{PNAD_TRIMESTRE})\")\n",
    "\n",
    "else:\n",
    "    # --- Caminho completo: download via BigQuery ---\n",
    "    print(\"Nenhum arquivo PNAD local encontrado. Iniciando download do BigQuery...\")\n",
    "    import basedosdados as bd\n",
    "\n",
    "    # Verificar trimestres disponíveis\n",
    "    query_check = \"\"\"\n",
    "    SELECT DISTINCT ano, trimestre, COUNT(*) as n_obs\n",
    "    FROM `basedosdados.br_ibge_pnadc.microdados`\n",
    "    WHERE ano >= 2024\n",
    "    GROUP BY ano, trimestre\n",
    "    ORDER BY ano DESC, trimestre DESC\n",
    "    LIMIT 5\n",
    "    \"\"\"\n",
    "    df_check = bd.read_sql(query_check, billing_project_id=GCP_PROJECT_ID)\n",
    "    print(f\"Trimestres disponíveis:\\n{df_check}\")\n",
    "\n",
    "    trimestre_existe = len(\n",
    "        df_check[(df_check['ano'] == PNAD_ANO) & (df_check['trimestre'] == PNAD_TRIMESTRE)]\n",
    "    ) > 0\n",
    "\n",
    "    if trimestre_existe:\n",
    "        ano_usar, trim_usar = PNAD_ANO, PNAD_TRIMESTRE\n",
    "    else:\n",
    "        ano_usar = int(df_check.iloc[0]['ano'])\n",
    "        trim_usar = int(df_check.iloc[0]['trimestre'])\n",
    "        print(f\"AVISO: {PNAD_ANO} Q{PNAD_TRIMESTRE} indisponível. Usando {ano_usar} Q{trim_usar}\")\n",
    "        PNAD_ANO = ano_usar\n",
    "        PNAD_TRIMESTRE = trim_usar\n",
    "\n",
    "    query = f\"\"\"\n",
    "    SELECT\n",
    "        ano,\n",
    "        trimestre,\n",
    "        sigla_uf,\n",
    "        v2007  AS sexo,\n",
    "        v2009  AS idade,\n",
    "        v2010  AS raca_cor,\n",
    "        vd3004 AS nivel_instrucao,\n",
    "        v4010  AS cod_ocupacao,\n",
    "        v4013  AS grupamento_atividade,\n",
    "        vd4009 AS posicao_ocupacao,\n",
    "        vd4016 AS rendimento_habitual,\n",
    "        vd4020 AS rendimento_efetivo,\n",
    "        vd4031 AS horas_habituais,\n",
    "        vd4035 AS horas_efetivas,\n",
    "        v1028  AS peso\n",
    "    FROM `basedosdados.br_ibge_pnadc.microdados`\n",
    "    WHERE ano = {ano_usar}\n",
    "      AND trimestre = {trim_usar}\n",
    "      AND v4010 IS NOT NULL\n",
    "    \"\"\"\n",
    "\n",
    "    print(f\"Executando query para {ano_usar} Q{trim_usar} (pode demorar 2-5 min)...\")\n",
    "    df_pnad_raw = bd.read_sql(query, billing_project_id=GCP_PROJECT_ID)\n",
    "\n",
    "    # Salvar parquet\n",
    "    ano_real = int(df_pnad_raw['ano'].iloc[0])\n",
    "    trim_real = int(df_pnad_raw['trimestre'].iloc[0])\n",
    "    output_path = DATA_RAW / f\"pnad_{ano_real}q{trim_real}.parquet\"\n",
    "    df_pnad_raw.to_parquet(output_path, index=False)\n",
    "    print(f\"Salvo em: {output_path}\")\n",
    "\n",
    "print(f\"\\ndf_pnad_raw: {df_pnad_raw.shape[0]:,} linhas x {df_pnad_raw.shape[1]} colunas\")\n",
    "print(f\"Período: {PNAD_ANO} Q{PNAD_TRIMESTRE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b9f4041",
   "metadata": {},
   "source": [
    "### 2b. Verificar dados microdados PNAD (CHECKPOINT)\n",
    "Verificar dados gerados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "4a02ceeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "CHECKPOINT - Microdados PNAD\n",
      "============================================================\n",
      "\n",
      "Shape: (220091, 15)\n",
      "Colunas: ['ano', 'trimestre', 'sigla_uf', 'sexo', 'idade', 'raca_cor', 'nivel_instrucao', 'cod_ocupacao', 'grupamento_atividade', 'posicao_ocupacao', 'rendimento_habitual', 'rendimento_efetivo', 'horas_habituais', 'horas_efetivas', 'peso']\n",
      "\n",
      "UFs presentes: 27\n",
      "População representada: 102.4 milhões\n",
      "\n",
      "Preenchimento das variáveis:\n",
      "   ano: 220,091 (100.0%)\n",
      "   trimestre: 220,091 (100.0%)\n",
      "   sigla_uf: 220,091 (100.0%)\n",
      "   sexo: 220,091 (100.0%)\n",
      "   idade: 220,091 (100.0%)\n",
      "   raca_cor: 220,091 (100.0%)\n",
      "   nivel_instrucao: 220,091 (100.0%)\n",
      "   cod_ocupacao: 220,091 (100.0%)\n",
      "   grupamento_atividade: 220,091 (100.0%)\n",
      "   posicao_ocupacao: 220,091 (100.0%)\n",
      "   rendimento_habitual: 215,370 (97.9%)\n",
      "   rendimento_efetivo: 215,405 (97.9%)\n",
      "   horas_habituais: 220,091 (100.0%)\n",
      "   horas_efetivas: 220,091 (100.0%)\n",
      "   peso: 220,091 (100.0%)\n",
      "\n",
      "Dtypes:\n",
      "ano                       Int64\n",
      "trimestre                 Int64\n",
      "sigla_uf                 object\n",
      "sexo                     object\n",
      "idade                     Int64\n",
      "raca_cor                 object\n",
      "nivel_instrucao          object\n",
      "cod_ocupacao             object\n",
      "grupamento_atividade     object\n",
      "posicao_ocupacao         object\n",
      "rendimento_habitual     float64\n",
      "rendimento_efetivo      float64\n",
      "horas_habituais           Int64\n",
      "horas_efetivas            Int64\n",
      "peso                    float64\n",
      "dtype: object\n",
      "\n",
      "Primeiras linhas:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ano</th>\n",
       "      <th>trimestre</th>\n",
       "      <th>sigla_uf</th>\n",
       "      <th>sexo</th>\n",
       "      <th>idade</th>\n",
       "      <th>raca_cor</th>\n",
       "      <th>nivel_instrucao</th>\n",
       "      <th>cod_ocupacao</th>\n",
       "      <th>grupamento_atividade</th>\n",
       "      <th>posicao_ocupacao</th>\n",
       "      <th>rendimento_habitual</th>\n",
       "      <th>rendimento_efetivo</th>\n",
       "      <th>horas_habituais</th>\n",
       "      <th>horas_efetivas</th>\n",
       "      <th>peso</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2025</td>\n",
       "      <td>3</td>\n",
       "      <td>RR</td>\n",
       "      <td>1</td>\n",
       "      <td>47</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>8322</td>\n",
       "      <td>49030</td>\n",
       "      <td>9</td>\n",
       "      <td>3000.0</td>\n",
       "      <td>2800.0</td>\n",
       "      <td>28</td>\n",
       "      <td>22</td>\n",
       "      <td>80.230030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2025</td>\n",
       "      <td>3</td>\n",
       "      <td>DF</td>\n",
       "      <td>2</td>\n",
       "      <td>35</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>4120</td>\n",
       "      <td>78000</td>\n",
       "      <td>1</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>3600.0</td>\n",
       "      <td>80</td>\n",
       "      <td>80</td>\n",
       "      <td>273.396289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2025</td>\n",
       "      <td>3</td>\n",
       "      <td>SE</td>\n",
       "      <td>1</td>\n",
       "      <td>62</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>5414</td>\n",
       "      <td>85012</td>\n",
       "      <td>7</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>18</td>\n",
       "      <td>18</td>\n",
       "      <td>216.050334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2025</td>\n",
       "      <td>3</td>\n",
       "      <td>SE</td>\n",
       "      <td>2</td>\n",
       "      <td>40</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>5221</td>\n",
       "      <td>56011</td>\n",
       "      <td>8</td>\n",
       "      <td>5000.0</td>\n",
       "      <td>5000.0</td>\n",
       "      <td>52</td>\n",
       "      <td>52</td>\n",
       "      <td>275.005513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2025</td>\n",
       "      <td>3</td>\n",
       "      <td>SE</td>\n",
       "      <td>2</td>\n",
       "      <td>34</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>5212</td>\n",
       "      <td>56020</td>\n",
       "      <td>9</td>\n",
       "      <td>1200.0</td>\n",
       "      <td>1200.0</td>\n",
       "      <td>26</td>\n",
       "      <td>26</td>\n",
       "      <td>201.275603</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    ano  trimestre sigla_uf sexo  idade raca_cor nivel_instrucao cod_ocupacao  \\\n",
       "0  2025          3       RR    1     47        4               4         8322   \n",
       "1  2025          3       DF    2     35        1               5         4120   \n",
       "2  2025          3       SE    1     62        4               2         5414   \n",
       "3  2025          3       SE    2     40        4               5         5221   \n",
       "4  2025          3       SE    2     34        4               5         5212   \n",
       "\n",
       "  grupamento_atividade posicao_ocupacao  rendimento_habitual  \\\n",
       "0                49030                9               3000.0   \n",
       "1                78000                1               2000.0   \n",
       "2                85012                7               2000.0   \n",
       "3                56011                8               5000.0   \n",
       "4                56020                9               1200.0   \n",
       "\n",
       "   rendimento_efetivo  horas_habituais  horas_efetivas        peso  \n",
       "0              2800.0               28              22   80.230030  \n",
       "1              3600.0               80              80  273.396289  \n",
       "2              2000.0               18              18  216.050334  \n",
       "3              5000.0               52              52  275.005513  \n",
       "4              1200.0               26              26  201.275603  "
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Etapa 1.2b - Preparação de Dados - Verificar dados microdados PNAD\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"CHECKPOINT - Microdados PNAD\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"\\nShape: {df_pnad_raw.shape}\")\n",
    "print(f\"Colunas: {list(df_pnad_raw.columns)}\")\n",
    "\n",
    "# UFs\n",
    "n_ufs = df_pnad_raw['sigla_uf'].nunique()\n",
    "print(f\"\\nUFs presentes: {n_ufs}\")\n",
    "if n_ufs != 27:\n",
    "    print(f\"  WARNING: Esperado 27 UFs, encontrado {n_ufs}\")\n",
    "\n",
    "# População\n",
    "pop_milhoes = df_pnad_raw['peso'].sum() / 1e6\n",
    "print(f\"População representada: {pop_milhoes:.1f} milhões\")\n",
    "\n",
    "# Linhas\n",
    "if len(df_pnad_raw) < 100_000:\n",
    "    print(f\"  WARNING: Apenas {len(df_pnad_raw):,} linhas (esperado > 100.000)\")\n",
    "\n",
    "# Verificar preenchimento das variáveis-chave\n",
    "print(f\"\\nPreenchimento das variáveis:\")\n",
    "for col in df_pnad_raw.columns:\n",
    "    n_valid = df_pnad_raw[col].notna().sum()\n",
    "    pct = n_valid / len(df_pnad_raw) * 100\n",
    "    flag = \"  \" if pct > 80 else \"  WARNING -\" if pct > 50 else \"  CRITICO -\"\n",
    "    print(f\"{flag} {col}: {n_valid:,} ({pct:.1f}%)\")\n",
    "\n",
    "# Tipos\n",
    "print(f\"\\nDtypes:\\n{df_pnad_raw.dtypes}\")\n",
    "\n",
    "# Amostra\n",
    "print(\"\\nPrimeiras linhas:\")\n",
    "df_pnad_raw.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c102868b",
   "metadata": {},
   "source": [
    "### 3a. Processar índice de exposição ILO\n",
    "Lê a planilha ILO com scores de exposição por ISCO-08, padroniza e gera níveis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "67346cc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lendo arquivo ILO: data/input/Final_Scores_ISCO08_Gmyrek_et_al_2025.xlsx\n",
      "Linhas raw (tarefas): 3,265\n",
      "Colunas disponíveis: ['label4d', 'label1d', 'ISCO_08', 'Title', 'taskID', 'Task_ISCO', 'score_2023', 'Weaviate Status', 'predicted_score_2025_gpt4o', 'prediction_justification_gpt4o', 'weaviate_status_gemini', 'predicted_score_2025_gemini', 'prediction_justification_gemini', 'score_2025', 'source', 'mean_score_2023', 'mean_score_2025', 'SD_2023', 'SD_2025', 'potential25', 'potential23']\n",
      "Colunas mapeadas: ['ISCO_08', 'Title', 'mean_score_2025', 'SD_2025', 'potential25']\n",
      "\n",
      "Ocupações únicas: 427\n",
      "Score médio: 0.297\n",
      "Score range: [0.090, 0.700]\n",
      "\n",
      "Salvo em: data/processed/ilo_exposure_clean.csv\n"
     ]
    }
   ],
   "source": [
    "# Etapa 1.3a - Preparação de Dados - Processar índice de exposição ILO\n",
    "\n",
    "print(f\"Lendo arquivo ILO: {ILO_FILE}\")\n",
    "df_ilo_raw = pd.read_excel(ILO_FILE)\n",
    "print(f\"Linhas raw (tarefas): {len(df_ilo_raw):,}\")\n",
    "print(f\"Colunas disponíveis: {list(df_ilo_raw.columns)}\")\n",
    "\n",
    "# Mapeamento de colunas\n",
    "col_mapping = {\n",
    "    'ISCO_08': 'isco_08',\n",
    "    'Title': 'occupation_title',\n",
    "    'mean_score_2025': 'exposure_score',\n",
    "    'SD_2025': 'exposure_sd',\n",
    "    'potential25': 'exposure_gradient',\n",
    "}\n",
    "\n",
    "available_cols = [c for c in col_mapping.keys() if c in df_ilo_raw.columns]\n",
    "print(f\"Colunas mapeadas: {available_cols}\")\n",
    "\n",
    "df_ilo_renamed = df_ilo_raw.rename(\n",
    "    columns={k: v for k, v in col_mapping.items() if k in df_ilo_raw.columns}\n",
    ")\n",
    "\n",
    "# Agregar por ocupação (arquivo original tem múltiplas tarefas por ocupação)\n",
    "df_ilo = df_ilo_renamed.groupby('isco_08').agg({\n",
    "    'occupation_title': 'first',\n",
    "    'exposure_score': 'mean',\n",
    "    'exposure_sd': 'mean',\n",
    "    'exposure_gradient': 'first',\n",
    "}).reset_index()\n",
    "\n",
    "# Garantir formato string com 4 dígitos\n",
    "df_ilo['isco_08_str'] = df_ilo['isco_08'].astype(str).str.zfill(4)\n",
    "\n",
    "print(f\"\\nOcupações únicas: {len(df_ilo):,}\")\n",
    "print(f\"Score médio: {df_ilo['exposure_score'].mean():.3f}\")\n",
    "print(f\"Score range: [{df_ilo['exposure_score'].min():.3f}, {df_ilo['exposure_score'].max():.3f}]\")\n",
    "\n",
    "# Salvar processado\n",
    "ilo_output = DATA_PROCESSED / \"ilo_exposure_clean.csv\"\n",
    "df_ilo.to_csv(ilo_output, index=False)\n",
    "print(f\"\\nSalvo em: {ilo_output}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d914cc02",
   "metadata": {},
   "source": [
    "### 3b. Verificar índice de exposição ILO\n",
    "Verificar: número de ocupações, coluna de score, distribuição por gradiente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "3fae4169",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "CHECKPOINT - Índice ILO\n",
      "============================================================\n",
      "\n",
      "Ocupações: 427\n",
      "Score range: [0.090, 0.700]\n",
      "\n",
      "Distribuição por gradiente:\n",
      "  Not Exposed: 231 ocupações\n",
      "  Minimal Exposure: 84 ocupações\n",
      "  Exposed: Gradient 2: 44 ocupações\n",
      "  Exposed: Gradient 3: 38 ocupações\n",
      "  Exposed: Gradient 1: 17 ocupações\n",
      "  Exposed: Gradient 4: 13 ocupações\n",
      "\n",
      "Amostra (5 maiores scores):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>isco_08_str</th>\n",
       "      <th>occupation_title</th>\n",
       "      <th>exposure_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>207</th>\n",
       "      <td>4132</td>\n",
       "      <td>Data Entry Clerks</td>\n",
       "      <td>0.70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>206</th>\n",
       "      <td>4131</td>\n",
       "      <td>Typists and Word Processing Operators</td>\n",
       "      <td>0.65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>220</th>\n",
       "      <td>4311</td>\n",
       "      <td>Accounting and Bookkeeping Clerks</td>\n",
       "      <td>0.64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>221</th>\n",
       "      <td>4312</td>\n",
       "      <td>Statistical, Finance and Insurance Clerks</td>\n",
       "      <td>0.64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>164</th>\n",
       "      <td>3311</td>\n",
       "      <td>Securities and Finance Dealers and Brokers</td>\n",
       "      <td>0.63</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    isco_08_str                            occupation_title  exposure_score\n",
       "207        4132                           Data Entry Clerks            0.70\n",
       "206        4131       Typists and Word Processing Operators            0.65\n",
       "220        4311           Accounting and Bookkeeping Clerks            0.64\n",
       "221        4312   Statistical, Finance and Insurance Clerks            0.64\n",
       "164        3311  Securities and Finance Dealers and Brokers            0.63"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Etapa 1.3b - Preparação de Dados - Verificar índice de exposição ILO\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"CHECKPOINT - Índice ILO\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Número de ocupações\n",
    "n_ocup = len(df_ilo)\n",
    "print(f\"\\nOcupações: {n_ocup}\")\n",
    "if n_ocup < 400:\n",
    "    print(f\"  WARNING: Poucas ocupações ({n_ocup}). Esperado ~427.\")\n",
    "\n",
    "# Range de scores\n",
    "score_min = df_ilo['exposure_score'].min()\n",
    "score_max = df_ilo['exposure_score'].max()\n",
    "print(f\"Score range: [{score_min:.3f}, {score_max:.3f}]\")\n",
    "if score_min < 0 or score_max > 1:\n",
    "    print(f\"  WARNING: Scores fora do intervalo [0, 1]\")\n",
    "\n",
    "# Distribuição por gradiente\n",
    "print(\"\\nDistribuição por gradiente:\")\n",
    "for grad, count in df_ilo['exposure_gradient'].value_counts().items():\n",
    "    print(f\"  {grad}: {count} ocupações\")\n",
    "\n",
    "# Amostra\n",
    "print(\"\\nAmostra (5 maiores scores):\")\n",
    "df_ilo.nlargest(5, 'exposure_score')[['isco_08_str', 'occupation_title', 'exposure_score']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "teqyrtn151e",
   "metadata": {},
   "source": [
    "### Notas sobre terminologia\n",
    "\n",
    "> **Sobre sexo:** A PNADc coleta a variável V2007 (sexo biológico: masculino/feminino). Esta pesquisa \n",
    "> não coleta identidade de gênero. Utilizamos o termo \"sexo\" ao longo desta análise, em conformidade \n",
    "> com a terminologia do IBGE.\n",
    "\n",
    "> **Sobre raça/cor:** Utilizamos a variável V2010 (autoclassificação de cor ou raça) com as cinco \n",
    "> categorias do IBGE: Branca, Preta, Parda, Amarela e Indígena. Quando apresentamos resultados \n",
    "> agregados em \"Negros\" (Pretos + Pardos), seguimos a convenção amplamente adotada na sociologia \n",
    "> e economia do trabalho brasileira (Osorio, 2003; Soares, 2008). Resultados desagregados estão \n",
    "> disponíveis nos apêndices.\n",
    "\n",
    "> **Sobre \"exposição\":** O índice da OIT mede o potencial de que tarefas ocupacionais sejam afetadas \n",
    "> pela IA generativa — seja por automação, seja por complementação/aumento de produtividade. \n",
    "> \"Exposição\" não é sinônimo de \"risco de desemprego\" ou \"ameaça\". Ocupações altamente expostas \n",
    "> podem tanto perder tarefas quanto ganhar produtividade, dependendo do contexto institucional, \n",
    "> regulatório e organizacional."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e01ab1f0",
   "metadata": {},
   "source": [
    "### 4a. Limpeza e variáveis derivadas – PNAD\n",
    "\n",
    "Filtra população de interesse, cria variáveis derivadas (região, grandes grupos COD, faixas de renda, etc.) e padroniza códigos de ocupação.\n",
    "\n",
    "**Entrada:** `data/raw/pnad_*.parquet`.  \n",
    "**Saída:** `data/processed/pnad_clean.csv`\n",
    "\n",
    "> **Nota metodológica — Inclusão de todos os ocupados:** A análise de exposição inclui todos \n",
    "> os ocupados com código de ocupação válido, **independentemente de terem renda declarada**. \n",
    "> A variável `tem_renda` sinaliza trabalhadores com rendimento habitual positivo. Para análises \n",
    "> de rendimento (tabelas salariais, faixas de renda), filtrar por `tem_renda == 1`.\n",
    "\n",
    "> **Nota metodológica — Faixas de renda em salários mínimos:** Optamos por classificar a \n",
    "> renda em faixas de salários mínimos (até 1 SM, 1-2 SM, 2-3 SM, 3-5 SM, 5+ SM) em vez de \n",
    "> quintis populacionais. Esta escolha se justifica por: (1) a elevada concentração de \n",
    "> rendimentos em torno de 1 SM no Brasil gera empates que distorcem os quintis \n",
    "> (Q1 absorveria ~31% da população); (2) faixas em SM são mais interpretáveis e amplamente \n",
    "> utilizadas na literatura brasileira de economia do trabalho.\n",
    "\n",
    "> **Nota metodológica — Winsorização:** Aplicamos winsorização nos percentis 1 e 99 da \n",
    "> distribuição de rendimento habitual, calculados com pesos amostrais (V1028), para limitar \n",
    "> a influência de valores extremos preservando o tamanho amostral. Esta técnica é preferível \n",
    "> ao trimming (que descarta observações) e é prática padrão em análises de renda com dados \n",
    "> de survey."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "5a0dfa0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observações iniciais: 220,091\n",
      "Após remover missings críticos: 220,091 (100.0%)\n",
      "Após filtrar 18-65 anos: 207,919 (94.5%)\n",
      "Após remover ocupações inválidas: 207,901\n",
      "\n",
      "Trabalhadores sem renda declarada: 3,759 obs (1.1 milhões)\n",
      "Taxa de formalidade: 37.0%\n",
      "Winsorização ponderada: P1 = R$ 200, P99 = R$ 21,000\n",
      "\n",
      "Distribuição por faixa de renda (SM = R$ 1518):\n",
      "  Até 1 SM: 30.3 milhões (31.3%)\n",
      "  1-2 SM: 39.0 milhões (40.4%)\n",
      "  2-3 SM: 10.1 milhões (10.5%)\n",
      "  3-5 SM: 9.5 milhões (9.8%)\n",
      "  5+ SM: 7.8 milhões (8.0%)\n",
      "\n",
      "Horas habituais: 207,901 (100.0%)\n",
      "Horas efetivas:  207,901 (100.0%)\n",
      "\n",
      "Salvo em: data/processed/pnad_clean.csv\n",
      "df_pnad: 207,901 linhas x 24 colunas\n"
     ]
    }
   ],
   "source": [
    "# Etapa 1.4a - Preparação de Dados - Limpeza e variáveis derivadas\n",
    "\n",
    "df_pnad = df_pnad_raw.copy()\n",
    "n_inicial = len(df_pnad)\n",
    "print(f\"Observações iniciais: {n_inicial:,}\")\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# LIMPEZA - Conversão de tipos\n",
    "# ---------------------------------------------------------------------------\n",
    "df_pnad['cod_ocupacao'] = df_pnad['cod_ocupacao'].astype(str).str.zfill(4)\n",
    "df_pnad['idade'] = pd.to_numeric(df_pnad['idade'], errors='coerce')\n",
    "df_pnad['rendimento_habitual'] = pd.to_numeric(df_pnad['rendimento_habitual'], errors='coerce')\n",
    "df_pnad['rendimento_efetivo'] = pd.to_numeric(df_pnad['rendimento_efetivo'], errors='coerce')\n",
    "df_pnad['horas_habituais'] = pd.to_numeric(df_pnad['horas_habituais'], errors='coerce')\n",
    "df_pnad['horas_efetivas'] = pd.to_numeric(df_pnad['horas_efetivas'], errors='coerce')\n",
    "df_pnad['peso'] = pd.to_numeric(df_pnad['peso'], errors='coerce')\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# LIMPEZA - Filtros\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "# Remover missings críticos (ocupação, idade, peso — NÃO renda)\n",
    "df_pnad = df_pnad.dropna(subset=['cod_ocupacao', 'idade', 'peso'])\n",
    "print(f\"Após remover missings críticos: {len(df_pnad):,} ({len(df_pnad)/n_inicial:.1%})\")\n",
    "\n",
    "# Filtrar faixa etária (18-65)\n",
    "df_pnad = df_pnad[(df_pnad['idade'] >= 18) & (df_pnad['idade'] <= 65)]\n",
    "print(f\"Após filtrar 18-65 anos: {len(df_pnad):,} ({len(df_pnad)/n_inicial:.1%})\")\n",
    "\n",
    "# Remover ocupações inválidas\n",
    "df_pnad = df_pnad[~df_pnad['cod_ocupacao'].isin(['0000', '9999'])]\n",
    "print(f\"Após remover ocupações inválidas: {len(df_pnad):,}\")\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# VARIÁVEIS DERIVADAS\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "# Flag de renda (em vez de excluir sem renda)\n",
    "df_pnad['tem_renda'] = (df_pnad['rendimento_habitual'].notna() & (df_pnad['rendimento_habitual'] > 0)).astype(int)\n",
    "n_sem_renda = (df_pnad['tem_renda'] == 0).sum()\n",
    "pop_sem_renda = df_pnad.loc[df_pnad['tem_renda'] == 0, 'peso'].sum() / 1e6\n",
    "print(f\"\\nTrabalhadores sem renda declarada: {n_sem_renda:,} obs ({pop_sem_renda:.1f} milhões)\")\n",
    "\n",
    "# Formalidade\n",
    "df_pnad['formal'] = df_pnad['posicao_ocupacao'].astype(str).isin(POSICAO_FORMAL).astype(int)\n",
    "print(f\"Taxa de formalidade: {df_pnad['formal'].mean():.1%}\")\n",
    "\n",
    "# Faixas etárias\n",
    "df_pnad['faixa_etaria'] = pd.cut(\n",
    "    df_pnad['idade'], bins=IDADE_BINS, labels=IDADE_LABELS\n",
    ")\n",
    "\n",
    "# Região\n",
    "df_pnad['regiao'] = df_pnad['sigla_uf'].map(REGIAO_MAP)\n",
    "\n",
    "# Raça agregada\n",
    "df_pnad['raca_agregada'] = df_pnad['raca_cor'].astype(str).map(RACA_AGREGADA_MAP)\n",
    "\n",
    "# Grande grupo ocupacional\n",
    "df_pnad['grande_grupo'] = df_pnad['cod_ocupacao'].str[0].map(GRANDES_GRUPOS)\n",
    "\n",
    "# Sexo como texto\n",
    "df_pnad['sexo_texto'] = df_pnad['sexo'].map({1: 'Homem', 2: 'Mulher', '1': 'Homem', '2': 'Mulher'})\n",
    "\n",
    "# Winsorização de renda (percentis ponderados 1 e 99) — APENAS para quem tem renda\n",
    "mask_renda = df_pnad['tem_renda'] == 1\n",
    "p01 = weighted_quantile(\n",
    "    df_pnad.loc[mask_renda, 'rendimento_habitual'],\n",
    "    df_pnad.loc[mask_renda, 'peso'], 0.01\n",
    ")\n",
    "p99 = weighted_quantile(\n",
    "    df_pnad.loc[mask_renda, 'rendimento_habitual'],\n",
    "    df_pnad.loc[mask_renda, 'peso'], 0.99\n",
    ")\n",
    "df_pnad['rendimento_winsor'] = df_pnad['rendimento_habitual'].clip(lower=p01, upper=p99)\n",
    "print(f\"Winsorização ponderada: P1 = R$ {p01:,.0f}, P99 = R$ {p99:,.0f}\")\n",
    "\n",
    "# Faixas de renda em salários mínimos\n",
    "df_pnad['faixa_renda_sm'] = pd.cut(\n",
    "    df_pnad['rendimento_habitual'] / SALARIO_MINIMO,\n",
    "    bins=[0, 1, 2, 3, 5, float('inf')],\n",
    "    labels=['Até 1 SM', '1-2 SM', '2-3 SM', '3-5 SM', '5+ SM'],\n",
    "    right=True,\n",
    "    include_lowest=True,\n",
    ")\n",
    "print(f\"\\nDistribuição por faixa de renda (SM = R$ {SALARIO_MINIMO}):\")\n",
    "for faixa, peso in df_pnad[df_pnad['tem_renda'] == 1].groupby('faixa_renda_sm')['peso'].sum().items():\n",
    "    pct = peso / df_pnad.loc[mask_renda, 'peso'].sum() * 100\n",
    "    print(f\"  {faixa}: {peso/1e6:.1f} milhões ({pct:.1f}%)\")\n",
    "\n",
    "# Verificar preenchimento de horas\n",
    "n_horas_hab = df_pnad['horas_habituais'].notna().sum()\n",
    "n_horas_efe = df_pnad['horas_efetivas'].notna().sum()\n",
    "print(f\"\\nHoras habituais: {n_horas_hab:,} ({n_horas_hab/len(df_pnad):.1%})\")\n",
    "print(f\"Horas efetivas:  {n_horas_efe:,} ({n_horas_efe/len(df_pnad):.1%})\")\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# SALVAR\n",
    "# ---------------------------------------------------------------------------\n",
    "pnad_clean_path = DATA_PROCESSED / \"pnad_clean.csv\"\n",
    "df_pnad.to_csv(pnad_clean_path, index=False)\n",
    "print(f\"\\nSalvo em: {pnad_clean_path}\")\n",
    "print(f\"df_pnad: {df_pnad.shape[0]:,} linhas x {df_pnad.shape[1]} colunas\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f2021a5",
   "metadata": {},
   "source": [
    "### 4b. Verificar Limpeza e variáveis derivadas – PNAD\n",
    "Verificar: número de linhas, colunas criadas, valores faltantes em COD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "7aca2b1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "CHECKPOINT - Limpeza PNAD\n",
      "============================================================\n",
      "\n",
      "Observações: 220,091 -> 207,901 (perda: 5.5%)\n",
      "  WARNING: grande_grupo tem 1,671 valores faltantes\n",
      "\n",
      "Ocupações únicas (COD): 428\n",
      "UFs: 27\n",
      "População representada: 97.8 milhões\n",
      "\n",
      "Distribuição por sexo:\n",
      "  Homem: 55.0 milhões\n",
      "  Mulher: 42.8 milhões\n",
      "\n",
      "Distribuição por região:\n",
      "  Sudeste: 43.4 milhões\n",
      "  Nordeste: 22.3 milhões\n",
      "  Sul: 15.7 milhões\n",
      "  Centro-Oeste: 8.5 milhões\n",
      "  Norte: 7.8 milhões\n",
      "\n",
      "Distribuição por faixa etária:\n",
      "faixa_etaria\n",
      "18-24    29595\n",
      "25-34    48629\n",
      "35-44    55588\n",
      "45-54    46136\n",
      "55+      27953\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Distribuição por faixa de renda (SM):\n",
      "faixa_renda_sm\n",
      "Até 1 SM    74904\n",
      "1-2 SM      77010\n",
      "2-3 SM      19520\n",
      "3-5 SM      18120\n",
      "5+ SM       14588\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Etapa 1.4b - Preparação de Dados - Verificar Limpeza e variáveis derivadas\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"CHECKPOINT - Limpeza PNAD\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Perda de observações\n",
    "pct_perda = 1 - len(df_pnad) / n_inicial\n",
    "print(f\"\\nObservações: {n_inicial:,} -> {len(df_pnad):,} (perda: {pct_perda:.1%})\")\n",
    "if pct_perda > 0.20:\n",
    "    print(f\"  WARNING: Perda de {pct_perda:.1%} das observações (> 20%)\")\n",
    "\n",
    "# Missings em variáveis derivadas\n",
    "for col in ['regiao', 'raca_agregada', 'grande_grupo', 'faixa_etaria', 'sexo_texto']:\n",
    "    n_miss = df_pnad[col].isna().sum()\n",
    "    if n_miss > 0:\n",
    "        print(f\"  WARNING: {col} tem {n_miss:,} valores faltantes\")\n",
    "\n",
    "print(f\"\\nOcupações únicas (COD): {df_pnad['cod_ocupacao'].nunique()}\")\n",
    "print(f\"UFs: {df_pnad['sigla_uf'].nunique()}\")\n",
    "print(f\"População representada: {df_pnad['peso'].sum()/1e6:.1f} milhões\")\n",
    "\n",
    "print(\"\\nDistribuição por sexo:\")\n",
    "for sexo, peso in df_pnad.groupby('sexo_texto')['peso'].sum().items():\n",
    "    print(f\"  {sexo}: {peso/1e6:.1f} milhões\")\n",
    "\n",
    "print(\"\\nDistribuição por região:\")\n",
    "for regiao, peso in df_pnad.groupby('regiao')['peso'].sum().sort_values(ascending=False).items():\n",
    "    print(f\"  {regiao}: {peso/1e6:.1f} milhões\")\n",
    "\n",
    "print(\"\\nDistribuição por faixa etária:\")\n",
    "print(df_pnad['faixa_etaria'].value_counts().sort_index())\n",
    "\n",
    "print(\"\\nDistribuição por faixa de renda (SM):\")\n",
    "print(df_pnad['faixa_renda_sm'].value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8c33c32",
   "metadata": {},
   "source": [
    "### 5a. Crosswalk COD → ISCO-08\n",
    "Mapear códigos de ocupação COD (PNAD) para ISCO-08 para permitir o merge com o índice ILO.\n",
    "\n",
    "#### Estratégia de correspondência\n",
    "\n",
    "A COD (Classificação de Ocupações para Pesquisas Domiciliares) do IBGE é derivada diretamente da ISCO-08 da OIT. Os códigos compartilham a mesma estrutura hierárquica de 4 dígitos, com o primeiro dígito representando os mesmos 9 grandes grupos ocupacionais (Fonte: IBGE, Nota Técnica COD 2010). Isso permite um match direto de string entre COD e ISCO-08 na maioria dos casos.\n",
    "\n",
    "Adotamos uma estratégia de correspondência hierárquica para maximizar a cobertura:\n",
    "\n",
    "1. **4 dígitos (exato):** match direto COD ↔ ISCO-08. Cobre ~98% das observações.\n",
    "2. **3 dígitos (subgrupo):** para códigos COD sem equivalente exato na ISCO-08, atribui-se a média do subgrupo (3 primeiros dígitos). Cobre ~1-2% adicional.\n",
    "3. **2 dígitos (grupo menor):** fallback para o grupo de 2 dígitos.\n",
    "4. **1 dígito (grande grupo):** fallback final para o grande grupo ocupacional.\n",
    "\n",
    "**Limitação:** Não há validação semântica título-a-título; possíveis \"falsos cognatos numéricos\" são mitigados pelos sanity checks por grande grupo (verificação de que a ordenação de exposição por grande grupo é coerente com a literatura)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "69e6612d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PNAD: 207,901 observações\n",
      "ILO:  427 ocupações ISCO-08\n",
      "\n",
      "Códigos ILO: 4d=427, 3d=127, 2d=40, 1d=9\n",
      "\n",
      "Match 4-digit: 203,617 (97.9%)\n",
      "Match 3-digit: 2,613 (1.3%)\n",
      "Match 2-digit: 0 (0.0%)\n",
      "Match 1-digit: 0 (0.0%)\n",
      "Sem match:     1,671 (0.8%)\n"
     ]
    }
   ],
   "source": [
    "# Etapa 1.5a - Preparação de Dados - Crosswalk COD → ISCO-08\n",
    "\n",
    "# Garantir formatos string\n",
    "df_ilo['isco_08_str'] = df_ilo['isco_08_str'].astype(str).str.zfill(4)\n",
    "df_pnad['cod_ocupacao'] = df_pnad['cod_ocupacao'].astype(str).str.zfill(4)\n",
    "\n",
    "print(f\"PNAD: {len(df_pnad):,} observações\")\n",
    "print(f\"ILO:  {len(df_ilo):,} ocupações ISCO-08\")\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# Criar dicionários de lookup em cada nível hierárquico\n",
    "# ---------------------------------------------------------------------------\n",
    "ilo_4d = df_ilo.groupby('isco_08_str')['exposure_score'].mean().to_dict()\n",
    "ilo_3d = df_ilo.groupby(df_ilo['isco_08_str'].str[:3])['exposure_score'].mean().to_dict()\n",
    "ilo_2d = df_ilo.groupby(df_ilo['isco_08_str'].str[:2])['exposure_score'].mean().to_dict()\n",
    "ilo_1d = df_ilo.groupby(df_ilo['isco_08_str'].str[:1])['exposure_score'].mean().to_dict()\n",
    "\n",
    "# Lookup do gradiente oficial ILO (potential25) — apenas para match 4-digit\n",
    "ilo_gradient_4d = df_ilo.groupby('isco_08_str')['exposure_gradient'].first().to_dict()\n",
    "\n",
    "print(f\"\\nCódigos ILO: 4d={len(ilo_4d)}, 3d={len(ilo_3d)}, 2d={len(ilo_2d)}, 1d={len(ilo_1d)}\")\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# Crosswalk hierárquico (4 → 3 → 2 → 1 dígito)\n",
    "# ---------------------------------------------------------------------------\n",
    "df_crosswalked = df_pnad.copy()\n",
    "df_crosswalked['exposure_score'] = np.nan\n",
    "df_crosswalked['exposure_gradient'] = None\n",
    "df_crosswalked['match_level'] = None\n",
    "\n",
    "# Nível 4-digit\n",
    "mask_4d = df_crosswalked['cod_ocupacao'].isin(ilo_4d.keys())\n",
    "df_crosswalked.loc[mask_4d, 'exposure_score'] = df_crosswalked.loc[mask_4d, 'cod_ocupacao'].map(ilo_4d)\n",
    "df_crosswalked.loc[mask_4d, 'exposure_gradient'] = df_crosswalked.loc[mask_4d, 'cod_ocupacao'].map(ilo_gradient_4d)\n",
    "df_crosswalked.loc[mask_4d, 'match_level'] = '4-digit'\n",
    "print(f\"\\nMatch 4-digit: {mask_4d.sum():,} ({mask_4d.mean():.1%})\")\n",
    "\n",
    "# Nível 3-digit\n",
    "mask_missing = df_crosswalked['exposure_score'].isna()\n",
    "cod_3d = df_crosswalked.loc[mask_missing, 'cod_ocupacao'].str[:3]\n",
    "mask_3d = cod_3d.isin(ilo_3d.keys())\n",
    "idx_3d = mask_missing[mask_missing].index[mask_3d.values]\n",
    "df_crosswalked.loc[idx_3d, 'exposure_score'] = cod_3d[mask_3d].map(ilo_3d).values\n",
    "df_crosswalked.loc[idx_3d, 'exposure_gradient'] = 'Sem classificação'\n",
    "df_crosswalked.loc[idx_3d, 'match_level'] = '3-digit'\n",
    "print(f\"Match 3-digit: {len(idx_3d):,} ({len(idx_3d)/len(df_crosswalked):.1%})\")\n",
    "\n",
    "# Nível 2-digit\n",
    "mask_missing = df_crosswalked['exposure_score'].isna()\n",
    "cod_2d = df_crosswalked.loc[mask_missing, 'cod_ocupacao'].str[:2]\n",
    "mask_2d = cod_2d.isin(ilo_2d.keys())\n",
    "idx_2d = mask_missing[mask_missing].index[mask_2d.values]\n",
    "df_crosswalked.loc[idx_2d, 'exposure_score'] = cod_2d[mask_2d].map(ilo_2d).values\n",
    "df_crosswalked.loc[idx_2d, 'exposure_gradient'] = 'Sem classificação'\n",
    "df_crosswalked.loc[idx_2d, 'match_level'] = '2-digit'\n",
    "print(f\"Match 2-digit: {len(idx_2d):,} ({len(idx_2d)/len(df_crosswalked):.1%})\")\n",
    "\n",
    "# Nível 1-digit\n",
    "mask_missing = df_crosswalked['exposure_score'].isna()\n",
    "cod_1d = df_crosswalked.loc[mask_missing, 'cod_ocupacao'].str[:1]\n",
    "mask_1d = cod_1d.isin(ilo_1d.keys())\n",
    "idx_1d = mask_missing[mask_missing].index[mask_1d.values]\n",
    "df_crosswalked.loc[idx_1d, 'exposure_score'] = cod_1d[mask_1d].map(ilo_1d).values\n",
    "df_crosswalked.loc[idx_1d, 'exposure_gradient'] = 'Sem classificação'\n",
    "df_crosswalked.loc[idx_1d, 'match_level'] = '1-digit'\n",
    "print(f\"Match 1-digit: {len(idx_1d):,} ({len(idx_1d)/len(df_crosswalked):.1%})\")\n",
    "\n",
    "# Sem match\n",
    "n_sem_match = df_crosswalked['exposure_score'].isna().sum()\n",
    "df_crosswalked.loc[df_crosswalked['exposure_score'].isna(), 'exposure_gradient'] = 'Sem classificação'\n",
    "print(f\"Sem match:     {n_sem_match:,} ({n_sem_match/len(df_crosswalked):.1%})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28851550",
   "metadata": {},
   "source": [
    "### 5a. Verificar Crosswalk COD → ISCO-08\n",
    "Verificar: cobertura do crosswalk (percentual de linhas com ISCO preenchido)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "ae624efd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "CHECKPOINT - Crosswalk COD → ISCO-08\n",
      "============================================================\n",
      "\n",
      "Cobertura total: 99.2%\n",
      "\n",
      "Distribuição por nível de match:\n",
      "  4-digit: 203,617 (97.9%)\n",
      "  3-digit: 2,613 (1.3%)\n",
      "\n",
      "Match genérico (1-digit + 2-digit): 0 (0.0%)\n",
      "  OK: Apenas 0.0% em match genérico.\n",
      "\n",
      "Estatísticas do exposure_score:\n",
      "  Média:  0.265\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Std:    0.144\n",
      "  Min:    0.090\n",
      "  Max:    0.700\n",
      "\n",
      "Exposição média por grande grupo (sanity check):\n",
      "  Apoio administrativo: 0.554\n",
      "  Dirigentes e gerentes: 0.400\n",
      "  Profissionais das ciências: 0.353\n",
      "  Técnicos nível médio: 0.345\n",
      "  Serviços e vendedores: 0.305\n",
      "  Operadores de máquinas: 0.223\n",
      "  Agropecuária qualificada: 0.174\n",
      "  Indústria qualificada: 0.151\n",
      "  Ocupações elementares: 0.130\n",
      "\n",
      "VALIDAÇÃO DE SANIDADE:\n",
      "  OK - Profissionais das ciências com exposição ALTA (0.353)\n",
      "  OK - Ocupações elementares com exposição BAIXA (0.130)\n"
     ]
    }
   ],
   "source": [
    "# Etapa 1.5b - Preparação de Dados - Verificar Crosswalk COD → ISCO-08\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"CHECKPOINT - Crosswalk COD → ISCO-08\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Cobertura total\n",
    "coverage = df_crosswalked['exposure_score'].notna().mean()\n",
    "print(f\"\\nCobertura total: {coverage:.1%}\")\n",
    "if coverage < 0.90:\n",
    "    print(f\"  WARNING: Cobertura {coverage:.1%} abaixo de 90%\")\n",
    "\n",
    "# Distribuição por nível de match\n",
    "print(\"\\nDistribuição por nível de match:\")\n",
    "for level, count in df_crosswalked['match_level'].value_counts().items():\n",
    "    pct = count / len(df_crosswalked) * 100\n",
    "    print(f\"  {level}: {count:,} ({pct:.1f}%)\")\n",
    "\n",
    "# FIX 3: Verificar concentração em match genérico (fallback)\n",
    "n_total = len(df_crosswalked)\n",
    "n_generic = df_crosswalked['match_level'].isin(['1-digit', '2-digit']).sum()\n",
    "pct_generic = n_generic / n_total * 100\n",
    "print(f\"\\nMatch genérico (1-digit + 2-digit): {n_generic:,} ({pct_generic:.1f}%)\")\n",
    "if pct_generic > 5:\n",
    "    print(f\"  WARNING: {pct_generic:.1f}% caiu em match genérico (>5%). \"\n",
    "          \"Scores podem não refletir a ocupação real.\")\n",
    "else:\n",
    "    print(f\"  OK: Apenas {pct_generic:.1f}% em match genérico.\")\n",
    "\n",
    "# Estatísticas de score\n",
    "print(f\"\\nEstatísticas do exposure_score:\")\n",
    "print(f\"  Média:  {df_crosswalked['exposure_score'].mean():.3f}\")\n",
    "print(f\"  Std:    {df_crosswalked['exposure_score'].std():.3f}\")\n",
    "print(f\"  Min:    {df_crosswalked['exposure_score'].min():.3f}\")\n",
    "print(f\"  Max:    {df_crosswalked['exposure_score'].max():.3f}\")\n",
    "\n",
    "# Sanity check: exposição por grande grupo\n",
    "print(\"\\nExposição média por grande grupo (sanity check):\")\n",
    "exp_grupos = df_crosswalked.groupby('grande_grupo').apply(\n",
    "    lambda x: weighted_mean(x['exposure_score'].dropna(), x.loc[x['exposure_score'].notna(), 'peso'])\n",
    ").sort_values(ascending=False)\n",
    "\n",
    "for grupo, score in exp_grupos.items():\n",
    "    print(f\"  {grupo}: {score:.3f}\")\n",
    "\n",
    "# Validações de sanidade\n",
    "print(\"\\nVALIDAÇÃO DE SANIDADE:\")\n",
    "if 'Profissionais das ciências' in exp_grupos.index:\n",
    "    val = exp_grupos['Profissionais das ciências']\n",
    "    if val > 0.30:\n",
    "        print(f\"  OK - Profissionais das ciências com exposição ALTA ({val:.3f})\")\n",
    "    else:\n",
    "        print(f\"  WARNING: Profissionais das ciências com exposição BAIXA ({val:.3f}). Esperado > 0.30\")\n",
    "\n",
    "if 'Ocupações elementares' in exp_grupos.index:\n",
    "    val = exp_grupos['Ocupações elementares']\n",
    "    if val < 0.20:\n",
    "        print(f\"  OK - Ocupações elementares com exposição BAIXA ({val:.3f})\")\n",
    "    else:\n",
    "        print(f\"  WARNING: Ocupações elementares com exposição ALTA ({val:.3f}). Esperado < 0.20\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea1e60d6",
   "metadata": {},
   "source": [
    "### 6. Merge final – PNAD + índice ILO\n",
    "Juntar a base PNAD (com ISCO-08) ao índice ILO por código de ocupação. Gera a base analítica final da Etapa 1.\n",
    "**Saída:** `data/output/pnad_ilo_merged.csv`\n",
    "\n",
    "#### Classificação de exposição\n",
    "\n",
    "Utilizamos a classificação oficial do WP140 da OIT (Tabela 5), que categoriza ocupações em 6 níveis de exposição com base em critérios bivariados — a média (μ) e o desvio-padrão (σ) dos scores de tarefa:\n",
    "\n",
    "| Categoria | Descrição |\n",
    "|-----------|-----------|\n",
    "| Not Exposed | Exposição negligenciável |\n",
    "| Minimal Exposure | Exposição mínima |\n",
    "| Gradient 1 | Exposição baixa (alto potencial de aumento de produtividade) |\n",
    "| Gradient 2 | Exposição moderada-baixa |\n",
    "| Gradient 3 | Exposição moderada-alta |\n",
    "| Gradient 4 | Exposição alta (maior potencial de automação) |\n",
    "\n",
    "Esta classificação vem pré-computada na coluna `potential25` do dataset publicado pela OIT (Gmyrek, Berg & Cappelli, 2025). Verificamos a consistência reproduzindo a lógica bivariada da Tabela 5, obtendo 99,1% de concordância (423/427 ocupações). Os 4 mismatches são casos de fronteira.\n",
    "\n",
    "Para ocupações com match hierárquico (3 dígitos), onde não há classificação individual disponível, atribuímos a categoria \"Sem classificação\". O score numérico (`exposure_score`) permanece disponível para essas ocupações."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "l5medrxgu7r",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "CHECKPOINT - Qualidade do Merge\n",
      "============================================================\n",
      "Observações totais:     207,901\n",
      "Com score de exposição: 206,230\n",
      "Sem score (NaN):        1,671\n",
      "% pop. sem score:       0.8%\n",
      "\n",
      "Distribuição por gradiente ILO (potential25):\n",
      "  Not Exposed: 52.2 milhões\n",
      "  Minimal Exposure: 15.2 milhões\n",
      "  Exposed: Gradient 2: 10.0 milhões\n",
      "  Exposed: Gradient 1: 8.7 milhões\n",
      "  Exposed: Gradient 4: 5.0 milhões\n",
      "  Exposed: Gradient 3: 4.8 milhões\n",
      "  Sem classificação: 1.8 milhões\n",
      "\n",
      "População por quintil de exposição (deve ser ~20% cada):\n",
      "  Q1 (Baixa): 20.8 milhões (21.5%)\n",
      "  Q2: 18.1 milhões (18.7%)\n",
      "  Q3: 21.4 milhões (22.0%)\n",
      "  Q4: 17.4 milhões (17.9%)\n",
      "  Q5 (Alta): 19.3 milhões (19.9%)\n",
      "\n",
      "Setores: 17 categorias\n",
      "Trabalhadores em setores críticos IA: 7.8 milhões\n",
      "\n",
      "Distribuição por setor:\n",
      "  Outros Serviços: 19.1 milhões\n",
      "  Ind. Transformação: 18.7 milhões\n",
      "  Construção: 7.2 milhões\n",
      "  Educação: 7.2 milhões\n",
      "  Saúde: 6.2 milhões\n",
      "  Transporte: 5.7 milhões\n",
      "  Serviços Domésticos: 5.3 milhões\n",
      "  Alojamento e Alimentação: 5.1 milhões\n",
      "  Administração Pública: 5.0 milhões\n",
      "  Serviços Administrativos: 4.6 milhões\n",
      "  Serviços Profissionais: 4.3 milhões *\n",
      "  Comércio: 3.2 milhões\n",
      "  Informação e Comunicação: 1.9 milhões *\n",
      "  Finanças e Seguros: 1.6 milhões *\n",
      "  Artes e Cultura: 1.2 milhões\n",
      "  Utilidades: 0.7 milhões\n",
      "  Atividades Imobiliárias: 0.7 milhões\n",
      "\n",
      "============================================================\n",
      "BASE FINAL CONSOLIDADA\n",
      "============================================================\n",
      "Observações:       207,901\n",
      "Com score:         206,230\n",
      "Cobertura:         99.2%\n",
      "Colunas:           31\n",
      "População total:   97.8 milhões\n",
      "  com renda:       96.7 milhões\n",
      "  sem renda:       1.1 milhões\n",
      "Setores:           17 categorias\n",
      "Setor crítico IA:  13,427 obs\n",
      "Salvo em:          data/output/pnad_ilo_merged.csv\n",
      "Tamanho em disco:  40.0 MB\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 207901 entries, 0 to 220090\n",
      "Data columns (total 31 columns):\n",
      " #   Column                Non-Null Count   Dtype   \n",
      "---  ------                --------------   -----   \n",
      " 0   ano                   207901 non-null  Int64   \n",
      " 1   trimestre             207901 non-null  Int64   \n",
      " 2   sigla_uf              207901 non-null  object  \n",
      " 3   regiao                207901 non-null  object  \n",
      " 4   sexo                  207901 non-null  object  \n",
      " 5   sexo_texto            207901 non-null  object  \n",
      " 6   idade                 207901 non-null  Int64   \n",
      " 7   faixa_etaria          207901 non-null  category\n",
      " 8   raca_cor              207901 non-null  object  \n",
      " 9   raca_agregada         207901 non-null  object  \n",
      " 10  nivel_instrucao       207901 non-null  object  \n",
      " 11  cod_ocupacao          207901 non-null  object  \n",
      " 12  grande_grupo          206230 non-null  object  \n",
      " 13  grupamento_atividade  207901 non-null  object  \n",
      " 14  setor_agregado        207901 non-null  object  \n",
      " 15  setor_critico_ia      207901 non-null  int64   \n",
      " 16  posicao_ocupacao      207901 non-null  object  \n",
      " 17  formal                207901 non-null  int64   \n",
      " 18  tem_renda             207901 non-null  int64   \n",
      " 19  rendimento_habitual   204142 non-null  float64 \n",
      " 20  rendimento_winsor     204142 non-null  float64 \n",
      " 21  rendimento_efetivo    204171 non-null  float64 \n",
      " 22  horas_habituais       207901 non-null  Int64   \n",
      " 23  horas_efetivas        207901 non-null  Int64   \n",
      " 24  faixa_renda_sm        204142 non-null  category\n",
      " 25  peso                  207901 non-null  float64 \n",
      " 26  exposure_score        206230 non-null  float64 \n",
      " 27  exposure_gradient     207901 non-null  object  \n",
      " 28  match_level           206230 non-null  object  \n",
      " 29  quintil_exposure      206230 non-null  category\n",
      " 30  decil_exposure        206230 non-null  category\n",
      "dtypes: Int64(5), category(4), float64(5), int64(3), object(14)\n",
      "memory usage: 54.3+ MB\n"
     ]
    }
   ],
   "source": [
    "# Etapa 1.6 - Preparação de Dados - Merge final PNAD + ILO\n",
    "\n",
    "df_final = df_crosswalked.copy()\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# Checkpoint de qualidade do merge\n",
    "# ---------------------------------------------------------------------------\n",
    "n_com_score = df_final['exposure_score'].notna().sum()\n",
    "n_sem_score = df_final['exposure_score'].isna().sum()\n",
    "pct_pop_perdida = df_final.loc[df_final['exposure_score'].isna(), 'peso'].sum() / df_final['peso'].sum() * 100\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"CHECKPOINT - Qualidade do Merge\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Observações totais:     {len(df_final):,}\")\n",
    "print(f\"Com score de exposição: {n_com_score:,}\")\n",
    "print(f\"Sem score (NaN):        {n_sem_score:,}\")\n",
    "print(f\"% pop. sem score:       {pct_pop_perdida:.1f}%\")\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# Distribuição por gradiente ILO (potential25)\n",
    "# ---------------------------------------------------------------------------\n",
    "print(\"\\nDistribuição por gradiente ILO (potential25):\")\n",
    "for grad, peso in df_final.groupby('exposure_gradient')['peso'].sum().sort_values(ascending=False).items():\n",
    "    print(f\"  {grad}: {peso/1e6:.1f} milhões\")\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# Quintis e decis de EXPOSIÇÃO — ponderados por peso amostral\n",
    "# Usa weighted_qcut (definida na célula 1) para que cada faixa represente\n",
    "# ~20% (quintis) ou ~10% (decis) da POPULAÇÃO, não da amostra.\n",
    "# ---------------------------------------------------------------------------\n",
    "mask_valid = df_final['exposure_score'].notna()\n",
    "\n",
    "df_final.loc[mask_valid, 'quintil_exposure'] = weighted_qcut(\n",
    "    df_final.loc[mask_valid, 'exposure_score'],\n",
    "    df_final.loc[mask_valid, 'peso'],\n",
    "    q=5,\n",
    "    labels=['Q1 (Baixa)', 'Q2', 'Q3', 'Q4', 'Q5 (Alta)'],\n",
    ")\n",
    "\n",
    "df_final.loc[mask_valid, 'decil_exposure'] = weighted_qcut(\n",
    "    df_final.loc[mask_valid, 'exposure_score'],\n",
    "    df_final.loc[mask_valid, 'peso'],\n",
    "    q=10,\n",
    "    labels=[f'D{i}' for i in range(1, 11)],\n",
    ")\n",
    "\n",
    "# Verificar distribuição populacional dos quintis de exposição\n",
    "print(\"\\nPopulação por quintil de exposição (deve ser ~20% cada):\")\n",
    "for q, peso in df_final.groupby('quintil_exposure')['peso'].sum().items():\n",
    "    pct = peso / df_final.loc[mask_valid, 'peso'].sum() * 100\n",
    "    print(f\"  {q}: {peso/1e6:.1f} milhões ({pct:.1f}%)\")\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# Agregação setorial — CNAE Domiciliar 2.0, Seções A-T (IBGE)\n",
    "# Usa CNAE_SETOR_MAP definido na célula de configuração.\n",
    "# ---------------------------------------------------------------------------\n",
    "df_final['cnae_2d'] = df_final['grupamento_atividade'].astype(str).str[:2]\n",
    "df_final['setor_agregado'] = df_final['cnae_2d'].map(CNAE_SETOR_MAP).fillna('Outros Serviços')\n",
    "\n",
    "# Flag de setores críticos para IA\n",
    "# Ref: Gmyrek et al. (2024); Eloundou et al. (2023)\n",
    "df_final['setor_critico_ia'] = df_final['setor_agregado'].isin(SETORES_CRITICOS_IA).astype(int)\n",
    "\n",
    "print(f\"\\nSetores: {df_final['setor_agregado'].nunique()} categorias\")\n",
    "print(f\"Trabalhadores em setores críticos IA: {df_final.loc[df_final['setor_critico_ia']==1, 'peso'].sum()/1e6:.1f} milhões\")\n",
    "\n",
    "print(\"\\nDistribuição por setor:\")\n",
    "for setor, peso in df_final.groupby('setor_agregado')['peso'].sum().sort_values(ascending=False).items():\n",
    "    flag = \" *\" if setor in SETORES_CRITICOS_IA else \"\"\n",
    "    print(f\"  {setor}: {peso/1e6:.1f} milhões{flag}\")\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# Selecionar colunas finais e salvar\n",
    "# ---------------------------------------------------------------------------\n",
    "cols_output = [\n",
    "    'ano', 'trimestre', 'sigla_uf', 'regiao',\n",
    "    'sexo', 'sexo_texto', 'idade', 'faixa_etaria',\n",
    "    'raca_cor', 'raca_agregada', 'nivel_instrucao',\n",
    "    'cod_ocupacao', 'grande_grupo',\n",
    "    'grupamento_atividade', 'setor_agregado', 'setor_critico_ia',\n",
    "    'posicao_ocupacao', 'formal', 'tem_renda',\n",
    "    'rendimento_habitual', 'rendimento_winsor', 'rendimento_efetivo',\n",
    "    'horas_habituais', 'horas_efetivas',\n",
    "    'faixa_renda_sm',\n",
    "    'peso',\n",
    "    'exposure_score', 'exposure_gradient', 'match_level',\n",
    "    'quintil_exposure', 'decil_exposure',\n",
    "]\n",
    "\n",
    "df_final = df_final[[c for c in cols_output if c in df_final.columns]]\n",
    "\n",
    "output_path = DATA_OUTPUT / \"pnad_ilo_merged.csv\"\n",
    "df_final.to_csv(output_path, index=False)\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# Resumo final\n",
    "# ---------------------------------------------------------------------------\n",
    "print(f\"\\n{'=' * 60}\")\n",
    "print(\"BASE FINAL CONSOLIDADA\")\n",
    "print(f\"{'=' * 60}\")\n",
    "print(f\"Observações:       {len(df_final):,}\")\n",
    "print(f\"Com score:         {df_final['exposure_score'].notna().sum():,}\")\n",
    "print(f\"Cobertura:         {df_final['exposure_score'].notna().mean():.1%}\")\n",
    "print(f\"Colunas:           {df_final.shape[1]}\")\n",
    "print(f\"População total:   {df_final['peso'].sum()/1e6:.1f} milhões\")\n",
    "print(f\"  com renda:       {df_final.loc[df_final['tem_renda']==1, 'peso'].sum()/1e6:.1f} milhões\")\n",
    "print(f\"  sem renda:       {df_final.loc[df_final['tem_renda']==0, 'peso'].sum()/1e6:.1f} milhões\")\n",
    "print(f\"Setores:           {df_final['setor_agregado'].nunique()} categorias\")\n",
    "print(f\"Setor crítico IA:  {df_final['setor_critico_ia'].sum():,} obs\")\n",
    "print(f\"Salvo em:          {output_path}\")\n",
    "print(f\"Tamanho em disco:  {output_path.stat().st_size / 1e6:.1f} MB\")\n",
    "\n",
    "df_final.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fkxj5zz52m",
   "metadata": {},
   "source": [
    "### Limitações desta etapa\n",
    "\n",
    "1. **Crosswalk hierárquico:** O match a 3 dígitos (subgrupo) suaviza diferenças entre ocupações dentro do mesmo subgrupo, afetando ~1,3% das observações. O score atribuído é a média do subgrupo, não o score específico da ocupação.\n",
    "\n",
    "2. **Trimestre único:** Os dados referem-se a um único trimestre (Q3/2025). Resultados podem variar sazonalmente, especialmente em setores com forte sazonalidade (agropecuária, comércio).\n",
    "\n",
    "3. **Variáveis indisponíveis:** As variáveis de tempo no emprego (`V4040`) e porte da empresa (`V4018`) não estão populadas na fonte utilizada (Base dos Dados/BigQuery) para o período analisado, limitando análises de estabilidade ocupacional e adoção de IA por tamanho de empresa.\n",
    "\n",
    "4. **Índice global aplicado ao Brasil:** O índice da OIT foi desenvolvido com foco global e pode não capturar especificidades do mercado de trabalho brasileiro, como a elevada informalidade (~40% da força de trabalho) e diferenças na adoção tecnológica entre setores formais e informais.\n",
    "\n",
    "5. **Exposição ≠ impacto:** O índice mede potencial de exposição das tarefas à IA generativa, não o impacto efetivo. A materialização do impacto depende de fatores como velocidade de adoção tecnológica, regulação, custos de implementação e respostas institucionais."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
