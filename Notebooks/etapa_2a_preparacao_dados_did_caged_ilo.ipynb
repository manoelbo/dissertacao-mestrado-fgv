{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "6ac33147",
      "metadata": {},
      "source": [
        "# ETAPA 2a — Preparação do Painel CAGED + ILO Exposure Index\n",
        "\n",
        "**Dissertação:** Inteligência Artificial Generativa e o Mercado de Trabalho Brasileiro: Uma Análise de Exposição Ocupacional e seus Efeitos Distributivos.\n",
        "\n",
        "**Aluno:** Manoel Brasil Orlandi\n",
        "\n",
        "---\n",
        "\n",
        "### Contextualização\n",
        "\n",
        "A rápida difusão de modelos de IA generativa (LLMs, geradores de imagem/código) levanta questões centrais sobre seus impactos no mercado de trabalho. Para mensurar esse potencial de impacto, a Organização Internacional do Trabalho (OIT) criou um índice de exposição ocupacional à IA generativa, publicado como *Working Paper* 140 (WP140). O índice atribui scores de exposição a cada ocupação da classificação ISCO-08, com base na avaliação de suas tarefas constituintes por modelos de linguagem e validação humana.\n",
        "\n",
        "Este notebook prepara uma base de dados que junta os dados do **Novo CAGED** (Cadastro Geral de Empregados e Desempregados) ao **índice de exposição à IA generativa da OIT**, para ser usado em um modelo de Diferenças-em-Diferenças (DiD) no Notebook 2b.\n",
        "\n",
        "### Objetivo\n",
        "\n",
        "Construir o **painel mensal de ocupações formais brasileiras (2021–2025)** a partir do Novo CAGED, realizar o **crosswalk CBO 2002 → ISCO-08** (especificação dual: 2 dígitos como principal, 4 dígitos para robustez), e fazer o merge com o índice de exposição à IA generativa da OIT (Gmyrek, Berg & Cappelli, 2025). O output final é um dataset analítico pronto para a estimação DiD.\n",
        "\n",
        "**Estratégia de crosswalk:** Análise principal a **2 dígitos** ISCO-08 (match por Sub-major Group com fallback hierárquico a Major Group), com robustez a **4 dígitos** via correspondência ISCO-88 ↔ ISCO-08 + fallback hierárquico em 6 níveis.\n",
        "\n",
        "**Inspiração metodológica:** Hui, Reshef & Zhou (2024), \"The Short-Term Effects of Generative Artificial Intelligence on Employment: Evidence from an Online Labor Market\" — adaptado para dados administrativos brasileiros (CAGED) com o índice ILO de exposição ocupacional.\n",
        "\n",
        "### Ficha Técnica dos Dados\n",
        "\n",
        "| Item | Descrição |\n",
        "|------|-----------|\n",
        "| **Fonte CAGED** | Ministério do Trabalho e Emprego (MTE), via Base dos Dados (BigQuery) |\n",
        "| **Dataset BigQuery** | `basedosdados.br_me_caged.microdados_movimentacao` |\n",
        "| **Período** | Janeiro/2021 — Dezembro/2025 (60 meses) |\n",
        "| **Unidade** | Movimentação individual (admissão ou desligamento) |\n",
        "| **Cobertura** | Emprego formal (CLT) em todo o Brasil |\n",
        "| **Índice ILO** | `ilo_exposure_clean.csv` — 427 ocupações ISCO-08 com exposure scores |\n",
        "| **Classificação** | CBO 2002 (CAGED) → ISCO-08 (ILO) via crosswalk hierárquico |\n",
        "\n",
        "### Referências principais\n",
        "- Gmyrek, P., Berg, J. & Cappelli, D. (2025). *Generative AI and Jobs: An updated global assessment*. ILO Working Paper 140.\n",
        "- Hui, X., Reshef, O. & Zhou, L. (2024). *The Short-Term Effects of Generative AI on Employment*. Organization Science, 35(6).\n",
        "- Brynjolfsson, E., Chandar, P. & Chen, J. (2025). *Canaries in the Coal Mine? Six Facts about the Recent Employment Effects of AI*.\n",
        "- Callaway, B. & Sant'Anna, P. (2021). *Difference-in-differences with multiple time periods*. Journal of Econometrics, 225(2).\n",
        "- Muendler, M.-A. & Poole, J.P. (2004). *Job Concordances for Brazil: Mapping CBO to ISCO-88*. UC San Diego.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "893a125d",
      "metadata": {},
      "source": [
        "### 1. Configuração do ambiente\n",
        "\n",
        "Definir caminhos, importar bibliotecas e configurar parâmetros do painel. Todos os caminhos são relativos ao diretório `notebook/`.\n",
        "\n",
        "> **Nota sobre a janela temporal:** Excluímos 2020 para evitar os efeitos distorcivos da pandemia de COVID-19 sobre o mercado de trabalho formal. O ano de 2020 apresentou quedas e recuperações atípicas que contaminariam o período pré-tratamento do DiD. A janela Jan/2021–Dez/2025 oferece 23 meses pré-ChatGPT e 31 meses pós.\n",
        "\n",
        "> **Nota sobre o Novo CAGED:** A partir de janeiro/2020, o CAGED foi substituído pelo sistema eSocial (Portaria SEPRT 1.127/2019). Usamos dados de 2021+ para consistência metodológica (eSocial já estabilizado)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "id": "f87dfc8f",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Instalado: xlrd\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m26.0.1\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3.10 -m pip install --upgrade pip\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "# Verificar dependências e instalar apenas o que faltar (rode esta célula primeiro)\n",
        "import importlib.util\n",
        "import subprocess\n",
        "import sys\n",
        "\n",
        "# (nome para import, nome para pip install)\n",
        "PACOTES = [\n",
        "    (\"pandas\", \"pandas\"),\n",
        "    (\"numpy\", \"numpy\"),\n",
        "    (\"pyarrow\", \"pyarrow\"),\n",
        "    (\"matplotlib\", \"matplotlib\"),\n",
        "    (\"seaborn\", \"seaborn\"),\n",
        "    (\"scipy\", \"scipy\"),\n",
        "    (\"pyfixest\", \"pyfixest\"),\n",
        "    (\"bcb\", \"python-bcb\"),  # IPCA Banco Central; import: from bcb import sgs\n",
        "    (\"xlrd\", \"xlrd\"),  # Leitura de .xls (Crosswalk SOC/ISCO, Estrutura COD) — Anexo 1\n",
        "    (\"openpyxl\", \"openpyxl\"),  # Leitura de .xlsx (Crosswalk SOC 2010 a 2018) — Anexo 1\n",
        "    (\"google.cloud.bigquery\", \"google-cloud-bigquery\"),\n",
        "]\n",
        "\n",
        "def ja_instalado(nome_import):\n",
        "    return importlib.util.find_spec(nome_import) is not None\n",
        "\n",
        "faltando = [pip for imp, pip in PACOTES if not ja_instalado(imp)]\n",
        "if faltando:\n",
        "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\"] + faltando)\n",
        "    print(\"Instalado:\", \", \".join(faltando))\n",
        "else:\n",
        "    print(\"Todas as dependências já estão instaladas.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "02d36db5",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Configuração carregada.\n",
            "  Período: 2021–2025 (60 meses)\n",
            "  Evento: ChatGPT — Nov/2022 (pós a partir de 12/2022)\n",
            "  Projeto GCP: mestrado-pnad-2026\n",
            "  ILO file: data/processed/ilo_exposure_clean.csv (existe: True)\n"
          ]
        }
      ],
      "source": [
        "# Etapa 2a.1 — Configuração do ambiente\n",
        "\n",
        "import warnings\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
        "\n",
        "# ---------------------------------------------------------------------------\n",
        "# Caminhos (relativos ao diretório do notebook)\n",
        "# ---------------------------------------------------------------------------\n",
        "DATA_INPUT     = Path(\"data/input\")\n",
        "DATA_RAW       = Path(\"data/raw\")\n",
        "DATA_PROCESSED = Path(\"data/processed\")\n",
        "DATA_OUTPUT    = Path(\"data/output\")\n",
        "\n",
        "for d in [DATA_INPUT, DATA_RAW, DATA_PROCESSED, DATA_OUTPUT]:\n",
        "    d.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# ---------------------------------------------------------------------------\n",
        "# Parâmetros do Painel CAGED\n",
        "# ---------------------------------------------------------------------------\n",
        "GCP_PROJECT_ID = \"mestrado-pnad-2026\"\n",
        "\n",
        "ANO_INICIO     = 2021\n",
        "ANO_FIM        = 2025\n",
        "ANO_TRATAMENTO = 2022\n",
        "MES_TRATAMENTO = 12   # Dezembro/2022 como primeiro mês \"pós\"\n",
        "\n",
        "SALARIO_MINIMO = {\n",
        "    2021: 1100, 2022: 1212, 2023: 1320, 2024: 1412, 2025: 1518\n",
        "}\n",
        "\n",
        "# ---------------------------------------------------------------------------\n",
        "# Colunas a selecionar do CAGED (BigQuery)\n",
        "# ---------------------------------------------------------------------------\n",
        "COLUNAS_CAGED = \"\"\"\n",
        "    ano, mes, sigla_uf, id_municipio, cbo_2002,\n",
        "    categoria, tipo_movimentacao, saldo_movimentacao,\n",
        "    salario_mensal, grau_instrucao, idade, sexo, raca_cor,\n",
        "    cnae_2_secao, cnae_2_subclasse, tamanho_estabelecimento_janeiro\n",
        "\"\"\"\n",
        "\n",
        "# ---------------------------------------------------------------------------\n",
        "# Arquivos de referência\n",
        "# ---------------------------------------------------------------------------\n",
        "ILO_FILE               = DATA_PROCESSED / \"ilo_exposure_clean.csv\"\n",
        "ISCO_08_88_FILE        = DATA_INPUT / \"Correspondência ISCO 08 a 88.xlsx\"\n",
        "ISCO_08_ESTRUTURA_FILE = DATA_INPUT / \"ISCO 08 Estruturas e Definições.xlsx\"\n",
        "MUENDLER_FILE          = DATA_INPUT / \"cbo-isco-conc.csv\"\n",
        "\n",
        "# ---------------------------------------------------------------------------\n",
        "# Checkpoints intermediários\n",
        "# ---------------------------------------------------------------------------\n",
        "PAINEL_MENSAL_FILE     = DATA_PROCESSED / \"painel_caged_mensal.parquet\"\n",
        "PAINEL_CROSSWALK_FILE  = DATA_PROCESSED / \"painel_caged_crosswalk.parquet\"\n",
        "PAINEL_TRATAMENTO_FILE = DATA_PROCESSED / \"painel_caged_tratamento.parquet\"\n",
        "PAINEL_FINAL_PARQUET   = DATA_OUTPUT / \"painel_caged_did_ready.parquet\"\n",
        "PAINEL_FINAL_CSV       = DATA_OUTPUT / \"painel_caged_did_ready.csv\"\n",
        "\n",
        "# ---------------------------------------------------------------------------\n",
        "# Cache: quando False, arquivos são deletados antes de (re)gerar (força reprocessamento)\n",
        "# ---------------------------------------------------------------------------\n",
        "KEEP_CAGED_RAW         = True   # data/raw/caged_*.parquet\n",
        "KEEP_PANEL_MENSAL      = True   # painel_caged_mensal.parquet\n",
        "KEEP_PANEL_CROSSWALK   = True   # painel_caged_crosswalk.parquet\n",
        "KEEP_PANEL_TRATAMENTO  = False   # painel_caged_tratamento.parquet\n",
        "KEEP_PANEL_FINAL       = False   # painel_caged_did_ready.parquet/csv\n",
        "KEEP_ANTHROPIC_INDEX   = True    # anthropic_automation_augmentation_cbo.parquet (Anexo 1)\n",
        "\n",
        "# Cache do índice Anthropic (Automation vs Augmentation) — Anexo 1\n",
        "ANTHROPIC_INDEX_CACHE  = DATA_PROCESSED / \"anthropic_automation_augmentation_cbo.parquet\"\n",
        "\n",
        "# ---------------------------------------------------------------------------\n",
        "# Deflator (salário real): índice base para IPCA\n",
        "# ---------------------------------------------------------------------------\n",
        "INDICE_BASE_ANO = 2024\n",
        "INDICE_BASE_MES = 12   # Dez/2024 = 100\n",
        "INDICE_BASE     = 100.0\n",
        "IPCA_MENSAL_FILE = DATA_PROCESSED / \"ipca_mensal.parquet\"\n",
        "\n",
        "# ---------------------------------------------------------------------------\n",
        "# Setor tecnológico (CNAE 2.0 seção): J = Informação e comunicação; M = Atividades profissionais/científicas\n",
        "# ---------------------------------------------------------------------------\n",
        "CNAE_SECOES_TECNOLOGICO = ['J']   # ou ['J', 'M'] para incluir atividades profissionais/científicas\n",
        "SETOR_TECNOLOGICO_LIMIAR = 0.5    # setor_tecnologico=1 quando pct_tecnologico_adm >= este limiar\n",
        "\n",
        "# ---------------------------------------------------------------------------\n",
        "# Grandes grupos CBO (para sanity checks)\n",
        "# ---------------------------------------------------------------------------\n",
        "GRANDES_GRUPOS_CBO = {\n",
        "    '0': 'Forças Armadas',\n",
        "    '1': 'Dirigentes',\n",
        "    '2': 'Profissionais das ciências',\n",
        "    '3': 'Técnicos nível médio',\n",
        "    '4': 'Trabalhadores de serv. admin.',\n",
        "    '5': 'Trabalhadores de serviços/comércio',\n",
        "    '6': 'Agropecuária',\n",
        "    '7': 'Produção industrial',\n",
        "    '8': 'Operadores de máquinas',\n",
        "    '9': 'Manutenção e reparação',\n",
        "}\n",
        "\n",
        "print(\"Configuração carregada.\")\n",
        "print(f\"  Período: {ANO_INICIO}–{ANO_FIM} ({(ANO_FIM - ANO_INICIO + 1) * 12} meses)\")\n",
        "print(f\"  Evento: ChatGPT — Nov/2022 (pós a partir de {MES_TRATAMENTO}/{ANO_TRATAMENTO})\")\n",
        "print(f\"  Projeto GCP: {GCP_PROJECT_ID}\")\n",
        "print(f\"  ILO file: {ILO_FILE} (existe: {ILO_FILE.exists()})\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9c1fb69a",
      "metadata": {},
      "source": [
        "### 2. Índice IPCA (deflator para salário real)\n",
        "\n",
        "Carregar série mensal do IPCA (Banco Central SGS) e construir índice com base Dez/2024 = 100, para deflacionar salários nominais no enriquecimento do painel."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "eeaba3d2",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Índice IPCA carregado do cache: ipca_mensal.parquet\n",
            "  Período: 2021-1 a 2025-12, base 2024/12 = 100.0\n"
          ]
        }
      ],
      "source": [
        "# Etapa 2a.2 — Índice IPCA mensal (base Dez/2024 = 100)\n",
        "# Fonte: Banco Central SGS (série 433 = variação mensal IPCA). Índice construído por acumulação.\n",
        "\n",
        "if IPCA_MENSAL_FILE.exists():\n",
        "    df_ipca = pd.read_parquet(IPCA_MENSAL_FILE)\n",
        "    print(f\"Índice IPCA carregado do cache: {IPCA_MENSAL_FILE.name}\")\n",
        "    print(f\"  Período: {df_ipca['ano'].min():.0f}-{df_ipca['mes'].min():.0f} a {df_ipca['ano'].max():.0f}-{df_ipca['mes'].max():.0f}, base {INDICE_BASE_ANO}/{INDICE_BASE_MES} = {INDICE_BASE}\")\n",
        "else:\n",
        "    from bcb import sgs\n",
        "    # Série 433 = IPCA - Variação mensal (%)\n",
        "    start = f\"{ANO_INICIO}-01-01\"\n",
        "    end = f\"{ANO_FIM}-12-31\"\n",
        "    raw = sgs.get({\"IPCA_var\": 433}, start=start, end=end)\n",
        "    raw = raw.reset_index()\n",
        "    raw.columns = [\"data\", \"variacao\"]\n",
        "    raw[\"ano\"] = raw[\"data\"].dt.year\n",
        "    raw[\"mes\"] = raw[\"data\"].dt.month\n",
        "    # Ordenar por tempo e construir índice: base = 100 no mês INDICE_BASE_ANO/INDICE_BASE_MES\n",
        "    raw = raw.sort_values([\"ano\", \"mes\"]).reset_index(drop=True)\n",
        "    raw[\"fator\"] = 1 + raw[\"variacao\"] / 100.0\n",
        "    # Índice em cadeia: 100 no mês base; para trás divide pelo fator do mês seguinte; para frente multiplica\n",
        "    base_idx = raw[(raw[\"ano\"] == INDICE_BASE_ANO) & (raw[\"mes\"] == INDICE_BASE_MES)].index\n",
        "    if len(base_idx) == 0:\n",
        "        raise ValueError(f\"Mês base {INDICE_BASE_ANO}/{INDICE_BASE_MES} não encontrado na série IPCA.\")\n",
        "    base_idx = base_idx[0]\n",
        "    indice = np.ones(len(raw)) * np.nan\n",
        "    indice[base_idx] = INDICE_BASE\n",
        "    for i in range(base_idx - 1, -1, -1):\n",
        "        indice[i] = indice[i + 1] / raw.loc[i + 1, \"fator\"]\n",
        "    for i in range(base_idx + 1, len(raw)):\n",
        "        indice[i] = indice[i - 1] * raw.loc[i, \"fator\"]\n",
        "    raw[\"indice\"] = indice\n",
        "    df_ipca = raw[[\"ano\", \"mes\", \"indice\"]].copy()\n",
        "    df_ipca.to_parquet(IPCA_MENSAL_FILE, index=False)\n",
        "    print(f\"Índice IPCA construído e salvo: {IPCA_MENSAL_FILE.name}\")\n",
        "    print(f\"  Período: {df_ipca['ano'].min():.0f}-{df_ipca['mes'].min():.0f} a {df_ipca['ano'].max():.0f}-{df_ipca['mes'].max():.0f}, base {INDICE_BASE_ANO}/{INDICE_BASE_MES} = {INDICE_BASE}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bd90657e",
      "metadata": {},
      "source": [
        "### 2a. Download dos microdados CAGED\n",
        "\n",
        "Extrair do Novo CAGED (BigQuery/Base dos Dados) todas as movimentações de emprego formal no período 2021–2025.\n",
        "\n",
        "| Item | Descrição |\n",
        "|------|-----------|\n",
        "| **Tabela BigQuery** | `basedosdados.br_me_caged.microdados_movimentacao` |\n",
        "| **Período** | 2021-01 a 2025-12 |\n",
        "| **Filtros** | `ano BETWEEN 2021 AND 2025` |\n",
        "| **Volume estimado** | ~20-30M de registros por ano, ~100-150M total |\n",
        "| **Estratégia** | Download ano a ano via `google-cloud-bigquery` (Storage API) com fallback para `basedosdados` |\n",
        "\n",
        "> **Nota metodológica — Volume de dados:** O CAGED registra ~20-25 milhões de movimentações/ano. Para 5 anos, esperamos ~100-125M de registros. O download é feito ano a ano para evitar OOM e timeout, com salvamento em parquets individuais (`caged_{ano}.parquet`).\n",
        "\n",
        "> **Nota sobre otimização:** Usamos a BigQuery Storage API (`create_bqstorage_client=True`) que transfere dados via gRPC/Arrow, sendo 2-5x mais rápida que o método padrão REST. Se o arquivo parquet já existir, o download é pulado automaticamente."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "451ca3b3",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Download dos microdados CAGED:\n",
            "  2021: Carregado do cache — 36,554,795 registros (380 MB)\n",
            "  2022: Carregado do cache — 42,475,516 registros (441 MB)\n",
            "  2023: Carregado do cache — 44,485,982 registros (469 MB)\n",
            "  2024: Carregado do cache — 48,996,040 registros (510 MB)\n",
            "  2025: Carregado do cache — 26,312,103 registros (269 MB)\n",
            "\n",
            "Total: 198,824,436 movimentações (2021–2025)\n",
            "Colunas: ['ano', 'mes', 'sigla_uf', 'id_municipio', 'cbo_2002', 'categoria', 'tipo_movimentacao', 'saldo_movimentacao', 'salario_mensal', 'grau_instrucao', 'idade', 'sexo', 'raca_cor', 'cnae_2_secao', 'cnae_2_subclasse', 'tamanho_estabelecimento_janeiro']\n"
          ]
        }
      ],
      "source": [
        "# Etapa 2a.2a — Download dos microdados CAGED\n",
        "# Estratégia: download ano a ano via BigQuery Storage API, com cache local em parquet.\n",
        "\n",
        "if not KEEP_CAGED_RAW:\n",
        "    for f in DATA_RAW.glob(\"caged_*.parquet\"):\n",
        "        f.unlink()\n",
        "        print(f\"Cache CAGED removido: {f.name}\")\n",
        "\n",
        "from google.cloud import bigquery\n",
        "\n",
        "def download_caged_ano(ano):\n",
        "    \"\"\"Baixar microdados CAGED de um ano via BigQuery Storage API.\"\"\"\n",
        "    parquet_path = DATA_RAW / f\"caged_{ano}.parquet\"\n",
        "\n",
        "    if parquet_path.exists():\n",
        "        size_mb = parquet_path.stat().st_size / 1e6\n",
        "        df = pd.read_parquet(parquet_path)\n",
        "        print(f\"  {ano}: Carregado do cache — {len(df):,} registros ({size_mb:.0f} MB)\")\n",
        "        return df\n",
        "\n",
        "    print(f\"  {ano}: Baixando do BigQuery...\", end=\"\", flush=True)\n",
        "    query = f\"\"\"\n",
        "    SELECT {COLUNAS_CAGED}\n",
        "    FROM `basedosdados.br_me_caged.microdados_movimentacao`\n",
        "    WHERE ano = {ano}\n",
        "    \"\"\"\n",
        "    client = bigquery.Client(project=GCP_PROJECT_ID)\n",
        "    df = client.query(query).to_dataframe(create_bqstorage_client=True)\n",
        "    df.to_parquet(parquet_path, index=False)\n",
        "    size_mb = parquet_path.stat().st_size / 1e6\n",
        "    print(f\" {len(df):,} registros ({size_mb:.0f} MB)\")\n",
        "    return df\n",
        "\n",
        "# ---------------------------------------------------------------------------\n",
        "# Download ano a ano\n",
        "# ---------------------------------------------------------------------------\n",
        "print(\"Download dos microdados CAGED:\")\n",
        "dfs_anuais = []\n",
        "for ano in range(ANO_INICIO, ANO_FIM + 1):\n",
        "    df_ano = download_caged_ano(ano)\n",
        "    dfs_anuais.append(df_ano)\n",
        "\n",
        "# Resumo (sem concatenar em memória para evitar OOM)\n",
        "total = sum(len(df) for df in dfs_anuais)\n",
        "print(f\"\\nTotal: {total:,} movimentações ({ANO_INICIO}–{ANO_FIM})\")\n",
        "print(f\"Colunas: {list(dfs_anuais[0].columns)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b4e45e2f",
      "metadata": {},
      "source": [
        "### 2b. Verificar dados CAGED (CHECKPOINT)\n",
        "\n",
        "Verificar integridade dos dados baixados: cobertura temporal (12 meses/ano), volume por ano, preenchimento de variáveis-chave, e formato dos códigos CBO.\n",
        "\n",
        "**Critérios de aceite:**\n",
        "- Todos os meses cobertos (Jan–Dez) para cada ano\n",
        "- CBO com >95% de preenchimento\n",
        "- ~20-30M registros por ano\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "63efdd50",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "CHECKPOINT — Microdados CAGED\n",
            "============================================================\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- 2021: 36,554,795 movimentações ---\n",
            "  Meses: 12 (OK)\n",
            "  CBO preenchido: 100.0%\n",
            "  Famílias CBO 4d únicas: 626\n",
            "  Admissões: 19,703,604 | Desligamentos: 16,851,191 | Saldo: +2,852,413\n",
            "\n",
            "--- 2022: 42,475,516 movimentações ---\n",
            "  Meses: 12 (OK)\n",
            "  CBO preenchido: 100.0%\n",
            "  Famílias CBO 4d únicas: 624\n",
            "  Admissões: 22,243,441 | Desligamentos: 20,232,075 | Saldo: +2,011,366\n",
            "\n",
            "--- 2023: 44,485,982 movimentações ---\n",
            "  Meses: 12 (OK)\n",
            "  CBO preenchido: 100.0%\n",
            "  Famílias CBO 4d únicas: 626\n",
            "  Admissões: 22,982,161 | Desligamentos: 21,503,821 | Saldo: +1,478,340\n",
            "\n",
            "--- 2024: 48,996,040 movimentações ---\n",
            "  Meses: 12 (OK)\n",
            "  CBO preenchido: 100.0%\n",
            "  Famílias CBO 4d únicas: 625\n",
            "  Admissões: 25,336,277 | Desligamentos: 23,659,763 | Saldo: +1,676,514\n",
            "\n",
            "--- 2025: 26,312,103 movimentações ---\n",
            "  Meses: 6 (ALERTA: 6 meses)\n",
            "  CBO preenchido: 100.0%\n",
            "  Famílias CBO 4d únicas: 622\n",
            "  Admissões: 13,763,059 | Desligamentos: 12,549,044 | Saldo: +1,214,015\n",
            "\n",
            "============================================================\n",
            "TOTAL: 198,824,436 movimentações (2021–2025)\n",
            "============================================================\n"
          ]
        }
      ],
      "source": [
        "# Etapa 2a.2b — CHECKPOINT: Verificar dados CAGED\n",
        "# Carrega cada parquet individualmente (para evitar OOM)\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"CHECKPOINT — Microdados CAGED\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "total_registros = 0\n",
        "for ano in range(ANO_INICIO, ANO_FIM + 1):\n",
        "    parquet_path = DATA_RAW / f\"caged_{ano}.parquet\"\n",
        "    df = pd.read_parquet(parquet_path)\n",
        "\n",
        "    # Volume\n",
        "    print(f\"\\n--- {ano}: {len(df):,} movimentações ---\")\n",
        "    total_registros += len(df)\n",
        "\n",
        "    # Cobertura mensal\n",
        "    meses = sorted(df['mes'].dropna().unique())\n",
        "    status = \"OK\" if len(meses) == 12 else f\"ALERTA: {len(meses)} meses\"\n",
        "    print(f\"  Meses: {len(meses)} ({status})\")\n",
        "\n",
        "    # Preenchimento CBO\n",
        "    cbo_pct = df['cbo_2002'].notna().mean()\n",
        "    print(f\"  CBO preenchido: {cbo_pct:.1%}\")\n",
        "\n",
        "    # CBOs únicos\n",
        "    cbos = df['cbo_2002'].dropna().astype(str).str[:4].nunique()\n",
        "    print(f\"  Famílias CBO 4d únicas: {cbos}\")\n",
        "\n",
        "    # Admissões vs desligamentos\n",
        "    if 'saldo_movimentacao' in df.columns:\n",
        "        adm = (df['saldo_movimentacao'] == 1).sum()\n",
        "        desl = (df['saldo_movimentacao'] == -1).sum()\n",
        "        print(f\"  Admissões: {adm:,} | Desligamentos: {desl:,} | Saldo: {adm-desl:+,}\")\n",
        "\n",
        "print(f\"\\n{'=' * 60}\")\n",
        "print(f\"TOTAL: {total_registros:,} movimentações ({ANO_INICIO}–{ANO_FIM})\")\n",
        "print(f\"{'=' * 60}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c1201bac",
      "metadata": {},
      "source": [
        "### 3a. Agregação: Microdados → Painel Mensal por Ocupação\n",
        "\n",
        "Agregar os microdados de movimentação (nível individual) em um painel mensal por ocupação CBO (4 dígitos). Cada linha do painel representará uma ocupação-mês com métricas agregadas.\n",
        "\n",
        "#### Estratégia de agregação\n",
        "\n",
        "Seguindo a abordagem de Hui, Reshef & Zhou (2024), construímos um painel ao nível de **ocupação × mês** com as seguintes métricas:\n",
        "\n",
        "| Métrica | Cálculo | Descrição |\n",
        "|---------|---------|-----------|\n",
        "| `admissoes` | Contagem de `saldo_movimentacao == 1` | Fluxo de contratação |\n",
        "| `desligamentos` | Contagem de `saldo_movimentacao == -1` | Fluxo de demissão |\n",
        "| `saldo` | `admissoes - desligamentos` | Criação líquida de empregos |\n",
        "| `salario_medio_adm` | Média do `salario_mensal` (admissões) | Nível salarial |\n",
        "| `salario_mediano_adm` | Mediana do `salario_mensal` (admissões) | Robustez a outliers |\n",
        "| `pct_mulher_adm` | % de `sexo == 3` nas admissões | Composição de gênero |\n",
        "| `pct_superior_adm` | % com `grau_instrucao >= 9` | Composição educacional |\n",
        "\n",
        "> **Nota — CBO 4 dígitos:** A CBO tem 6 dígitos (XXXX-XX), onde os 4 primeiros definem a \"família\" ocupacional. Para o crosswalk com ISCO-08, usamos os 4 primeiros dígitos (família CBO ≈ unit group ISCO-08).\n",
        "\n",
        "> **Nota — Otimização de memória:** O processamento é feito ano a ano para evitar OOM (Out-of-Memory) com ~100M+ registros. Flags booleanos são pré-computados como float para permitir agregação vetorizada (evitando lambdas lentas). A mediana é calculada separadamente por ser computacionalmente cara.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c45a92c2",
      "metadata": {},
      "source": [
        "**Validação da codificação `sexo` (CAGED/Base dos Dados):** Confirmar que os valores são 1 = Masculino e 3 = Feminino (não há código 2). O agregado `pct_mulher_adm` usa `sexo == 3`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "c349d573",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "sexo — value_counts (ano 2024):\n",
            "sexo\n",
            "1    28494836\n",
            "3    20500968\n",
            "9         236\n",
            "Name: count, dtype: int64\n",
            "  Esperado: 1 = Masculino, 3 = Feminino (Base dos Dados/CAGED).\n"
          ]
        }
      ],
      "source": [
        "# Conferir codificação de sexo nos microdados CAGED (um ano como amostra)\n",
        "_ano_amostra = 2024\n",
        "_path_amostra = DATA_RAW / f\"caged_{_ano_amostra}.parquet\"\n",
        "if not _path_amostra.exists():\n",
        "    _ano_amostra = ANO_INICIO\n",
        "    _path_amostra = DATA_RAW / f\"caged_{_ano_amostra}.parquet\"\n",
        "if _path_amostra.exists():\n",
        "    _df_sexo = pd.read_parquet(_path_amostra, columns=[\"sexo\"])\n",
        "    print(f\"sexo — value_counts (ano {_ano_amostra}):\")\n",
        "    print(_df_sexo[\"sexo\"].value_counts().sort_index())\n",
        "    print(\"  Esperado: 1 = Masculino, 3 = Feminino (Base dos Dados/CAGED).\")\n",
        "else:\n",
        "    print(\"Nenhum parquet de microdados encontrado para verificar sexo.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "cea3a3ad",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Painel carregado do checkpoint: painel_caged_mensal.parquet\n",
            "  32,988 linhas, 629 ocupações, 54 períodos\n",
            "\n",
            "Painel final: 32,988 linhas\n",
            "  Ocupações: 629, Períodos: 54\n",
            "  Shape: (32988, 49)\n"
          ]
        }
      ],
      "source": [
        "# Etapa 2a.3a — Agregação: Microdados → Painel Mensal por Ocupação\n",
        "# Checkpoint: se o painel já existe, carrega direto.\n",
        "\n",
        "if not KEEP_PANEL_MENSAL and PAINEL_MENSAL_FILE.exists():\n",
        "    PAINEL_MENSAL_FILE.unlink()\n",
        "    print(f\"Cache removido: {PAINEL_MENSAL_FILE.name}\")\n",
        "\n",
        "if PAINEL_MENSAL_FILE.exists():\n",
        "    painel = pd.read_parquet(PAINEL_MENSAL_FILE)\n",
        "    print(f\"Painel carregado do checkpoint: {PAINEL_MENSAL_FILE.name}\")\n",
        "    print(f\"  {len(painel):,} linhas, {painel['cbo_4d'].nunique()} ocupações, \"\n",
        "          f\"{painel['periodo'].nunique()} períodos\")\n",
        "else:\n",
        "    print(\"Construindo painel a partir dos microdados (ano a ano)...\")\n",
        "    paineis_anuais = []\n",
        "\n",
        "    for ano in range(ANO_INICIO, ANO_FIM + 1):\n",
        "        print(f\"\\n  Processando {ano}...\", flush=True)\n",
        "        df = pd.read_parquet(DATA_RAW / f\"caged_{ano}.parquet\")\n",
        "\n",
        "        # CBO 4 dígitos\n",
        "        df['cbo_2002'] = df['cbo_2002'].astype(str).str.strip()\n",
        "        df['cbo_4d'] = df['cbo_2002'].str[:4]\n",
        "        df = df[df['cbo_4d'].str.len() == 4]\n",
        "        df = df[df['cbo_4d'].str.isdigit()]\n",
        "        df = df[~df['cbo_4d'].isin(['0000'])]\n",
        "\n",
        "        # Variáveis temporais\n",
        "        df['periodo'] = df['ano'].astype(str) + '-' + df['mes'].astype(str).str.zfill(2)\n",
        "        df['periodo_num'] = df['ano'].astype(int) * 100 + df['mes'].astype(int)\n",
        "        df['post'] = (df['periodo_num'] >= ANO_TRATAMENTO * 100 + MES_TRATAMENTO).astype(int)\n",
        "\n",
        "        # Flags booleanos (CAGED: sexo 1=Masc, 3=Fem; alinhado à Etapa 1a)\n",
        "        CODIGO_SEXO_MULHER = 3\n",
        "        CODIGOS_RACA_BRANCA, CODIGOS_RACA_NEGRA = [1], [2, 4]\n",
        "        IDADE_CORTE_JOVEM = 29\n",
        "        CODIGOS_ESCOLARIDADE_SUPERIOR = ['9', '10', '11', '12', '13']\n",
        "        df['is_mulher'] = (df['sexo'].astype(str) == str(CODIGO_SEXO_MULHER)).astype(float)\n",
        "        raca_str = df['raca_cor'].astype(str)\n",
        "        df['is_branco'] = raca_str.isin([str(c) for c in CODIGOS_RACA_BRANCA]).astype(float)\n",
        "        df['is_negro'] = raca_str.isin([str(c) for c in CODIGOS_RACA_NEGRA]).astype(float)\n",
        "        df['is_jovem'] = (df['idade'] <= IDADE_CORTE_JOVEM).astype(float)\n",
        "        df['is_superior'] = df['grau_instrucao'].astype(str).isin(CODIGOS_ESCOLARIDADE_SUPERIOR).astype(float)\n",
        "        # Setor tecnológico (CNAE 2.0 seção: J = Informação e comunicação, etc.)\n",
        "        df['is_setor_tech'] = df['cnae_2_secao'].astype(str).str.strip().isin(CNAE_SECOES_TECNOLOGICO).astype(float)\n",
        "\n",
        "        # Separar admissões e desligamentos\n",
        "        df_adm = df[df['saldo_movimentacao'] == 1].copy()\n",
        "        df_desl = df[df['saldo_movimentacao'] == -1]\n",
        "        print(f\"    {ano}: {len(df_adm):,} admissões, {len(df_desl):,} desligamentos\", flush=True)\n",
        "\n",
        "        # Colunas de salário mascaradas (média condicional por grupo)\n",
        "        df_adm['sal_mulher'] = np.where(df_adm['is_mulher'] == 1, df_adm['salario_mensal'], np.nan)\n",
        "        df_adm['sal_homem'] = np.where(df_adm['is_mulher'] == 0, df_adm['salario_mensal'], np.nan)\n",
        "        df_adm['sal_branco'] = np.where(df_adm['is_branco'] == 1, df_adm['salario_mensal'], np.nan)\n",
        "        df_adm['sal_negro'] = np.where(df_adm['is_negro'] == 1, df_adm['salario_mensal'], np.nan)\n",
        "        df_adm['sal_jovem'] = np.where(df_adm['is_jovem'] == 1, df_adm['salario_mensal'], np.nan)\n",
        "        df_adm['sal_naojovem'] = np.where(df_adm['is_jovem'] == 0, df_adm['salario_mensal'], np.nan)\n",
        "        df_adm['sal_sup'] = np.where(df_adm['is_superior'] == 1, df_adm['salario_mensal'], np.nan)\n",
        "        df_adm['sal_med'] = np.where(df_adm['is_superior'] == 0, df_adm['salario_mensal'], np.nan)\n",
        "        df_adm['is_homem'] = 1 - df_adm['is_mulher']\n",
        "\n",
        "        # Agregar admissões + heterogeneidade demográfica\n",
        "        painel_adm = df_adm.groupby(['cbo_4d', 'ano', 'mes']).agg(\n",
        "            admissoes=('saldo_movimentacao', 'count'),\n",
        "            salario_medio_adm=('salario_mensal', 'mean'),\n",
        "            idade_media_adm=('idade', 'mean'),\n",
        "            pct_mulher_adm=('is_mulher', 'mean'),\n",
        "            pct_superior_adm=('is_superior', 'mean'),\n",
        "            pct_branco_adm=('is_branco', 'mean'),\n",
        "            pct_negro_adm=('is_negro', 'mean'),\n",
        "            pct_jovem_adm=('is_jovem', 'mean'),\n",
        "            pct_tecnologico_adm=('is_setor_tech', 'mean'),\n",
        "            salario_medio_mulher=('sal_mulher', 'mean'),\n",
        "            salario_medio_homem=('sal_homem', 'mean'),\n",
        "            salario_medio_branco=('sal_branco', 'mean'),\n",
        "            salario_medio_negro=('sal_negro', 'mean'),\n",
        "            salario_medio_jovem=('sal_jovem', 'mean'),\n",
        "            salario_medio_naojovem=('sal_naojovem', 'mean'),\n",
        "            salario_medio_superior=('sal_sup', 'mean'),\n",
        "            salario_medio_medio=('sal_med', 'mean'),\n",
        "            admissoes_mulher=('is_mulher', 'sum'),\n",
        "            admissoes_homem=('is_homem', 'sum'),\n",
        "            admissoes_jovem=('is_jovem', 'sum'),\n",
        "            admissoes_negro=('is_negro', 'sum'),\n",
        "        ).reset_index()\n",
        "\n",
        "        # Mediana separada (performance)\n",
        "        mediana = df_adm.groupby(['cbo_4d', 'ano', 'mes'])['salario_mensal'].median().reset_index()\n",
        "        mediana.columns = ['cbo_4d', 'ano', 'mes', 'salario_mediano_adm']\n",
        "        painel_adm = painel_adm.merge(mediana, on=['cbo_4d', 'ano', 'mes'], how='left')\n",
        "\n",
        "        # Agregar desligamentos\n",
        "        painel_desl = df_desl.groupby(['cbo_4d', 'ano', 'mes']).agg(\n",
        "            desligamentos=('saldo_movimentacao', 'count'),\n",
        "            salario_medio_desl=('salario_mensal', 'mean'),\n",
        "        ).reset_index()\n",
        "\n",
        "        # Merge\n",
        "        p = painel_adm.merge(painel_desl, on=['cbo_4d', 'ano', 'mes'], how='outer').fillna(0)\n",
        "        p['saldo'] = p['admissoes'] - p['desligamentos']\n",
        "        p['n_movimentacoes'] = p['admissoes'] + p['desligamentos']\n",
        "        p['setor_tecnologico'] = (p['pct_tecnologico_adm'] >= SETOR_TECNOLOGICO_LIMIAR).astype(int)\n",
        "        p['periodo'] = p['ano'].astype(int).astype(str) + '-' + p['mes'].astype(int).astype(str).str.zfill(2)\n",
        "        p['periodo_num'] = p['ano'].astype(int) * 100 + p['mes'].astype(int)\n",
        "        p['post'] = (p['periodo_num'] >= ANO_TRATAMENTO * 100 + MES_TRATAMENTO).astype(int)\n",
        "        p['ln_admissoes'] = np.log(p['admissoes'] + 1)\n",
        "        p['ln_desligamentos'] = np.log(p['desligamentos'] + 1)\n",
        "        p['ln_salario_adm'] = np.log(p['salario_medio_adm'].clip(lower=1))\n",
        "        p['cbo_2d'] = p['cbo_4d'].str[:2]\n",
        "        # Logs de heterogeneidade (salários e admissões por grupo)\n",
        "        for grp in ['mulher', 'homem', 'branco', 'negro', 'jovem', 'naojovem', 'superior', 'medio']:\n",
        "            col = f'salario_medio_{grp}'\n",
        "            if col in p.columns:\n",
        "                p[f'ln_salario_{grp}'] = np.log(p[col].clip(lower=1))\n",
        "        for grp in ['mulher', 'homem', 'jovem', 'negro']:\n",
        "            col = f'admissoes_{grp}'\n",
        "            if col in p.columns:\n",
        "                p[f'ln_admissoes_{grp}'] = np.log(p[col].astype(float) + 1)\n",
        "\n",
        "        paineis_anuais.append(p)\n",
        "        print(f\"    → {len(p):,} linhas no painel\", flush=True)\n",
        "\n",
        "    painel = pd.concat(paineis_anuais, ignore_index=True)\n",
        "    painel.to_parquet(PAINEL_MENSAL_FILE, index=False)\n",
        "    print(f\"\\nPainel salvo: {PAINEL_MENSAL_FILE.name}\")\n",
        "\n",
        "print(f\"\\nPainel final: {len(painel):,} linhas\")\n",
        "print(f\"  Ocupações: {painel['cbo_4d'].nunique()}, Períodos: {painel['periodo'].nunique()}\")\n",
        "print(f\"  Shape: {painel.shape}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d1e75561",
      "metadata": {},
      "source": [
        "### 3b. Verificar painel agregado (CHECKPOINT)\n",
        "\n",
        "Verificar integridade do painel: dimensões, balanceamento (ocupações × períodos), cobertura temporal, distribuição de variáveis-chave e série temporal agregada.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "f3b53806",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "CHECKPOINT — Painel Ocupação × Mês\n",
            "============================================================\n",
            "\n",
            "Ocupações: 629\n",
            "Períodos: 54\n",
            "Painel teórico (balanceado): 33,966\n",
            "Painel real: 32,988\n",
            "Balanceamento: 97.1%\n",
            "\n",
            "Meses por ocupação:\n",
            "  Min: 2, Max: 54, Média: 52.4\n",
            "  Ocupações com < 12 meses: 9\n",
            "  Ocupações com todos os 54 meses: 589\n",
            "\n",
            "Estatísticas descritivas:\n",
            "       admissoes  desligamentos    saldo  salario_medio_adm\n",
            "count    32988.0        32988.0  32988.0            32988.0\n",
            "mean      3153.5         2873.6    279.9             6131.1\n",
            "std      13880.0        12450.5   2323.0           158475.4\n",
            "min          0.0            0.0 -46650.0                0.0\n",
            "25%         60.0           60.0    -16.0             1782.8\n",
            "50%        293.0          289.0      7.0             2371.3\n",
            "75%       1338.0         1264.0    107.0             3811.2\n",
            "max     289900.0       284338.0  90556.0         18799538.2\n",
            "\n",
            "Série temporal (primeiros e últimos 3 meses):\n",
            " periodo_num  total_adm  total_desl   sal_medio\n",
            "      202101    1550075     1293016 3697.157086\n",
            "      202102    1715425     1317510 3723.743109\n",
            "      202103    1626885     1450555 3252.934263\n",
            "...\n",
            " periodo_num  total_adm  total_desl    sal_medio\n",
            "      202504    2282187     2024659  4011.260301\n",
            "      202505    2256225     2107233  4441.730675\n",
            "      202506    2139182     1972561 11519.380585\n"
          ]
        }
      ],
      "source": [
        "# Etapa 2a.3b — CHECKPOINT: Verificar painel agregado\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"CHECKPOINT — Painel Ocupação × Mês\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# 1. Dimensões\n",
        "n_ocup = painel['cbo_4d'].nunique()\n",
        "n_periodos = painel['periodo'].nunique()\n",
        "print(f\"\\nOcupações: {n_ocup}\")\n",
        "print(f\"Períodos: {n_periodos}\")\n",
        "print(f\"Painel teórico (balanceado): {n_ocup * n_periodos:,}\")\n",
        "print(f\"Painel real: {len(painel):,}\")\n",
        "print(f\"Balanceamento: {len(painel) / (n_ocup * n_periodos):.1%}\")\n",
        "\n",
        "# 2. Ocupações com poucos meses\n",
        "ocup_meses = painel.groupby('cbo_4d')['periodo'].nunique()\n",
        "print(f\"\\nMeses por ocupação:\")\n",
        "print(f\"  Min: {ocup_meses.min()}, Max: {ocup_meses.max()}, Média: {ocup_meses.mean():.1f}\")\n",
        "print(f\"  Ocupações com < 12 meses: {(ocup_meses < 12).sum()}\")\n",
        "print(f\"  Ocupações com todos os {n_periodos} meses: {(ocup_meses == n_periodos).sum()}\")\n",
        "\n",
        "# 3. Estatísticas descritivas\n",
        "print(\"\\nEstatísticas descritivas:\")\n",
        "print(painel[['admissoes', 'desligamentos', 'saldo', 'salario_medio_adm']].describe().round(1))\n",
        "\n",
        "# 4. Série temporal agregada\n",
        "ts = painel.groupby('periodo_num').agg(\n",
        "    total_adm=('admissoes', 'sum'),\n",
        "    total_desl=('desligamentos', 'sum'),\n",
        "    sal_medio=('salario_medio_adm', 'mean'),\n",
        ").reset_index()\n",
        "print(\"\\nSérie temporal (primeiros e últimos 3 meses):\")\n",
        "print(ts.head(3).to_string(index=False))\n",
        "print(\"...\")\n",
        "print(ts.tail(3).to_string(index=False))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4e2cbb00",
      "metadata": {},
      "source": [
        "#### Verificação: outlier salarial em Jun/2025\n",
        "\n",
        "A série temporal agregada no checkpoint acima pode mostrar salário médio elevado em Jun/2025 (~3× os meses anteriores). Abaixo verificamos se isso reflete **(i)** poucas células ocupação×mês com salário muito alto e/ou poucas movimentações, **(ii)** possível publicação parcial do mês, ou **(iii)** padrão real dos dados. Conforme o diagnóstico, pode-se documentar, filtrar células com poucas movimentações ou truncar a janela em Mai/2025."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "713ec067",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Jun/2025 — Distribuição de salario_medio_adm (células ocupação×mês):\n",
            "count        615.00\n",
            "mean       11519.38\n",
            "std       170295.09\n",
            "min            0.00\n",
            "50%         2647.30\n",
            "90%         7652.32\n",
            "95%        10449.24\n",
            "99%        38462.94\n",
            "max      4216672.39\n",
            "Name: salario_medio_adm, dtype: float64\n",
            "\n",
            "Células com n_movimentacoes < 50 e salario_medio_adm > R$ 10,000: 9\n",
            "  Admissões nessas células: 125 (0.01% do total de Jun/2025)\n",
            "cbo_4d  admissoes  n_movimentacoes  salario_medio_adm\n",
            "  2423          1                1       80466.670000\n",
            "  1237         22               48       43743.163182\n",
            "  1222         19               49       41853.112105\n",
            "  1234         15               36       38843.381333\n",
            "  1221          6               15       35815.645000\n",
            "  1223         19               36       14490.656842\n",
            "  2422          1                1       14097.220000\n",
            "  2622         24               48       12752.264583\n",
            "  1031         18               20       10953.792222\n",
            "\n",
            "Comparativo 2025:\n",
            "  Mai/2025: células=614, total_adm=2,256,225, sal_medio(agg)=4,442\n",
            "  Jun/2025: células=615, total_adm=2,139,182, sal_medio(agg)=11,519\n",
            "  Média mensal 2025: células~306, total_adm~2,293,843\n"
          ]
        }
      ],
      "source": [
        "# Diagnóstico: Jun/2025 — distribuição de salario_medio_adm e células extremas\n",
        "jun = painel[painel['periodo_num'] == 202506].copy()\n",
        "print(\"Jun/2025 — Distribuição de salario_medio_adm (células ocupação×mês):\")\n",
        "print(jun['salario_medio_adm'].describe(percentiles=[0.5, 0.9, 0.95, 0.99]).round(2))\n",
        "print()\n",
        "\n",
        "# Células com poucas movimentações e salário alto\n",
        "limiar_mov = 50\n",
        "limiar_sal = 10_000\n",
        "mask_extremo = (jun['n_movimentacoes'] < limiar_mov) & (jun['salario_medio_adm'] > limiar_sal)\n",
        "n_extremo = mask_extremo.sum()\n",
        "adm_jun_total = jun['admissoes'].sum()\n",
        "adm_extremo = jun.loc[mask_extremo, 'admissoes'].sum()\n",
        "print(f\"Células com n_movimentacoes < {limiar_mov} e salario_medio_adm > R$ {limiar_sal:,.0f}: {n_extremo}\")\n",
        "print(f\"  Admissões nessas células: {adm_extremo:,} ({100*adm_extremo/adm_jun_total:.2f}% do total de Jun/2025)\")\n",
        "if n_extremo > 0:\n",
        "    print(jun.loc[mask_extremo, ['cbo_4d', 'admissoes', 'n_movimentacoes', 'salario_medio_adm']].sort_values('salario_medio_adm', ascending=False).head(15).to_string(index=False))\n",
        "print()\n",
        "\n",
        "# Comparação com Mai/2025 e média 2025\n",
        "mai = painel[painel['periodo_num'] == 202505]\n",
        "ano2025 = painel[painel['periodo_num'] // 100 == 2025]\n",
        "print(\"Comparativo 2025:\")\n",
        "print(f\"  Mai/2025: células={len(mai)}, total_adm={mai['admissoes'].sum():,.0f}, sal_medio(agg)={mai['salario_medio_adm'].mean():,.0f}\")\n",
        "print(f\"  Jun/2025: células={len(jun)}, total_adm={adm_jun_total:,.0f}, sal_medio(agg)={jun['salario_medio_adm'].mean():,.0f}\")\n",
        "print(f\"  Média mensal 2025: células~{len(ano2025)//12:.0f}, total_adm~{ano2025.groupby('periodo_num')['admissoes'].sum().mean():,.0f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9935501e",
      "metadata": {},
      "source": [
        "### 4a. Crosswalk CBO 2002 → ISCO-08\n",
        "\n",
        "Mapear os códigos CBO 2002 (usados no CAGED) para ISCO-08 (usados no índice ILO). **Esta é a etapa metodologicamente mais delicada do pipeline.**\n",
        "\n",
        "#### Contexto\n",
        "\n",
        "A CBO 2002 foi construída com base na ISCO-88/ISCO-08, compartilhando a mesma estrutura hierárquica:\n",
        "\n",
        "| Nível | CBO 2002 | ISCO-08 | Alinhamento |\n",
        "|-------|----------|---------|-------------|\n",
        "| 1 dígito | Grande Grupo (10) | Major Group (10) | Perfeito |\n",
        "| 2 dígitos | Subgrupo Principal (~46) | Sub-major Group (43) | Bom (14 CBOs sem match direto) |\n",
        "| 3 dígitos | Subgrupo (~194) | Minor Group (130) | Parcial (~45% direto) |\n",
        "| 4 dígitos | Família (~629) | Unit Group (427) | Divergente (~28% direto) |\n",
        "\n",
        "#### Estratégia adotada: Dual (2d principal + 4d robustez)\n",
        "\n",
        "**PARTE A — Especificação PRINCIPAL (2 dígitos):**\n",
        "- CBO 2d → ISCO-08 Sub-major Group (match direto)\n",
        "- Fallback: CBO 1d → ISCO-08 Major Group (média)\n",
        "- Cobertura esperada: **100%**\n",
        "\n",
        "**PARTE B — Especificação de ROBUSTEZ (4 dígitos, fallback hierárquico em 6 níveis):**\n",
        "\n",
        "| Nível | Estratégia | Cobertura esperada |\n",
        "|-------|------------|--------------------|\n",
        "| N1 | CBO 4d = ISCO-08 4d (match direto) | ~28% |\n",
        "| N2 | CBO 4d = ISCO-88 4d → ISCO-08 via correspondência oficial | ~+9% |\n",
        "| N3 | CBO 3d = ISCO-08 3d (média Minor Group) | ~+20% |\n",
        "| N4 | CBO 3d = ISCO-88 3d → ISCO-08 via correspondência | ~+1% |\n",
        "| N5 | CBO 2d = ISCO-08 2d (= especificação principal) | ~+18% |\n",
        "| N6 | CBO 1d = ISCO-08 1d (média Major Group) | ~+24% |\n",
        "\n",
        "> **Nota — Muendler (CBO 1994):** O arquivo `cbo-isco-conc.csv` de Muendler & Poole (2004) mapeia CBO **1994** → ISCO-88, NÃO a CBO 2002 usada no CAGED. Por isso, o match 4d via Muendler é limitado. A estratégia principal utiliza a similaridade estrutural entre CBO 2002 e ISCO-08/88 com fallback hierárquico.\n",
        "\n",
        "> **Nota sobre atenuação:** Se o crosswalk a 4 dígitos introduz erro de medição, o efeito típico é **atenuação** (viés em direção a zero). Encontrar efeito significativo mesmo com erro de medição sugere que o efeito real é provavelmente maior.\n",
        "\n",
        "**Validação do crosswalk:** O notebook não usa Muendler para o match principal; utiliza a correspondência oficial ISCO-08↔88 e fallback hierárquico. Na especificação 2d: match direto CBO 2d → ISCO-08 Sub-major Group, com fallback a 1 dígito (Major Group). Na 4d: fallback em 6 níveis (N1→N6). Resultado verificado: cobertura 100% em 2d e 4d, correlação entre exposure_score_2d e exposure_score_4d ~0,915 — consistente com o planejamento e pronto para o DiD no Notebook 2b."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "5fdb8d1d",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Crosswalk carregado do checkpoint: painel_caged_crosswalk.parquet\n",
            "  32,988 linhas, cobertura 2d: 100.0%, 4d: 100.0%\n"
          ]
        }
      ],
      "source": [
        "# Etapa 2a.4a — Crosswalk CBO 2002 → ISCO-08 (Dual: 2d principal + 4d robustez)\n",
        "# Checkpoint: se o painel com crosswalk já existe, carrega direto.\n",
        "\n",
        "if not KEEP_PANEL_CROSSWALK and PAINEL_CROSSWALK_FILE.exists():\n",
        "    PAINEL_CROSSWALK_FILE.unlink()\n",
        "    print(f\"Cache removido: {PAINEL_CROSSWALK_FILE.name}\")\n",
        "\n",
        "if PAINEL_CROSSWALK_FILE.exists():\n",
        "    painel = pd.read_parquet(PAINEL_CROSSWALK_FILE)\n",
        "    print(f\"Crosswalk carregado do checkpoint: {PAINEL_CROSSWALK_FILE.name}\")\n",
        "    print(f\"  {len(painel):,} linhas, cobertura 2d: {painel['exposure_score_2d'].notna().mean():.1%}, \"\n",
        "          f\"4d: {painel['exposure_score_4d'].notna().mean():.1%}\")\n",
        "else:\n",
        "    # ══════════════════════════════════════════════════════════════════════\n",
        "    # Carregar dados de referência\n",
        "    # ══════════════════════════════════════════════════════════════════════\n",
        "    df_ilo = pd.read_csv(ILO_FILE)\n",
        "    df_ilo['isco_08_str'] = df_ilo['isco_08'].astype(str).str.zfill(4)\n",
        "    print(f\"Índice ILO carregado: {len(df_ilo)} ocupações ISCO-08\")\n",
        "    print(f\"  Score range: [{df_ilo['exposure_score'].min():.3f}, {df_ilo['exposure_score'].max():.3f}]\")\n",
        "\n",
        "    # Dicts ILO em múltiplos níveis\n",
        "    codes = df_ilo['isco_08_str']\n",
        "    ilo_4d = df_ilo.groupby('isco_08_str')['exposure_score'].mean().to_dict()\n",
        "    ilo_3d = df_ilo.assign(g=codes.str[:3]).groupby('g')['exposure_score'].mean().to_dict()\n",
        "    ilo_2d = df_ilo.assign(g=codes.str[:2]).groupby('g')['exposure_score'].mean().to_dict()\n",
        "    ilo_1d = df_ilo.assign(g=codes.str[:1]).groupby('g')['exposure_score'].mean().to_dict()\n",
        "\n",
        "    # Correspondência ISCO-08 ↔ ISCO-88 (arquivo local)\n",
        "    isco88_to_08 = {}\n",
        "    isco88_3d_to_08_3d = {}\n",
        "    if ISCO_08_88_FILE.exists():\n",
        "        df_corr = pd.read_excel(ISCO_08_88_FILE, sheet_name='ISCO-08 to 88')\n",
        "        df_corr['isco08_4d'] = df_corr['ISCO-08 code'].astype(str).str.strip().str.zfill(4)\n",
        "        df_corr['isco88_4d'] = df_corr['ISCO-88 code'].astype(str).str.strip().str.zfill(4)\n",
        "        isco88_to_08 = df_corr.groupby('isco88_4d')['isco08_4d'].apply(list).to_dict()\n",
        "        df_corr['isco88_3d'] = df_corr['isco88_4d'].str[:3]\n",
        "        df_corr['isco08_3d'] = df_corr['isco08_4d'].str[:3]\n",
        "        isco88_3d_to_08_3d = df_corr.groupby('isco88_3d')['isco08_3d'].apply(\n",
        "            lambda x: list(set(x))).to_dict()\n",
        "        print(f\"  Correspondência ISCO-08↔88: {len(df_corr)} mapeamentos\")\n",
        "\n",
        "    # ══════════════════════════════════════════════════════════════════════\n",
        "    # PARTE A: 2 dígitos (PRINCIPAL)\n",
        "    # ══════════════════════════════════════════════════════════════════════\n",
        "    print(f\"\\n{'='*60}\\nPARTE A: Crosswalk 2 dígitos (PRINCIPAL)\\n{'='*60}\")\n",
        "\n",
        "    painel['exposure_score_2d'] = painel['cbo_2d'].map(ilo_2d)\n",
        "    painel['match_level_2d'] = np.where(painel['exposure_score_2d'].notna(), '2-digit', None)\n",
        "\n",
        "    # Fallback a 1 dígito\n",
        "    mask_na = painel['exposure_score_2d'].isna()\n",
        "    if mask_na.any():\n",
        "        cbo_1d = painel.loc[mask_na, 'cbo_4d'].str[:1]\n",
        "        painel.loc[mask_na, 'exposure_score_2d'] = cbo_1d.map(ilo_1d).values\n",
        "        painel.loc[mask_na, 'match_level_2d'] = '1-digit (fallback)'\n",
        "\n",
        "    painel['exposure_score'] = painel['exposure_score_2d']\n",
        "    cov_2d = painel['exposure_score_2d'].notna().mean()\n",
        "    print(f\"  COBERTURA 2d: {cov_2d:.1%}\")\n",
        "    for lvl, cnt in painel['match_level_2d'].value_counts().items():\n",
        "        print(f\"    {lvl}: {cnt:,} ({cnt/len(painel):.1%})\")\n",
        "\n",
        "    # ══════════════════════════════════════════════════════════════════════\n",
        "    # PARTE B: 4 dígitos (ROBUSTEZ) — fallback hierárquico 6 níveis\n",
        "    # ══════════════════════════════════════════════════════════════════════\n",
        "    print(f\"\\n{'='*60}\\nPARTE B: Crosswalk 4 dígitos (ROBUSTEZ)\\n{'='*60}\")\n",
        "\n",
        "    cbos_unicos = sorted(painel['cbo_4d'].unique())\n",
        "    cbo_score_4d = {}\n",
        "    cbo_match_level = {}\n",
        "    counts = {'N1': 0, 'N2': 0, 'N3': 0, 'N4': 0, 'N5': 0, 'N6': 0, 'sem': 0}\n",
        "\n",
        "    for cbo in cbos_unicos:\n",
        "        score, level = None, None\n",
        "\n",
        "        # N1: CBO 4d = ISCO-08 4d\n",
        "        if cbo in ilo_4d:\n",
        "            score, level = ilo_4d[cbo], 'N1: ISCO-08 4d direto'\n",
        "            counts['N1'] += 1\n",
        "        # N2: CBO 4d = ISCO-88 4d → ISCO-08\n",
        "        if score is None and cbo in isco88_to_08:\n",
        "            scores_c = [ilo_4d[c] for c in isco88_to_08[cbo] if c in ilo_4d]\n",
        "            if scores_c:\n",
        "                score, level = np.mean(scores_c), 'N2: via ISCO-88→08 4d'\n",
        "                counts['N2'] += 1\n",
        "        # N3: CBO 3d = ISCO-08 3d\n",
        "        if score is None and cbo[:3] in ilo_3d:\n",
        "            score, level = ilo_3d[cbo[:3]], 'N3: ISCO-08 3d'\n",
        "            counts['N3'] += 1\n",
        "        # N4: CBO 3d = ISCO-88 3d → ISCO-08 3d\n",
        "        if score is None and cbo[:3] in isco88_3d_to_08_3d:\n",
        "            scores_c = [ilo_3d[c] for c in isco88_3d_to_08_3d[cbo[:3]] if c in ilo_3d]\n",
        "            if scores_c:\n",
        "                score, level = np.mean(scores_c), 'N4: via ISCO-88→08 3d'\n",
        "                counts['N4'] += 1\n",
        "        # N5: CBO 2d = ISCO-08 2d\n",
        "        if score is None and cbo[:2] in ilo_2d:\n",
        "            score, level = ilo_2d[cbo[:2]], 'N5: ISCO-08 2d'\n",
        "            counts['N5'] += 1\n",
        "        # N6: CBO 1d = ISCO-08 1d\n",
        "        if score is None and cbo[:1] in ilo_1d:\n",
        "            score, level = ilo_1d[cbo[:1]], 'N6: ISCO-08 1d'\n",
        "            counts['N6'] += 1\n",
        "\n",
        "        if score is not None:\n",
        "            cbo_score_4d[cbo] = score\n",
        "            cbo_match_level[cbo] = level\n",
        "        else:\n",
        "            counts['sem'] += 1\n",
        "\n",
        "    painel['exposure_score_4d'] = painel['cbo_4d'].map(cbo_score_4d)\n",
        "    painel['match_level_4d'] = painel['cbo_4d'].map(cbo_match_level)\n",
        "\n",
        "    total = len(cbos_unicos)\n",
        "    print(f\"  CBOs 4d únicos: {total}\")\n",
        "    for k, v in counts.items():\n",
        "        if v > 0:\n",
        "            print(f\"    {k}: {v} ({v/total:.1%})\")\n",
        "    print(f\"  COBERTURA 4d: {painel['exposure_score_4d'].notna().mean():.1%}\")\n",
        "\n",
        "    # Correlação 2d vs 4d\n",
        "    df_check = painel[['cbo_4d', 'exposure_score_2d', 'exposure_score_4d']].drop_duplicates('cbo_4d')\n",
        "    corr = df_check['exposure_score_2d'].corr(df_check['exposure_score_4d'])\n",
        "    print(f\"\\n  Correlação Pearson (2d vs 4d): {corr:.4f}\")\n",
        "\n",
        "    painel.to_parquet(PAINEL_CROSSWALK_FILE, index=False)\n",
        "    print(f\"\\nSalvo: {PAINEL_CROSSWALK_FILE.name}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "639cc50d",
      "metadata": {},
      "source": [
        "**CBOs 2 dígitos sem match direto em ISCO-08:** Subgrupos principais CBO que não possuem equivalente direto em Sub-major Group ISCO-08; recebem score via fallback a 1 dígito (Major Group). Lista abaixo (a partir do painel com crosswalk)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "7a414cab",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CBOs 2d sem match direto (fallback a Major Group): 14\n",
            "Códigos: ['10', '20', '27', '30', '37', '39', '64', '76', '77', '78', '79', '84', '86', '99']\n",
            "\n",
            "Exemplo (cbo_2d=10): cbo_4d presentes: ['1010', '1011', '1020', '1021', '1030', '1031']...\n"
          ]
        }
      ],
      "source": [
        "# Listar CBO 2d que caem no fallback a 1 dígito (sem match direto em ISCO-08 Sub-major Group)\n",
        "if 'match_level_2d' in painel.columns:\n",
        "    fallback_2d = painel[painel['match_level_2d'] == '1-digit (fallback)']['cbo_2d'].unique()\n",
        "    fallback_2d = sorted(fallback_2d)\n",
        "    print(f\"CBOs 2d sem match direto (fallback a Major Group): {len(fallback_2d)}\")\n",
        "    print(\"Códigos:\", fallback_2d)\n",
        "    # Opcional: exemplos de cbo_4d por um desses 2d\n",
        "    if len(fallback_2d) > 0:\n",
        "        ex = painel[painel['cbo_2d'] == fallback_2d[0]][['cbo_2d', 'cbo_4d']].drop_duplicates()\n",
        "        print(f\"\\nExemplo (cbo_2d={fallback_2d[0]}): cbo_4d presentes: {ex['cbo_4d'].tolist()[:8]}...\")\n",
        "else:\n",
        "    print(\"Coluna match_level_2d não encontrada (crosswalk pode ter sido carregado sem essa coluna).\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "48dd6bcb",
      "metadata": {},
      "source": [
        "### 4b. Verificar crosswalk (CHECKPOINT)\n",
        "\n",
        "Validar qualidade do crosswalk nas DUAS especificações: principal (2 dígitos) e robustez (4 dígitos). Verificar cobertura, distribuição de scores, correlação entre especificações e sanity check por grande grupo CBO.\n",
        "\n",
        "**Critérios de aceite:**\n",
        "- Cobertura 2d ≥ 95% (esperado ~100%)\n",
        "- Cobertura 4d ≥ 80% (esperado ~100% com fallback)\n",
        "- Correlação 2d vs 4d > 0.8 (consistência entre especificações)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "de59ed6d",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "CHECKPOINT — Crosswalk CBO → ISCO-08 (Dual)\n",
            "============================================================\n",
            "\n",
            "--- Cobertura ---\n",
            "  2 dígitos (PRINCIPAL): 100.0%\n",
            "  4 dígitos (ROBUSTEZ):  100.0%\n",
            "\n",
            "--- Estatísticas dos scores ---\n",
            "\n",
            "exposure_score_2d (PRINCIPAL):\n",
            "count    32988.0000\n",
            "mean         0.2778\n",
            "std          0.1243\n",
            "min          0.1167\n",
            "25%          0.1658\n",
            "50%          0.2459\n",
            "75%          0.3725\n",
            "max          0.6325\n",
            "Name: exposure_score_2d, dtype: float64\n",
            "\n",
            "exposure_score_4d (ROBUSTEZ):\n",
            "count    32988.0000\n",
            "mean         0.2830\n",
            "std          0.1315\n",
            "min          0.0900\n",
            "25%          0.1658\n",
            "50%          0.2500\n",
            "75%          0.3650\n",
            "max          0.7000\n",
            "Name: exposure_score_4d, dtype: float64\n",
            "\n",
            "--- Correlação 2d vs 4d ---\n",
            "  Pearson: 0.9150\n",
            "  Alta correlação — bom sinal de consistência.\n",
            "\n",
            "--- Exposição por grande grupo CBO ---\n",
            "Grande Grupo                               Score 2d   Score 4d\n",
            "--------------------------------------------------------------\n",
            "  Dirigentes                                  0.367      0.375\n",
            "  Profissionais das ciências                  0.393      0.396\n",
            "  Técnicos nível médio                        0.334      0.338\n",
            "  Trabalhadores de serv. admin.               0.580      0.557\n",
            "  Trabalhadores de serviços/comércio          0.243      0.247\n",
            "  Agropecuária                                0.157      0.161\n",
            "  Produção industrial                         0.161      0.165\n",
            "  Operadores de máquinas                      0.213      0.217\n",
            "  Manutenção e reparação                      0.143      0.187\n",
            "  (!) = diferença > 0.1 entre 2d e 4d\n"
          ]
        }
      ],
      "source": [
        "# Etapa 2a.4b — CHECKPOINT: Verificar crosswalk CBO → ISCO-08\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"CHECKPOINT — Crosswalk CBO → ISCO-08 (Dual)\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# 1. Cobertura\n",
        "coverage_2d = painel['exposure_score_2d'].notna().mean()\n",
        "coverage_4d = painel['exposure_score_4d'].notna().mean()\n",
        "print(f\"\\n--- Cobertura ---\")\n",
        "print(f\"  2 dígitos (PRINCIPAL): {coverage_2d:.1%}\")\n",
        "print(f\"  4 dígitos (ROBUSTEZ):  {coverage_4d:.1%}\")\n",
        "if coverage_2d < 0.95:\n",
        "    print(f\"  ALERTA: Cobertura 2d abaixo de 95%!\")\n",
        "if coverage_4d < 0.80:\n",
        "    print(f\"  AVISO: Cobertura 4d abaixo de 80%.\")\n",
        "\n",
        "# 2. Estatísticas dos scores\n",
        "print(f\"\\n--- Estatísticas dos scores ---\")\n",
        "print(f\"\\nexposure_score_2d (PRINCIPAL):\")\n",
        "print(painel['exposure_score_2d'].describe().round(4))\n",
        "print(f\"\\nexposure_score_4d (ROBUSTEZ):\")\n",
        "print(painel['exposure_score_4d'].describe().round(4))\n",
        "\n",
        "# 3. Correlação\n",
        "mask_both = painel['exposure_score_2d'].notna() & painel['exposure_score_4d'].notna()\n",
        "if mask_both.any():\n",
        "    corr = painel.loc[mask_both, 'exposure_score_2d'].corr(\n",
        "        painel.loc[mask_both, 'exposure_score_4d'])\n",
        "    print(f\"\\n--- Correlação 2d vs 4d ---\")\n",
        "    print(f\"  Pearson: {corr:.4f}\")\n",
        "    print(f\"  {'Alta correlação — bom sinal de consistência.' if corr > 0.8 else 'Correlação moderada.'}\")\n",
        "\n",
        "# 4. Sanity check por grande grupo CBO\n",
        "painel['grande_grupo_cbo'] = painel['cbo_4d'].str[0]\n",
        "print(f\"\\n--- Exposição por grande grupo CBO ---\")\n",
        "print(f\"{'Grande Grupo':<40} {'Score 2d':>10} {'Score 4d':>10}\")\n",
        "print(\"-\" * 62)\n",
        "for gg, nome in sorted(GRANDES_GRUPOS_CBO.items()):\n",
        "    mask = painel['grande_grupo_cbo'] == gg\n",
        "    if mask.any():\n",
        "        s2d = painel.loc[mask, 'exposure_score_2d'].mean()\n",
        "        s4d = painel.loc[mask, 'exposure_score_4d'].mean()\n",
        "        s4d_str = f\"{s4d:.3f}\" if not np.isnan(s4d) else \"N/A\"\n",
        "        flag = \" (!)\" if not np.isnan(s4d) and abs(s2d - s4d) > 0.1 else \"\"\n",
        "        print(f\"  {nome:<38} {s2d:>10.3f} {s4d_str:>10}{flag}\")\n",
        "print(f\"  (!) = diferença > 0.1 entre 2d e 4d\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3bbe3871",
      "metadata": {},
      "source": [
        "### 5a. Definição de tratamento\n",
        "\n",
        "Definir as variáveis de tratamento para a análise DiD. O tratamento é baseado na **exposição ocupacional à IA generativa**: ocupações com alta exposição (top 20%) vs. baixa exposição.\n",
        "\n",
        "#### Variáveis criadas\n",
        "\n",
        "| Variável | Definição | Uso |\n",
        "|----------|-----------|-----|\n",
        "| `alta_exp` | 1 se `exposure_score_2d >= percentil 80` | **Especificação principal** |\n",
        "| `alta_exp_10` | 1 se `exposure_score_2d >= percentil 90` | Robustez (cutoff) |\n",
        "| `alta_exp_25` | 1 se `exposure_score_2d >= percentil 75` | Robustez (cutoff) |\n",
        "| `alta_exp_mediana` | 1 se `exposure_score_2d >= mediana` | Alternativa binária |\n",
        "| `quintil_exp` | Quintil de exposição (Q1–Q5) | Análise por quantil |\n",
        "| `alta_exp_4d` | 1 se `exposure_score_4d >= percentil 80` | Robustez (crosswalk 4d) |\n",
        "| `did` | `post × alta_exp` | Interação DiD principal |\n",
        "| `did_4d` | `post × alta_exp_4d` | Interação DiD robustez |\n",
        "\n",
        "> **Nota:** Os thresholds são calculados sobre a distribuição de **ocupações** (uma obs por CBO), não ponderada por volume de movimentações. Cada ocupação tem peso igual na definição do tratamento.\n",
        "\n",
        "> **Nota — Tratamento contínuo:** Além das dummies, `exposure_score_2d` e `exposure_score_4d` podem ser usados diretamente como tratamento contínuo em especificações alternativas, conforme Hui et al. (2024)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "8798f7cf",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Thresholds de exposição (2d, PRINCIPAL):\n",
            "  alta_exp_10: 0.4433 (78 ocupações, 12%)\n",
            "  alta_exp: 0.3854 (131 ocupações, 21%)\n",
            "  alta_exp_25: 0.3725 (165 ocupações, 26%)\n",
            "  alta_exp_mediana: 0.2459 (332 ocupações, 53%)\n",
            "\n",
            "Threshold 4d (p80): 0.3863 (137 ocupações)\n",
            "\n",
            "--- Distribuição de tratamento ---\n",
            "  Alta exp 2d (top 20%): 20.3% das obs\n",
            "  Alta exp 4d (top 20%): 21.3% das obs\n",
            "  Períodos pré:  14,058\n",
            "  Períodos pós:  18,930\n",
            "  Concordância 2d vs 4d: 93.1%\n",
            "\n",
            "Tabela de contingência (2d, principal):\n",
            "alta_exp  Controle  Tratamento    All\n",
            "post                                 \n",
            "Pré          11195        2863  14058\n",
            "Pós          15092        3838  18930\n",
            "All          26287        6701  32988\n"
          ]
        }
      ],
      "source": [
        "# Etapa 2a.5a — Definição de tratamento\n",
        "\n",
        "# ── Thresholds sobre a distribuição de ocupações (2d) ──\n",
        "ocup_scores_2d = painel.groupby('cbo_4d')['exposure_score_2d'].first().dropna()\n",
        "\n",
        "thresholds_2d = {\n",
        "    'alta_exp_10':      ocup_scores_2d.quantile(0.90),\n",
        "    'alta_exp':         ocup_scores_2d.quantile(0.80),  # PRINCIPAL\n",
        "    'alta_exp_25':      ocup_scores_2d.quantile(0.75),\n",
        "    'alta_exp_mediana':  ocup_scores_2d.quantile(0.50),\n",
        "}\n",
        "\n",
        "print(\"Thresholds de exposição (2d, PRINCIPAL):\")\n",
        "for name, val in thresholds_2d.items():\n",
        "    n_above = (ocup_scores_2d >= val).sum()\n",
        "    pct = n_above / len(ocup_scores_2d) * 100\n",
        "    print(f\"  {name}: {val:.4f} ({n_above} ocupações, {pct:.0f}%)\")\n",
        "\n",
        "# ── Dummies de tratamento 2d ──\n",
        "for name, threshold in thresholds_2d.items():\n",
        "    painel[name] = (painel['exposure_score_2d'] >= threshold).astype(int)\n",
        "\n",
        "# Quintis\n",
        "painel['quintil_exp'] = pd.qcut(\n",
        "    painel['exposure_score_2d'].rank(method='first'),\n",
        "    q=5,\n",
        "    labels=['Q1 (Baixa)', 'Q2', 'Q3', 'Q4', 'Q5 (Alta)']\n",
        ")\n",
        "\n",
        "# ── Dummies 4d (ROBUSTEZ) ──\n",
        "ocup_scores_4d = painel.groupby('cbo_4d')['exposure_score_4d'].first().dropna()\n",
        "threshold_4d_80 = ocup_scores_4d.quantile(0.80)\n",
        "painel['alta_exp_4d'] = (painel['exposure_score_4d'] >= threshold_4d_80).astype(int)\n",
        "print(f\"\\nThreshold 4d (p80): {threshold_4d_80:.4f} ({(ocup_scores_4d >= threshold_4d_80).sum()} ocupações)\")\n",
        "\n",
        "# ── Interações DiD ──\n",
        "painel['did'] = painel['post'] * painel['alta_exp']\n",
        "painel['did_4d'] = painel['post'] * painel['alta_exp_4d']\n",
        "\n",
        "# ── Resumo ──\n",
        "print(f\"\\n--- Distribuição de tratamento ---\")\n",
        "print(f\"  Alta exp 2d (top 20%): {painel['alta_exp'].mean():.1%} das obs\")\n",
        "print(f\"  Alta exp 4d (top 20%): {painel['alta_exp_4d'].mean():.1%} das obs\")\n",
        "print(f\"  Períodos pré:  {painel[painel['post']==0].shape[0]:,}\")\n",
        "print(f\"  Períodos pós:  {painel[painel['post']==1].shape[0]:,}\")\n",
        "\n",
        "concordancia = (painel['alta_exp'] == painel['alta_exp_4d']).mean()\n",
        "print(f\"  Concordância 2d vs 4d: {concordancia:.1%}\")\n",
        "\n",
        "# Tabela de contingência\n",
        "ct = pd.crosstab(\n",
        "    painel['post'].map({0: 'Pré', 1: 'Pós'}),\n",
        "    painel['alta_exp'].map({0: 'Controle', 1: 'Tratamento'}),\n",
        "    margins=True\n",
        ")\n",
        "print(f\"\\nTabela de contingência (2d, principal):\")\n",
        "print(ct)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5c7c990d",
      "metadata": {},
      "source": [
        "### 5b. Verificar tratamento (CHECKPOINT)\n",
        "\n",
        "Validar a definição de tratamento: top/bottom ocupações por exposição, distribuição por quintil, e concordância entre especificações 2d e 4d.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "bc1457bd",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "CHECKPOINT — Definição de Tratamento\n",
            "============================================================\n",
            "\n",
            "--- Top 10 ocupações MAIS expostas ---\n",
            "  CBO 4101: score=0.632, admissões=343,660  (Trabalhadores de serv. admin.)\n",
            "  CBO 4102: score=0.632, admissões=109,397  (Trabalhadores de serv. admin.)\n",
            "  CBO 4110: score=0.632, admissões=7,169,112  (Trabalhadores de serv. admin.)\n",
            "  CBO 4121: score=0.632, admissões=36,099  (Trabalhadores de serv. admin.)\n",
            "  CBO 4122: score=0.632, admissões=217,727  (Trabalhadores de serv. admin.)\n",
            "  CBO 4131: score=0.632, admissões=521,115  (Trabalhadores de serv. admin.)\n",
            "  CBO 4132: score=0.632, admissões=178,605  (Trabalhadores de serv. admin.)\n",
            "  CBO 4141: score=0.632, admissões=4,019,302  (Trabalhadores de serv. admin.)\n",
            "  CBO 4142: score=0.632, admissões=406,552  (Trabalhadores de serv. admin.)\n",
            "  CBO 4151: score=0.632, admissões=36,816  (Trabalhadores de serv. admin.)\n",
            "\n",
            "--- 10 ocupações MENOS expostas ---\n",
            "  CBO 9101: score=0.117, admissões=44,866  (Manutenção e reparação)\n",
            "  CBO 9102: score=0.117, admissões=13,108  (Manutenção e reparação)\n",
            "  CBO 9109: score=0.117, admissões=1,394  (Manutenção e reparação)\n",
            "  CBO 9111: score=0.117, admissões=38,881  (Manutenção e reparação)\n",
            "  CBO 9112: score=0.117, admissões=83,226  (Manutenção e reparação)\n",
            "  CBO 9113: score=0.117, admissões=489,020  (Manutenção e reparação)\n",
            "  CBO 9131: score=0.117, admissões=85,145  (Manutenção e reparação)\n",
            "  CBO 9141: score=0.117, admissões=9,542  (Manutenção e reparação)\n",
            "  CBO 9142: score=0.117, admissões=1,889  (Manutenção e reparação)\n",
            "  CBO 9143: score=0.117, admissões=7,158  (Manutenção e reparação)\n",
            "\n",
            "--- Estatísticas por quintil de exposição ---\n",
            "  Q1 (Baixa): n=6,598, exposure=0.144, adm_mean=3200, sal_medio=4,469\n",
            "  Q2: n=6,597, exposure=0.180, adm_mean=2207, sal_medio=2,599\n",
            "  Q3: n=6,598, exposure=0.252, adm_mean=3676, sal_medio=7,018\n",
            "  Q4: n=6,597, exposure=0.348, adm_mean=2735, sal_medio=8,072\n",
            "  Q5 (Alta): n=6,598, exposure=0.466, adm_mean=3950, sal_medio=8,497\n",
            "\n",
            "--- Concordância 2d vs 4d: 93.1% ---\n",
            "\n",
            "============================================================\n",
            "CHECKPOINT CONCLUÍDO\n",
            "============================================================\n"
          ]
        }
      ],
      "source": [
        "# Etapa 2a.5b — CHECKPOINT: Verificar definição de tratamento\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"CHECKPOINT — Definição de Tratamento\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# 1. Top 10 ocupações mais expostas\n",
        "print(\"\\n--- Top 10 ocupações MAIS expostas ---\")\n",
        "top10 = painel.groupby('cbo_4d').agg(\n",
        "    exposure=('exposure_score', 'first'),\n",
        "    admissoes_total=('admissoes', 'sum'),\n",
        ").nlargest(10, 'exposure')\n",
        "for cbo, row in top10.iterrows():\n",
        "    nome = GRANDES_GRUPOS_CBO.get(cbo[0], '')\n",
        "    print(f\"  CBO {cbo}: score={row['exposure']:.3f}, admissões={row['admissoes_total']:,.0f}  ({nome})\")\n",
        "\n",
        "# 2. Bottom 10 ocupações menos expostas\n",
        "print(f\"\\n--- 10 ocupações MENOS expostas ---\")\n",
        "bot10 = painel.groupby('cbo_4d').agg(\n",
        "    exposure=('exposure_score', 'first'),\n",
        "    admissoes_total=('admissoes', 'sum'),\n",
        ").nsmallest(10, 'exposure')\n",
        "for cbo, row in bot10.iterrows():\n",
        "    nome = GRANDES_GRUPOS_CBO.get(cbo[0], '')\n",
        "    print(f\"  CBO {cbo}: score={row['exposure']:.3f}, admissões={row['admissoes_total']:,.0f}  ({nome})\")\n",
        "\n",
        "# 3. Distribuição por quintil\n",
        "print(f\"\\n--- Estatísticas por quintil de exposição ---\")\n",
        "for q in ['Q1 (Baixa)', 'Q2', 'Q3', 'Q4', 'Q5 (Alta)']:\n",
        "    sub = painel[painel['quintil_exp'] == q]\n",
        "    if len(sub) > 0:\n",
        "        print(f\"  {q}: n={len(sub):,}, \"\n",
        "              f\"exposure={sub['exposure_score'].mean():.3f}, \"\n",
        "              f\"adm_mean={sub['admissoes'].mean():.0f}, \"\n",
        "              f\"sal_medio={sub['salario_medio_adm'].mean():,.0f}\")\n",
        "\n",
        "# 4. Concordância\n",
        "concordancia = (painel['alta_exp'] == painel['alta_exp_4d']).mean()\n",
        "print(f\"\\n--- Concordância 2d vs 4d: {concordancia:.1%} ---\")\n",
        "\n",
        "print(f\"\\n{'=' * 60}\")\n",
        "print(f\"CHECKPOINT CONCLUÍDO\")\n",
        "print(f\"{'=' * 60}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1b075f48",
      "metadata": {},
      "source": [
        "### 6a. Enriquecimento do painel (variáveis adicionais)\n",
        "\n",
        "Adicionar variáveis de controle e contexto temporal ao painel para a análise DiD.\n",
        "\n",
        "| Variável | Cálculo | Uso |\n",
        "|----------|---------|-----|\n",
        "| `tempo_relativo_meses` | Meses desde Dez/2022 (t=0) | Event study |\n",
        "| `trend` | Tendência linear (0, 1, 2, ...) | Controle de tendência |\n",
        "| `mes_do_ano` | Mês do ano (1-12) | Dummies de sazonalidade |\n",
        "| `salario_sm` | `salario_medio_adm / SM do ano` | Normalização salarial |\n",
        "| `grande_grupo_nome` | Nome do grande grupo CBO | Efeitos fixos |"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "id": "2519c01b",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tempo relativo: [-23, 30] meses\n",
            "Referência (t=0): 12/2022\n",
            "\n",
            "Variáveis adicionadas: tempo_relativo_meses, trend, mes_do_ano, salario_sm, ln_salario_sm, salario_real_adm, ln_salario_real_adm, grande_grupo_nome\n",
            "Colunas totais: 74\n"
          ]
        }
      ],
      "source": [
        "# Etapa 2a.6a — Enriquecimento do painel\n",
        "\n",
        "def periodo_num_to_months(pn):\n",
        "    \"\"\"Converter periodo_num (YYYYMM) para contagem absoluta de meses.\"\"\"\n",
        "    return (pn // 100) * 12 + (pn % 100)\n",
        "\n",
        "# ── Tempo relativo ao tratamento ──\n",
        "ref_periodo = ANO_TRATAMENTO * 100 + MES_TRATAMENTO\n",
        "painel['meses_abs'] = painel['periodo_num'].apply(periodo_num_to_months)\n",
        "ref_meses = periodo_num_to_months(ref_periodo)\n",
        "painel['tempo_relativo_meses'] = painel['meses_abs'] - ref_meses\n",
        "\n",
        "print(f\"Tempo relativo: [{painel['tempo_relativo_meses'].min()}, \"\n",
        "      f\"{painel['tempo_relativo_meses'].max()}] meses\")\n",
        "print(f\"Referência (t=0): {MES_TRATAMENTO}/{ANO_TRATAMENTO}\")\n",
        "\n",
        "# ── Tendência temporal e sazonalidade ──\n",
        "painel['trend'] = painel['meses_abs'] - painel['meses_abs'].min()\n",
        "painel['mes_do_ano'] = painel['mes'].astype(int)\n",
        "\n",
        "# ── Normalização salarial ──\n",
        "painel['sm_ano'] = painel['ano'].astype(int).map(SALARIO_MINIMO)\n",
        "painel['salario_sm'] = painel['salario_medio_adm'] / painel['sm_ano']\n",
        "painel['ln_salario_sm'] = np.log(painel['salario_sm'].clip(lower=0.1))\n",
        "\n",
        "# ── Salário real (deflacionado pelo IPCA, base Dez/2024 = 100) ──\n",
        "painel = painel.merge(df_ipca[['ano', 'mes', 'indice']], on=['ano', 'mes'], how='left')\n",
        "painel['salario_real_adm'] = painel['salario_medio_adm'] * (INDICE_BASE / painel['indice'])\n",
        "painel['ln_salario_real_adm'] = np.log(painel['salario_real_adm'].clip(lower=1))\n",
        "\n",
        "# ── Grande grupo ocupacional ──\n",
        "painel['grande_grupo_cbo'] = painel['cbo_4d'].str[0]\n",
        "painel['grande_grupo_nome'] = painel['grande_grupo_cbo'].map(GRANDES_GRUPOS_CBO)\n",
        "\n",
        "print(f\"\\nVariáveis adicionadas: tempo_relativo_meses, trend, mes_do_ano, \"\n",
        "      f\"salario_sm, ln_salario_sm, salario_real_adm, ln_salario_real_adm, grande_grupo_nome\")\n",
        "print(f\"Colunas totais: {painel.shape[1]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5a04d913",
      "metadata": {},
      "source": [
        "### Anexo 1: Integração Anthropic (Automation vs Augmentation)\n",
        "\n",
        "Integração do **Anthropic Economic Index** (Brynjolfsson et al., 2025 — \"Canaries in the Coal Mine\") para diferenciar ocupações onde a IA atua como **Automação** (substitui trabalho) vs **Augmentação** (complementa). O índice é construído a partir dos modos de colaboração (directive, feedback loop = automação; learning, task iteration, validation = augmentação) e mapeado para CBO 4d via SOC → ISCO-08 → COD, com imputação hierárquica para cobertura total.\n",
        "\n",
        "**Arquivos necessários em `data/input` (copiar das pastas do repositório se não existirem):**\n",
        "\n",
        "| Arquivo | Origem no repositório |\n",
        "|---------|------------------------|\n",
        "| `aei_raw_claude_ai_2025-11-13_to_2025-11-20.csv` | `EconomicIndex/release_2026_01_15/data/intermediate/` ou `etapa2_anthropic_index/` |\n",
        "| `onet_task_statements.csv` | `EconomicIndex/release_2025_09_15/data/intermediate/` |\n",
        "| `Crosswalk SOC 2010 a 2018.xlsx` | `etapa3_crosswalk_onet_isco08/data_input/` |\n",
        "| `Crosswalk SOC 2010 ISCO-08.xls` | `etapa3_crosswalk_onet_isco08/data_input/` |\n",
        "| `Estrutura Ocupação COD.xls` | `etapa1_ia_generativa/data/raw/` ou `etapa3_crosswalk_onet_isco08/data_input/` |\n",
        "\n",
        "Se `anthropic_automation_augmentation_cbo.parquet` existir em `data/processed` e `KEEP_ANTHROPIC_INDEX = True`, o processamento pesado é pulado e o cache é carregado."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "id": "470b1b6a",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Copiado: aei_raw_claude_ai_2025-11-13_to_2025-11-20.csv\n",
            "Copiado: onet_task_statements.csv\n",
            "Copiado: Crosswalk SOC 2010 a 2018.xlsx\n",
            "Copiado: Crosswalk SOC 2010 ISCO-08.xls\n",
            "Inputs em data/input conferidos.\n"
          ]
        }
      ],
      "source": [
        "# Anexo 1 — Copiar inputs para data/input (se ainda não estiverem) para notebook autocontido\n",
        "import shutil\n",
        "# Raiz do projeto: se estamos em notebook/, sobe um nível\n",
        "_cwd = Path(\".\").resolve()\n",
        "REPO_ROOT = _cwd.parent if (_cwd / \"etapa3_crosswalk_onet_isco08\").exists() == False else _cwd\n",
        "origins = [\n",
        "    (REPO_ROOT / \"EconomicIndex\" / \"release_2026_01_15\" / \"data\" / \"intermediate\" / \"aei_raw_claude_ai_2025-11-13_to_2025-11-20.csv\", DATA_INPUT / \"aei_raw_claude_ai_2025-11-13_to_2025-11-20.csv\"),\n",
        "    (REPO_ROOT / \"EconomicIndex\" / \"release_2025_09_15\" / \"data\" / \"intermediate\" / \"onet_task_statements.csv\", DATA_INPUT / \"onet_task_statements.csv\"),\n",
        "    (REPO_ROOT / \"etapa3_crosswalk_onet_isco08\" / \"data_input\" / \"Crosswalk SOC 2010 a 2018.xlsx\", DATA_INPUT / \"Crosswalk SOC 2010 a 2018.xlsx\"),\n",
        "    (REPO_ROOT / \"etapa3_crosswalk_onet_isco08\" / \"data_input\" / \"Crosswalk SOC 2010 ISCO-08.xls\", DATA_INPUT / \"Crosswalk SOC 2010 ISCO-08.xls\"),\n",
        "    (REPO_ROOT / \"etapa1_ia_generativa\" / \"data\" / \"raw\" / \"Estrutura Ocupação COD.xls\", DATA_INPUT / \"Estrutura Ocupação COD.xls\"),\n",
        "]\n",
        "for src, dst in origins:\n",
        "    if src.exists() and (not dst.exists() or dst.stat().st_size == 0):\n",
        "        shutil.copy2(src, dst)\n",
        "        print(f\"Copiado: {dst.name}\")\n",
        "    elif not src.exists() and dst.exists():\n",
        "        print(f\"Já em data/input: {dst.name}\")\n",
        "    elif not src.exists():\n",
        "        print(f\"AVISO: não encontrado no repo: {src.relative_to(REPO_ROOT) if src.is_relative_to(REPO_ROOT) else src}\")\n",
        "print(\"Inputs em data/input conferidos.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "id": "ca697ae7",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Anexo 1 — Carregar cache ou rodar pipeline Anthropic (Automation vs Augmentation)\n",
        "def _weighted_mean(values, weights):\n",
        "    mask = ~(pd.isna(values) | pd.isna(weights))\n",
        "    if mask.sum() == 0: return np.nan\n",
        "    v, w = values[mask], weights[mask]\n",
        "    return np.average(v, weights=w) if w.sum() > 0 else v.mean()\n",
        "\n",
        "def _calculate_indices(df_task):\n",
        "    required_modes = [\"directive\", \"feedback loop\", \"learning\", \"task iteration\", \"validation\"]\n",
        "    for m in required_modes:\n",
        "        if m not in df_task.columns: df_task[m] = 0\n",
        "    df_task[\"total_collab\"] = df_task[required_modes].sum(axis=1)\n",
        "    mask = df_task[\"total_collab\"] > 0\n",
        "    df_task.loc[mask, \"automation_share\"] = (df_task.loc[mask, \"directive\"] + df_task.loc[mask, \"feedback loop\"]) / df_task.loc[mask, \"total_collab\"]\n",
        "    df_task.loc[mask, \"augmentation_share\"] = (df_task.loc[mask, \"learning\"] + df_task.loc[mask, \"task iteration\"] + df_task.loc[mask, \"validation\"]) / df_task.loc[mask, \"total_collab\"]\n",
        "    df_task[\"automation_share\"] = df_task[\"automation_share\"].fillna(0)\n",
        "    df_task[\"augmentation_share\"] = df_task[\"augmentation_share\"].fillna(0)\n",
        "    df_task[\"automation_index\"] = df_task[\"automation_share\"] - df_task[\"augmentation_share\"]\n",
        "    df_task[\"dominant_mode\"] = np.where(df_task[\"automation_index\"] > 0, \"automation\", \"augmentation\")\n",
        "    return df_task\n",
        "\n",
        "df_anthropic_cbo = None\n",
        "if KEEP_ANTHROPIC_INDEX and ANTHROPIC_INDEX_CACHE.exists():\n",
        "    df_anthropic_cbo = pd.read_parquet(ANTHROPIC_INDEX_CACHE)\n",
        "    print(f\"Índice Anthropic carregado do cache: {ANTHROPIC_INDEX_CACHE.name} ({len(df_anthropic_cbo)} ocupações CBO 4d)\")\n",
        "else:\n",
        "    # Pipeline completo\n",
        "    AEI_RAW = DATA_INPUT / \"aei_raw_claude_ai_2025-11-13_to_2025-11-20.csv\"\n",
        "    ONET_TASKS = DATA_INPUT / \"onet_task_statements.csv\"\n",
        "    CROSSWALK_10_18 = DATA_INPUT / \"Crosswalk SOC 2010 a 2018.xlsx\"\n",
        "    CROSSWALK_ISCO = DATA_INPUT / \"Crosswalk SOC 2010 ISCO-08.xls\"\n",
        "    ESTRUTURA_COD = DATA_INPUT / \"Estrutura Ocupação COD.xls\"\n",
        "    if not AEI_RAW.exists() or not ONET_TASKS.exists():\n",
        "        raise FileNotFoundError(\"Coloque aei_raw_claude_ai_*.csv e onet_task_statements.csv em data/input. Veja Anexo 1.\")\n",
        "    # 1) Carregar Anthropic + O*NET\n",
        "    df_raw = pd.read_csv(AEI_RAW)\n",
        "    mask = (df_raw[\"facet\"] == \"onet_task::collaboration\") & (df_raw[\"geo_id\"] == \"GLOBAL\")\n",
        "    df_collab = df_raw[mask].copy()\n",
        "    df_collab[[\"task_desc\", \"mode\"]] = df_collab[\"cluster_name\"].str.rsplit(\"::\", n=1, expand=True)\n",
        "    df_collab[\"task_desc_clean\"] = df_collab[\"task_desc\"].str.strip().str.lower().str.rstrip(\".\")\n",
        "    df_pivot = df_collab.pivot_table(index=\"task_desc_clean\", columns=\"mode\", values=\"value\", aggfunc=\"first\").reset_index()\n",
        "    df_onet = pd.read_csv(ONET_TASKS)\n",
        "    df_onet[\"task_desc_clean\"] = df_onet[\"Task\"].str.strip().str.lower().str.rstrip(\".\")\n",
        "    task_mapping = df_onet[[\"task_desc_clean\", \"O*NET-SOC Code\"]].drop_duplicates()\n",
        "    task_mapping.columns = [\"task_desc_clean\", \"onet_soc_code\"]\n",
        "    df_pivot = df_pivot.merge(task_mapping, on=\"task_desc_clean\", how=\"inner\")\n",
        "    mask_count = (df_raw[\"facet\"] == \"onet_task\") & (df_raw[\"variable\"] == \"onet_task_count\") & (df_raw[\"geo_id\"] == \"GLOBAL\")\n",
        "    df_counts = df_raw[mask_count][[\"cluster_name\", \"value\"]].rename(columns={\"cluster_name\": \"task_desc\", \"value\": \"usage_volume\"})\n",
        "    df_counts[\"task_desc_clean\"] = df_counts[\"task_desc\"].str.strip().str.lower().str.rstrip(\".\")\n",
        "    df_counts = df_counts.groupby(\"task_desc_clean\")[\"usage_volume\"].sum().reset_index()\n",
        "    df_task = df_pivot.merge(df_counts, on=\"task_desc_clean\", how=\"inner\")\n",
        "    df_task = _calculate_indices(df_task)\n",
        "    df_onet[\"soc_6d\"] = df_onet[\"O*NET-SOC Code\"].str.split(\".\").str[0]\n",
        "    soc_mapping = df_onet[[\"O*NET-SOC Code\", \"soc_6d\", \"Title\"]].drop_duplicates()\n",
        "    soc_mapping.columns = [\"onet_soc_code\", \"soc_6d\", \"occupation_title\"]\n",
        "    df_merged = df_task.merge(soc_mapping, on=\"onet_soc_code\", how=\"inner\")\n",
        "    def agg_wm(x): return _weighted_mean(x, df_merged.loc[x.index, \"usage_volume\"])\n",
        "    df_soc = df_merged.groupby([\"soc_6d\", \"occupation_title\"]).agg(\n",
        "        automation_share=(\"automation_share\", agg_wm), augmentation_share=(\"augmentation_share\", agg_wm), usage_volume=(\"usage_volume\", \"sum\")\n",
        "    ).reset_index()\n",
        "    df_soc[\"automation_index_cai\"] = df_soc[\"automation_share\"] - df_soc[\"augmentation_share\"]\n",
        "    df_soc[\"automation_share_cai\"] = df_soc[\"automation_share\"]\n",
        "    df_soc[\"augmentation_share_cai\"] = df_soc[\"augmentation_share\"]\n",
        "    df_soc.to_csv(DATA_PROCESSED / \"onet_automation_augmentation_index.csv\", index=False)\n",
        "    # 2) Crosswalk SOC -> ISCO-08 (fallback SOC 2018 -> 2010 -> ISCO)\n",
        "    df_10_18 = pd.read_excel(CROSSWALK_10_18, skiprows=7).iloc[:, :4]\n",
        "    df_10_18.columns = [\"soc_2010_code\", \"soc_2010_title\", \"soc_2018_code\", \"soc_2018_title\"]\n",
        "    df_10_18[\"soc_2018_code\"] = df_10_18[\"soc_2018_code\"].astype(str).str.replace(\".00\", \"\", regex=False).str.strip()\n",
        "    df_10_18 = df_10_18.dropna(subset=[\"soc_2010_code\", \"soc_2018_code\"])\n",
        "    df_soc_isco = pd.read_excel(CROSSWALK_ISCO, sheet_name=\"2010 SOC to ISCO-08\", skiprows=7)\n",
        "    df_soc_isco.columns = [\"soc_2010_code\", \"soc_2010_title\", \"part\", \"isco_08_code\", \"isco_08_title\", \"comment\"]\n",
        "    df_soc_isco = df_soc_isco.dropna(subset=[\"soc_2010_code\", \"isco_08_code\"])\n",
        "    df_soc_isco[\"isco_08_code\"] = df_soc_isco[\"isco_08_code\"].astype(str).str.replace(\".0\", \"\", regex=False).str.zfill(4)\n",
        "    df_merged = df_soc.merge(df_10_18, left_on=\"soc_6d\", right_on=\"soc_2018_code\", how=\"inner\").merge(df_soc_isco, on=\"soc_2010_code\", how=\"inner\")\n",
        "    def agg_isco(x): return _weighted_mean(x, df_merged.loc[x.index, \"usage_volume\"])\n",
        "    df_isco = df_merged.groupby(\"isco_08_code\").agg(\n",
        "        automation_share_cai=(\"automation_share_cai\", agg_isco), augmentation_share_cai=(\"augmentation_share_cai\", agg_isco),\n",
        "        automation_index_cai=(\"automation_index_cai\", agg_isco), usage_volume=(\"usage_volume\", \"sum\")\n",
        "    ).reset_index()\n",
        "    df_isco[\"dominant_mode_cai\"] = np.where(df_isco[\"automation_index_cai\"] > 0, \"automation\", \"augmentation\")\n",
        "    df_isco.to_csv(DATA_PROCESSED / \"isco_automation_augmentation_index.csv\", index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "id": "61b60bc4",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Pipeline Anthropic concluído. Cache salvo: data/processed/anthropic_automation_augmentation_cbo.parquet. Ocupações: 435.\n"
          ]
        }
      ],
      "source": [
        "# Anexo 1 — Continuação: ISCO -> COD, imputação hierárquica, salvar cache (só roda se pipeline foi executado)\n",
        "if df_anthropic_cbo is None:\n",
        "    try:\n",
        "        df_esco = pd.read_csv(DATA_PROCESSED / \"isco_automation_augmentation_index.csv\")\n",
        "    except Exception:\n",
        "        raise RuntimeError(\"Execute a célula anterior do Anexo 1 primeiro.\")\n",
        "    ESTRUTURA_COD = DATA_INPUT / \"Estrutura Ocupação COD.xls\"\n",
        "    if not ESTRUTURA_COD.exists():\n",
        "        raise FileNotFoundError(\"Coloque Estrutura Ocupação COD.xls em data/input.\")\n",
        "    try:\n",
        "        df_cod = pd.read_excel(ESTRUTURA_COD, sheet_name=\"Estrutura COD\", engine=\"xlrd\", header=1)\n",
        "    except Exception:\n",
        "        df_cod = pd.read_excel(ESTRUTURA_COD, sheet_name=\"Estrutura COD\", engine=\"openpyxl\", header=1)\n",
        "    df_cod.columns = [\"grande_grupo\", \"subgrupo_principal\", \"subgrupo\", \"grupo_base\", \"denominacao\"]\n",
        "    df_gb = df_cod.dropna(subset=[\"grupo_base\"])[[\"grupo_base\", \"denominacao\"]].copy()\n",
        "    df_gb[\"cod_cod\"] = df_gb[\"grupo_base\"].astype(str).str.replace(\".0\", \"\", regex=False).str.zfill(4)\n",
        "    df_esco[\"isco_08_code\"] = df_esco[\"isco_08_code\"].astype(str).str.zfill(4)\n",
        "    df_cod_merge = df_gb[[\"cod_cod\", \"denominacao\"]].merge(df_esco, left_on=\"cod_cod\", right_on=\"isco_08_code\", how=\"left\")\n",
        "    metrics_cols = [\"automation_share_cai\", \"augmentation_share_cai\", \"automation_index_cai\"]\n",
        "    df_cod_merge[\"imputation_method\"] = np.where(df_cod_merge[\"automation_index_cai\"].notna(), \"direct_match\", \"missing\")\n",
        "    df_isco = df_esco.copy()\n",
        "    df_isco[\"isco_3d\"] = df_isco[\"isco_08_code\"].astype(str).str[:3]\n",
        "    df_isco[\"isco_2d\"] = df_isco[\"isco_08_code\"].astype(str).str[:2]\n",
        "    if \"usage_volume\" not in df_isco.columns: df_isco[\"usage_volume\"] = 1.0\n",
        "    avg_3d = df_isco.groupby(\"isco_3d\").apply(lambda g: pd.Series({c: _weighted_mean(g[c], g[\"usage_volume\"]) for c in metrics_cols if c in g.columns})).reset_index()\n",
        "    avg_2d = df_isco.groupby(\"isco_2d\").apply(lambda g: pd.Series({c: _weighted_mean(g[c], g[\"usage_volume\"]) for c in metrics_cols if c in g.columns})).reset_index()\n",
        "    df_cod_merge[\"cod_3d\"] = df_cod_merge[\"cod_cod\"].astype(str).str[:3]\n",
        "    df_cod_merge[\"cod_2d\"] = df_cod_merge[\"cod_cod\"].astype(str).str[:2]\n",
        "    mask_miss = df_cod_merge[\"automation_index_cai\"].isna()\n",
        "    for idx in df_cod_merge[mask_miss].index:\n",
        "        c3 = df_cod_merge.loc[idx, \"cod_3d\"]\n",
        "        row3 = avg_3d[avg_3d[\"isco_3d\"] == c3]\n",
        "        if len(row3) > 0:\n",
        "            for c in metrics_cols:\n",
        "                if c in row3.columns: df_cod_merge.loc[idx, c] = row3[c].iloc[0]\n",
        "            df_cod_merge.loc[idx, \"imputation_method\"] = \"hierarchical_3d_mean\"\n",
        "    mask_miss = df_cod_merge[\"automation_index_cai\"].isna()\n",
        "    for idx in df_cod_merge[mask_miss].index:\n",
        "        c2 = df_cod_merge.loc[idx, \"cod_2d\"]\n",
        "        row2 = avg_2d[avg_2d[\"isco_2d\"] == c2]\n",
        "        if len(row2) > 0:\n",
        "            for c in metrics_cols:\n",
        "                if c in row2.columns: df_cod_merge.loc[idx, c] = row2[c].iloc[0]\n",
        "            df_cod_merge.loc[idx, \"imputation_method\"] = \"hierarchical_2d_mean\"\n",
        "    mask_still = df_cod_merge[\"automation_index_cai\"].isna()\n",
        "    df_cod_merge.loc[mask_still, metrics_cols] = 0.0\n",
        "    df_cod_merge.loc[mask_still, \"imputation_method\"] = \"zero_imputation_no_data\"\n",
        "    df_cod_merge[\"dominant_mode_cai\"] = np.where(df_cod_merge[\"automation_index_cai\"] > 0, \"automation\", \"augmentation\")\n",
        "    df_cod_merge.to_csv(DATA_PROCESSED / \"cod_automation_augmentation_index_final.csv\", index=False)\n",
        "    df_anthropic_cbo = df_cod_merge[[\"cod_cod\", \"automation_share_cai\", \"augmentation_share_cai\", \"automation_index_cai\", \"dominant_mode_cai\", \"imputation_method\"]].copy()\n",
        "    df_anthropic_cbo = df_anthropic_cbo.rename(columns={\"cod_cod\": \"cbo_4d\", \"automation_index_cai\": \"anthropic_automation_index\"})\n",
        "    df_anthropic_cbo[\"cbo_4d\"] = df_anthropic_cbo[\"cbo_4d\"].astype(str).str.zfill(4)\n",
        "    df_anthropic_cbo.to_parquet(ANTHROPIC_INDEX_CACHE, index=False)\n",
        "    print(f\"Pipeline Anthropic concluído. Cache salvo: {ANTHROPIC_INDEX_CACHE}. Ocupações: {len(df_anthropic_cbo)}.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "id": "023ac036",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Anexo 1 — Merge concluído. anthropic_automation_index: cobertura 100.0%. is_automation=5.4%, is_augmentation=94.6%\n"
          ]
        }
      ],
      "source": [
        "# Anexo 1 — Merge do índice Anthropic ao painel e criação das dummies (automation vs augmentation)\n",
        "# Garantir formato cbo_4d string 4 dígitos para o merge\n",
        "_cbo = painel[\"cbo_4d\"].astype(str).str.zfill(4)\n",
        "df_anthropic_cbo[\"cbo_4d\"] = df_anthropic_cbo[\"cbo_4d\"].astype(str).str.zfill(4)\n",
        "cols_merge = [c for c in [\"anthropic_automation_index\", \"automation_share_cai\", \"augmentation_share_cai\", \"dominant_mode_cai\"] if c in df_anthropic_cbo.columns]\n",
        "idx_cbo = df_anthropic_cbo.set_index(\"cbo_4d\")[cols_merge]\n",
        "painel = painel.copy()\n",
        "for c in cols_merge:\n",
        "    painel[c] = _cbo.map(idx_cbo[c] if c in idx_cbo.columns else idx_cbo.squeeze()).values\n",
        "# Score contínuo: preencher NaN com 0 (imputação hierárquica já cobriu todas as CBOs no índice)\n",
        "painel[\"anthropic_automation_index\"] = painel[\"anthropic_automation_index\"].fillna(0.0)\n",
        "# Dummies: Automação = 1 quando automation > augmentation (índice > 0); Augmentação = 1 quando augmentation > automation\n",
        "painel[\"is_automation\"] = (painel[\"anthropic_automation_index\"] > 0).astype(int)\n",
        "painel[\"is_augmentation\"] = (painel[\"anthropic_automation_index\"] <= 0).astype(int)\n",
        "print(f\"Anexo 1 — Merge concluído. anthropic_automation_index: cobertura {painel['anthropic_automation_index'].notna().mean():.1%}. \"\n",
        "      f\"is_automation={painel['is_automation'].mean():.1%}, is_augmentation={painel['is_augmentation'].mean():.1%}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0626572e",
      "metadata": {},
      "source": [
        "### 7. Salvar dataset analítico final\n",
        "\n",
        "Selecionar colunas finais, remover ocupações sem score de exposição e salvar o dataset pronto para a análise DiD (Notebook 2b).\n",
        "\n",
        "**Saída:**\n",
        "- `data/output/painel_caged_did_ready.parquet` — formato eficiente para análise\n",
        "- `data/output/painel_caged_did_ready.csv` — backup legível"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "id": "f56e1220",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cache removido: painel_caged_did_ready.parquet\n",
            "Cache removido: painel_caged_did_ready.csv\n",
            "  Verificação: colunas obrigatórias para o 2b presentes e sem NA.\n",
            "\n",
            "============================================================\n",
            "DATASET ANALÍTICO FINAL — ETAPA 2a\n",
            "============================================================\n",
            "  Observações:        32,988\n",
            "  Ocupações (CBO 4d): 629\n",
            "  Períodos:           54 meses\n",
            "    Pré-tratamento:   23\n",
            "    Pós-tratamento:   31\n",
            "  Cobertura 2d:       100.0%\n",
            "  Cobertura 4d:       100.0%\n",
            "  Tratamento 2d:      20.3% das obs\n",
            "  Tratamento 4d:      21.3% das obs\n",
            "  Colunas:            59\n",
            "\n",
            "  Salvo em:\n",
            "    data/output/painel_caged_did_ready.parquet\n",
            "    data/output/painel_caged_did_ready.csv\n",
            "    Tamanho: 7.2 MB (parquet), 21.0 MB (csv)\n",
            "\n",
            "  Info:\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 32988 entries, 0 to 32987\n",
            "Data columns (total 59 columns):\n",
            " #   Column                      Non-Null Count  Dtype   \n",
            "---  ------                      --------------  -----   \n",
            " 0   cbo_4d                      32988 non-null  object  \n",
            " 1   cbo_2d                      32988 non-null  object  \n",
            " 2   ano                         32988 non-null  Int64   \n",
            " 3   mes                         32988 non-null  Int64   \n",
            " 4   periodo                     32988 non-null  object  \n",
            " 5   periodo_num                 32988 non-null  int64   \n",
            " 6   admissoes                   32988 non-null  Int64   \n",
            " 7   desligamentos               32988 non-null  Int64   \n",
            " 8   saldo                       32988 non-null  Int64   \n",
            " 9   n_movimentacoes             32988 non-null  Int64   \n",
            " 10  ln_admissoes                32988 non-null  Float64 \n",
            " 11  ln_desligamentos            32988 non-null  Float64 \n",
            " 12  salario_medio_adm           32988 non-null  float64 \n",
            " 13  salario_mediano_adm         32988 non-null  float64 \n",
            " 14  salario_medio_desl          32988 non-null  float64 \n",
            " 15  ln_salario_adm              32988 non-null  float64 \n",
            " 16  salario_sm                  32988 non-null  float64 \n",
            " 17  ln_salario_sm               32988 non-null  float64 \n",
            " 18  salario_real_adm            32988 non-null  float64 \n",
            " 19  ln_salario_real_adm         32988 non-null  float64 \n",
            " 20  idade_media_adm             32988 non-null  Float64 \n",
            " 21  pct_mulher_adm              32988 non-null  float64 \n",
            " 22  pct_superior_adm            32988 non-null  float64 \n",
            " 23  pct_branco_adm              32988 non-null  float64 \n",
            " 24  pct_negro_adm               32988 non-null  float64 \n",
            " 25  pct_jovem_adm               32988 non-null  float64 \n",
            " 26  pct_tecnologico_adm         32988 non-null  float64 \n",
            " 27  setor_tecnologico           32988 non-null  int64   \n",
            " 28  ln_salario_homem            32988 non-null  float64 \n",
            " 29  ln_salario_mulher           32988 non-null  float64 \n",
            " 30  ln_salario_jovem            32988 non-null  float64 \n",
            " 31  ln_salario_naojovem         32988 non-null  float64 \n",
            " 32  ln_salario_branco           32988 non-null  float64 \n",
            " 33  ln_salario_negro            32988 non-null  float64 \n",
            " 34  ln_salario_superior         32988 non-null  float64 \n",
            " 35  ln_salario_medio            32988 non-null  float64 \n",
            " 36  ln_admissoes_homem          32988 non-null  float64 \n",
            " 37  ln_admissoes_mulher         32988 non-null  float64 \n",
            " 38  ln_admissoes_jovem          32988 non-null  float64 \n",
            " 39  ln_admissoes_negro          32988 non-null  float64 \n",
            " 40  exposure_score_2d           32988 non-null  float64 \n",
            " 41  exposure_score_4d           32988 non-null  float64 \n",
            " 42  alta_exp                    32988 non-null  int64   \n",
            " 43  alta_exp_10                 32988 non-null  int64   \n",
            " 44  alta_exp_25                 32988 non-null  int64   \n",
            " 45  alta_exp_mediana            32988 non-null  int64   \n",
            " 46  quintil_exp                 32988 non-null  category\n",
            " 47  alta_exp_4d                 32988 non-null  int64   \n",
            " 48  post                        32988 non-null  int64   \n",
            " 49  did                         32988 non-null  int64   \n",
            " 50  did_4d                      32988 non-null  int64   \n",
            " 51  tempo_relativo_meses        32988 non-null  int64   \n",
            " 52  trend                       32988 non-null  int64   \n",
            " 53  mes_do_ano                  32988 non-null  int64   \n",
            " 54  grande_grupo_cbo            32988 non-null  object  \n",
            " 55  grande_grupo_nome           32988 non-null  object  \n",
            " 56  anthropic_automation_index  32988 non-null  float64 \n",
            " 57  is_automation               32988 non-null  int64   \n",
            " 58  is_augmentation             32988 non-null  int64   \n",
            "dtypes: Float64(3), Int64(6), category(1), float64(29), int64(15), object(5)\n",
            "memory usage: 14.9+ MB\n"
          ]
        }
      ],
      "source": [
        "# Etapa 2a.7 — Salvar dataset analítico final\n",
        "\n",
        "if not KEEP_PANEL_FINAL:\n",
        "    if PAINEL_FINAL_PARQUET.exists():\n",
        "        PAINEL_FINAL_PARQUET.unlink()\n",
        "        print(f\"Cache removido: {PAINEL_FINAL_PARQUET.name}\")\n",
        "    if PAINEL_FINAL_CSV.exists():\n",
        "        PAINEL_FINAL_CSV.unlink()\n",
        "        print(f\"Cache removido: {PAINEL_FINAL_CSV.name}\")\n",
        "\n",
        "# ── Selecionar colunas finais ──\n",
        "cols_finais = [\n",
        "    # Identificação\n",
        "    'cbo_4d', 'cbo_2d', 'ano', 'mes', 'periodo', 'periodo_num',\n",
        "    # Outcomes\n",
        "    'admissoes', 'desligamentos', 'saldo', 'n_movimentacoes',\n",
        "    'ln_admissoes', 'ln_desligamentos',\n",
        "    'salario_medio_adm', 'salario_mediano_adm', 'salario_medio_desl',\n",
        "    'ln_salario_adm', 'salario_sm', 'ln_salario_sm',\n",
        "    'salario_real_adm', 'ln_salario_real_adm',\n",
        "    # Demografia das admissões (proporções)\n",
        "    'idade_media_adm', 'pct_mulher_adm', 'pct_superior_adm', 'pct_branco_adm', 'pct_negro_adm', 'pct_jovem_adm',\n",
        "    'pct_tecnologico_adm', 'setor_tecnologico',\n",
        "    # Heterogeneidade: salários (log)\n",
        "    'ln_salario_homem', 'ln_salario_mulher', 'ln_salario_jovem', 'ln_salario_naojovem',\n",
        "    'ln_salario_branco', 'ln_salario_negro', 'ln_salario_superior', 'ln_salario_medio',\n",
        "    # Heterogeneidade: volumes (log)\n",
        "    'ln_admissoes_homem', 'ln_admissoes_mulher', 'ln_admissoes_jovem', 'ln_admissoes_negro',\n",
        "    # Exposição IA — DUAL\n",
        "    'exposure_score_2d',   # PRINCIPAL\n",
        "    'exposure_score_4d',   # ROBUSTEZ\n",
        "    # Tratamento — DUAL\n",
        "    'alta_exp',            # Top 20% score 2d (PRINCIPAL)\n",
        "    'alta_exp_10', 'alta_exp_25', 'alta_exp_mediana', 'quintil_exp',\n",
        "    'alta_exp_4d',         # Top 20% score 4d (ROBUSTEZ)\n",
        "    # Temporal\n",
        "    'post', 'did', 'did_4d', 'tempo_relativo_meses', 'trend', 'mes_do_ano',\n",
        "    # Classificação\n",
        "    'grande_grupo_cbo', 'grande_grupo_nome',\n",
        "    # Anthropic (Automation vs Augmentation) — Anexo 1\n",
        "    'anthropic_automation_index', 'is_automation', 'is_augmentation',\n",
        "]\n",
        "\n",
        "cols_existentes = [c for c in cols_finais if c in painel.columns]\n",
        "cols_faltantes = [c for c in cols_finais if c not in painel.columns]\n",
        "if cols_faltantes:\n",
        "    print(f\"AVISO: Colunas não encontradas: {cols_faltantes}\")\n",
        "\n",
        "painel_final = painel[cols_existentes].copy()\n",
        "\n",
        "# ── Remover ocupações sem score principal (2d) ──\n",
        "n_antes = len(painel_final)\n",
        "painel_final = painel_final[painel_final['exposure_score_2d'].notna()]\n",
        "n_depois = len(painel_final)\n",
        "if n_antes > n_depois:\n",
        "    print(f\"Removidas {n_antes - n_depois:,} linhas sem exposure_score_2d\")\n",
        "\n",
        "# ── Verificação para o Notebook 2b ──\n",
        "cols_obrigatorias = ['exposure_score_2d', 'exposure_score_4d', 'alta_exp', 'did', 'tempo_relativo_meses', 'post']\n",
        "for c in cols_obrigatorias:\n",
        "    if c not in painel_final.columns:\n",
        "        raise ValueError(f\"Coluna obrigatória ausente para o DiD (Notebook 2b): {c}\")\n",
        "if painel_final[cols_obrigatorias].isna().any().any():\n",
        "    raise ValueError(\"NA em coluna obrigatória para o DiD (Notebook 2b).\")\n",
        "print(\"  Verificação: colunas obrigatórias para o 2b presentes e sem NA.\")\n",
        "\n",
        "# ── Salvar ──\n",
        "painel_final.to_parquet(PAINEL_FINAL_PARQUET, index=False)\n",
        "painel_final.to_csv(PAINEL_FINAL_CSV, index=False)\n",
        "\n",
        "# ══════════════════════════════════════════════════════════════════════\n",
        "# RESUMO FINAL\n",
        "# ══════════════════════════════════════════════════════════════════════\n",
        "print(f\"\\n{'=' * 60}\")\n",
        "print(\"DATASET ANALÍTICO FINAL — ETAPA 2a\")\n",
        "print(f\"{'=' * 60}\")\n",
        "print(f\"  Observações:        {len(painel_final):,}\")\n",
        "print(f\"  Ocupações (CBO 4d): {painel_final['cbo_4d'].nunique()}\")\n",
        "print(f\"  Períodos:           {painel_final['periodo'].nunique()} meses\")\n",
        "print(f\"    Pré-tratamento:   {painel_final[painel_final['post']==0]['periodo'].nunique()}\")\n",
        "print(f\"    Pós-tratamento:   {painel_final[painel_final['post']==1]['periodo'].nunique()}\")\n",
        "print(f\"  Cobertura 2d:       {painel_final['exposure_score_2d'].notna().mean():.1%}\")\n",
        "print(f\"  Cobertura 4d:       {painel_final['exposure_score_4d'].notna().mean():.1%}\")\n",
        "print(f\"  Tratamento 2d:      {painel_final['alta_exp'].mean():.1%} das obs\")\n",
        "print(f\"  Tratamento 4d:      {painel_final['alta_exp_4d'].mean():.1%} das obs\")\n",
        "print(f\"  Colunas:            {painel_final.shape[1]}\")\n",
        "print(f\"\\n  Salvo em:\")\n",
        "print(f\"    {PAINEL_FINAL_PARQUET}\")\n",
        "print(f\"    {PAINEL_FINAL_CSV}\")\n",
        "pq_mb = PAINEL_FINAL_PARQUET.stat().st_size / 1e6\n",
        "csv_mb = PAINEL_FINAL_CSV.stat().st_size / 1e6\n",
        "print(f\"    Tamanho: {pq_mb:.1f} MB (parquet), {csv_mb:.1f} MB (csv)\")\n",
        "\n",
        "print(f\"\\n  Info:\")\n",
        "painel_final.info()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c169961f",
      "metadata": {},
      "source": [
        "### Limitações desta etapa\n",
        "\n",
        "1. **Novo CAGED (descontinuidade 2020):** A transição para o eSocial (2020) pode afetar a comparabilidade. Mitigamos ao iniciar em 2021 (eSocial já estabilizado, sem efeitos COVID).\n",
        "\n",
        "2. **Fluxos vs. estoques:** O CAGED mede movimentações (admissões/desligamentos), não o estoque de empregados. Quedas em admissões não significam necessariamente queda no emprego total — podem refletir menor rotatividade. Esta é a mesma lógica usada por Hui et al. (2024) com dados do Upwork.\n",
        "\n",
        "3. **Crosswalk CBO → ISCO-08 (especificação principal, 2 dígitos):** Ao agregar por Sub-major Group com fallback a Major Group, perdemos variação intragrupo. Ocupações diferentes dentro do mesmo grupo recebem o mesmo score. A especificação de robustez a 4 dígitos (com fallback hierárquico em 6 níveis) ajuda a avaliar se essa agregação afeta os resultados.\n",
        "\n",
        "4. **Crosswalk CBO → ISCO-08 (robustez, 4 dígitos):** O match direto CBO 4d = ISCO-08 4d cobre apenas ~28% das ocupações. Para o restante, usamos fallback hierárquico (via correspondência ISCO-88→ISCO-08, médias a 3d, 2d e 1d). Quanto mais granular o match, mais preciso o score — mas mesmo com fallback, a correlação entre as especificações 2d e 4d é >0.91, indicando consistência. Erro de medição no tratamento tipicamente atenua os coeficientes (viés em direção a zero).\n",
        "\n",
        "5. **Muendler: CBO 1994, não CBO 2002:** O arquivo de concordância Muendler & Poole (2004) mapeia a CBO *1994* (formato X-XX.XX), não a CBO 2002 (XXXX) usada no CAGED. A utilidade do Muendler para match 4d direto é limitada. A estratégia adotada usa a similaridade estrutural entre CBO 2002 e ISCO-08/88 (ambas baseadas na ISCO), combinada com a tabela oficial de correspondência ISCO-08↔ISCO-88.\n",
        "\n",
        "6. **Emprego formal apenas:** O CAGED cobre apenas o mercado formal (CLT). A informalidade (~40% da força de trabalho brasileira) não é capturada. Efeitos da IA sobre o setor informal requerem fontes alternativas (PNAD).\n",
        "\n",
        "7. **Índice global aplicado ao Brasil:** Mesma limitação da Etapa 1 — o índice ILO foi desenvolvido com foco global e pode não capturar especificidades do mercado de trabalho brasileiro.\n",
        "\n",
        "---\n",
        "\n",
        "### Checklist de entregáveis\n",
        "\n",
        "- [x] `data/raw/caged_{ano}.parquet` — Microdados CAGED por ano (2021–2025)\n",
        "- [x] `data/input/cbo-isco-conc.csv` — Concordância Muendler CBO 1994→ISCO-88\n",
        "- [x] `data/input/Correspondência ISCO 08 a 88.xlsx` — Tabela oficial ISCO-08↔ISCO-88\n",
        "- [x] `data/processed/ilo_exposure_clean.csv` — Índice ILO processado (reusado da Etapa 1)\n",
        "- [x] `data/output/painel_caged_did_ready.parquet` — Dataset analítico final (com scores 2d e 4d)\n",
        "- [x] `data/output/painel_caged_did_ready.csv` — Backup CSV\n",
        "- [x] Todos os CHECKPOINTs passando sem warnings críticos\n",
        "- [x] Cobertura crosswalk 2d = 100%\n",
        "- [x] Cobertura crosswalk 4d = 100% (com fallback hierárquico)\n",
        "- [x] Correlação entre scores 2d e 4d: 0.9147\n",
        "- [x] Sanity check por grande grupo coerente com a literatura\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.15"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
